# 01

æˆ‘ä¾†ç‚ºé€™å€‹ç¨‹å¼ç¢¼æ·»åŠ è©³ç´°çš„ä¸­æ–‡è¨»è§£ï¼š

```python
import streamlit as st
from datetime import datetime
import random
from utils.llm_manager import LLMManager
from utils.story_processor import StoryProcessor

# ========== åˆå§‹åŒ–è¨­å®š ==========
# è¨­å®š Streamlit é é¢é…ç½®
st.set_page_config(
    page_title="PlotWeaver",  # ç€è¦½å™¨æ¨™ç±¤æ¨™é¡Œ
    layout="wide",            # ä½¿ç”¨å¯¬ç‰ˆé¢é…ç½®
    page_icon="ğŸ­"           # é é¢åœ–æ¨™
)

# åˆå§‹åŒ– session stateï¼ˆä¿å­˜æ‡‰ç”¨ç¨‹å¼ç‹€æ…‹ï¼‰
if 'llm_manager' not in st.session_state:
    st.session_state.llm_manager = LLMManager()        # LLM ç®¡ç†å™¨å¯¦ä¾‹
    st.session_state.processor = StoryProcessor()      # æ•…äº‹è™•ç†å™¨å¯¦ä¾‹
    st.session_state.generated_plots = []              # å„²å­˜ç”Ÿæˆçš„æƒ…ç¯€
    st.session_state.selected_plots = []               # å„²å­˜é¸ä¸­çš„æƒ…ç¯€
    st.session_state.clustered_plots = []              # å„²å­˜åˆ†ç¾¤å¾Œçš„æƒ…ç¯€

# ä¸»æ¨™é¡Œå’Œæè¿°
st.title("ğŸ­ PlotWeaver")
st.markdown("Generate creative stories using multiple AI models from YourAPI and Stima")

# ========== å´é‚Šæ¬„é…ç½® ==========
with st.sidebar:
    st.header("âš™ï¸ Configuration")
    
    # ç²å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
    available_models = st.session_state.llm_manager.get_available_models()
    
    # æª¢æŸ¥æ˜¯å¦æœ‰å¯ç”¨æ¨¡å‹
    if not available_models:
        # æ²’æœ‰å¯ç”¨æ¨¡å‹æ™‚é¡¯ç¤ºéŒ¯èª¤å’Œè¨­å®šæŒ‡å¼•
        st.error("âŒ No models available")
        st.info("""
        Please add your Stima API key in Secrets:
        
        1. Click 'Manage app' (bottom right)
        2. Go to Settings â†’ Secrets
        3. Add: STIMA_API_KEY = "your_key"
        
        Get your API key from:
        https://stima.tech
        """)
    else:
        # æœ‰å¯ç”¨æ¨¡å‹æ™‚é¡¯ç¤ºæˆåŠŸè¨Šæ¯
        st.success(f"âœ… {len(available_models)} models available")
        
        # å±•é–‹å¼é¸å–®é¡¯ç¤ºæ‰€æœ‰å¯ç”¨æ¨¡å‹
        with st.expander("ğŸ“‹ Available Models"):
            for model in available_models:
                st.write(f"â€¢ {model['display_name']}")
    
    st.divider()
    
    # ========== API é€£æ¥ç‹€æ…‹æª¢æŸ¥ ==========
    st.subheader("ğŸ“Š API Status")
    try:
        # æª¢æŸ¥ LLM ç®¡ç†å™¨æ˜¯å¦æœ‰ client å±¬æ€§ä¸”ä¸ç‚ºç©º
        if hasattr(st.session_state.llm_manager, 'client') and st.session_state.llm_manager.client:
            st.success("âœ… Stima API Connected")
        else:
            st.error("âŒ Stima API Not Connected")
            st.info("Please check your API key")
    except:
        # éŒ¯èª¤è™•ç†ï¼šæ ¹æ“šæ˜¯å¦æœ‰å¯ç”¨æ¨¡å‹åˆ¤æ–·é€£æ¥ç‹€æ…‹
        if available_models:
            st.success("âœ… API Connected")
        else:
            st.error("âŒ API Not Connected")

# ========== ä¸»è¦å…§å®¹å€ï¼šä¸‰å€‹æ¨™ç±¤é  ==========
tab1, tab2, tab3 = st.tabs(["ğŸ“ Generate Plots", "âœ… Select Plots", "ğŸ“– Generate Story"])

# ========== Tab 1: ç”Ÿæˆæƒ…ç¯€ ==========
with tab1:
    st.header("Step 1: Generate Story Plots")
    
    # å»ºç«‹å…©æ¬„ä½ˆå±€
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # åŸºæœ¬æ•…äº‹ä¸»é¡Œè¼¸å…¥æ¡†
        base_prompt = st.text_area(
            "Enter your story theme or idea:",
            "A mysterious adventure in a futuristic city where AI and humans coexist",  # é è¨­å€¼
            height=100
        )
        
        # æ¨¡å‹é¸æ“‡å€åŸŸ
        if available_models:
            # å‰µå»ºé¡¯ç¤ºåç¨±åˆ°å¯¦éš› key çš„å°æ‡‰å­—å…¸
            model_options = {model['display_name']: model['key'] 
                           for model in available_models}
            
            # å¤šé¸æ¡†ï¼šé¸æ“‡è¦ä½¿ç”¨çš„æ¨¡å‹
            selected_display_names = st.multiselect(
                "Select models for plot generation (max 5):",
                list(model_options.keys()),
                default=list(model_options.keys())[:min(3, len(model_options))]  # é è¨­é¸å‰3å€‹
            )
            
            # å°‡é¡¯ç¤ºåç¨±è½‰æ›å›å¯¦éš›çš„æ¨¡å‹ keys
            selected_models = [model_options[name] for name in selected_display_names]
            
            # å»ºç«‹å…©å€‹æŒ‰éˆ•æ¬„
            col_a, col_b = st.columns(2)
            with col_a:
                # éš¨æ©Ÿé¸æ“‡æ¨¡å‹æŒ‰éˆ•
                if st.button("ğŸ² Random Selection"):
                    num_models = min(5, len(available_models))
                    random_models = random.sample(list(model_options.keys()), 
                                                random.randint(2, num_models))
                    st.rerun()  # é‡æ–°åŸ·è¡Œæ‡‰ç”¨ç¨‹å¼ä»¥æ›´æ–°ä»‹é¢
            
            with col_b:
                # è¨­å®šæ¯å€‹æ¨¡å‹ç”Ÿæˆçš„æƒ…ç¯€æ•¸é‡
                num_variations = st.number_input("Plots per model:", 
                                               min_value=1, max_value=3, value=2)
    
    with col2:
        # é¡¯ç¤ºä½¿ç”¨æç¤º
        st.info("""
        **ğŸ’¡ Tips:**
        - YourAPI supports GPT, Claude, Gemini, Llama models
        - Stima provides various open-source models
        - Mix different models for creative diversity
        """)
    
    # ========== ç”Ÿæˆæƒ…ç¯€æŒ‰éˆ•å’Œè™•ç†é‚è¼¯ ==========
    if st.button("ğŸš€ Generate Plots", type="primary", disabled=not available_models):
        with st.spinner("Generating plots from multiple models..."):
            plots = []  # å„²å­˜æ‰€æœ‰ç”Ÿæˆçš„æƒ…ç¯€
            progress_bar = st.progress(0)  # é€²åº¦æ¢
            total_tasks = len(selected_models) * num_variations  # ç¸½ä»»å‹™æ•¸
            current_task = 0
            
            # éæ­·æ¯å€‹é¸å®šçš„æ¨¡å‹
            for model_key in selected_models:
                # ç²å–æ¨¡å‹é¡¯ç¤ºåç¨±
                model_name = next(m['display_name'] for m in available_models 
                                if m['key'] == model_key)
                
                # ç‚ºæ¯å€‹æ¨¡å‹ç”ŸæˆæŒ‡å®šæ•¸é‡çš„è®ŠåŒ–ç‰ˆæœ¬
                for i in range(num_variations):
                    current_task += 1
                    progress_bar.progress(current_task / total_tasks)  # æ›´æ–°é€²åº¦æ¢
                    
                    # æ§‹å»ºæƒ…ç¯€ç”Ÿæˆæç¤ºè©
                    plot_prompt = f"""Create a unique story plot based on: {base_prompt}
                    
                    Variation {i+1}: Focus on a different aspect or perspective.
                    Be creative and original. About 50-100 words."""
                    
                    # èª¿ç”¨ LLM ç”Ÿæˆæƒ…ç¯€
                    plot_text = st.session_state.llm_manager.generate_plot(
                        model_key, plot_prompt
                    )
                    
                    # å„²å­˜ç”Ÿæˆçš„æƒ…ç¯€
                    plots.append({
                        'model': model_name,
                        'plot': plot_text,
                        'id': f"{model_key}_{i}"  # å”¯ä¸€è­˜åˆ¥ç¢¼
                    })
            
            progress_bar.empty()  # æ¸…é™¤é€²åº¦æ¢
            st.session_state.generated_plots = plots  # ä¿å­˜åˆ° session state
            
            # ========== ä½¿ç”¨ AI è‡ªå‹•åˆ†ç¾¤æƒ…ç¯€ ==========
            if len(plots) > 0:
                with st.spinner("Analyzing and clustering plots..."):
                    # èª¿ç”¨ AI åˆ†ç¾¤åŠŸèƒ½
                    st.session_state.clustered_plots = st.session_state.llm_manager.cluster_plots_with_ai(plots)
                st.success(f"âœ… Generated {len(plots)} plots and organized them into {len(st.session_state.clustered_plots)} groups")
    
    # ========== é¡¯ç¤ºç”Ÿæˆçš„æƒ…ç¯€ ==========
    if st.session_state.generated_plots:
        st.divider()
        st.subheader("ğŸ“š Generated Plots")
        
        # ä½¿ç”¨å±•é–‹å™¨é¡¯ç¤ºæ¯å€‹æƒ…ç¯€
        for i, plot in enumerate(st.session_state.generated_plots):
            with st.expander(f"Plot {i+1} - {plot['model']}", expanded=(i==0)):  # ç¬¬ä¸€å€‹é è¨­å±•é–‹
                st.write(plot['plot'])
                st.caption(f"Model: {plot['model']} | ID: {plot['id']}")

# ========== Tab 2: é¸æ“‡æƒ…ç¯€ ==========
with tab2:
    st.header("Step 2: Select Plot Groups")
    
    if st.session_state.clustered_plots:
        st.info(f"Found {len(st.session_state.clustered_plots)} plot groups. Select the plots you want to use for story generation.")
        
        # æ¸…é™¤é¸æ“‡æŒ‰éˆ•
        if st.button("ğŸ”„ Clear Selection"):
            st.session_state.selected_plots = []
            st.rerun()
        
        # éæ­·æ¯å€‹æƒ…ç¯€ç¾¤çµ„
        for group_idx, group in enumerate(st.session_state.clustered_plots):
            with st.expander(f"**Group {group_idx+1}: {group['theme']}**", expanded=True):
                # é¡¯ç¤ºç¾¤çµ„å…±åŒå…ƒç´ ï¼ˆå¦‚æœæœ‰ï¼‰
                if 'common_elements' in group:
                    st.caption(f"Common elements: {group['common_elements']}")
                
                st.write("**Plots in this group:**")
                
                # é¡¯ç¤ºè©²ç¾¤çµ„ä¸­çš„æ‰€æœ‰æƒ…ç¯€
                for idx in group['plot_indices']:
                    if idx <= len(st.session_state.generated_plots):
                        plot = st.session_state.generated_plots[idx-1]
                        
                        # å»ºç«‹é¸æ“‡æ¡†å’Œå…§å®¹æ¬„
                        col1, col2 = st.columns([1, 10])
                        with col1:
                            # å‹¾é¸æ¡†ç”¨æ–¼é¸æ“‡æƒ…ç¯€
                            is_selected = st.checkbox("", key=f"plot_select_{idx}",
                                                    value=(plot in st.session_state.selected_plots))
                        with col2:
                            st.write(f"**{plot['model']}**")
                            st.write(plot['plot'])
                        
                        # æ›´æ–°é¸ä¸­çš„æƒ…ç¯€åˆ—è¡¨
                        if is_selected and plot not in st.session_state.selected_plots:
                            st.session_state.selected_plots.append(plot)
                        elif not is_selected and plot in st.session_state.selected_plots:
                            st.session_state.selected_plots.remove(plot)
                
                st.divider()
        
        # é¡¯ç¤ºå·²é¸æ“‡çš„æƒ…ç¯€æ•¸é‡
        if st.session_state.selected_plots:
            st.success(f"âœ… Selected {len(st.session_state.selected_plots)} plots")
    else:
        st.info("ğŸ“ Please generate plots first in Tab 1")

# ========== Tab 3: ç”Ÿæˆå®Œæ•´æ•…äº‹ ==========
with tab3:
    st.header("Step 3: Generate Complete Story")
    
    if st.session_state.selected_plots:
        st.subheader("ğŸ“‹ Selected Plots")
        
        # é¡¯ç¤ºå·²é¸æ“‡çš„æƒ…ç¯€ï¼ˆå¯å±•é–‹ï¼‰
        with st.expander("View selected plots", expanded=False):
            for idx, plot in enumerate(st.session_state.selected_plots):
                st.write(f"{idx+1}. **{plot['model']}**: {plot['plot']}")
        
        st.divider()
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            # é¸æ“‡ç”¨æ–¼ç”Ÿæˆæ•…äº‹çš„æ¨¡å‹
            if available_models:
                # å‰µå»ºæ¨¡å‹é¸é …å­—å…¸
                story_model_options = {model['display_name']: model['key'] 
                                     for model in available_models}
                
                # å¤šé¸æ¡†ï¼šé¸æ“‡ç”Ÿæˆæ•…äº‹çš„æ¨¡å‹ï¼ˆæœ€å¤š3å€‹ï¼‰
                selected_story_displays = st.multiselect(
                    "Select models for story generation (max 3):",
                    list(story_model_options.keys()),
                    default=list(story_model_options.keys())[:min(2, len(story_model_options))],
                    max_selections=3
                )
                
                # è½‰æ›ç‚ºå¯¦éš›çš„æ¨¡å‹ keys
                story_models = [story_model_options[name] for name in selected_story_displays]
            
            # æ•…äº‹é¢¨æ ¼é¸æ“‡
            story_style = st.selectbox(
                "Story style:",
                ["Narrative", "Mystery", "Sci-Fi", "Fantasy", "Romance", 
                 "Thriller", "Comedy", "Drama", "Horror", "Adventure"]
            )
            
            # æ•…äº‹é•·åº¦é¸æ“‡
            story_length = st.select_slider(
                "Story length:",
                options=["Short (500 words)", "Medium (1000 words)", "Long (1500 words)"],
                value="Medium (1000 words)"
            )
        
        with col2:
            # é¡¯ç¤ºç”Ÿæˆæç¤º
            st.info("""
            **ğŸ“– Story Generation:**
            - Multiple models create different versions
            - Each brings unique perspective
            - Export as Markdown files
            """)
        
        # ========== ç”Ÿæˆæ•…äº‹æŒ‰éˆ•å’Œè™•ç†é‚è¼¯ ==========
        if st.button("âœ¨ Generate Stories", type="primary"):
            with st.spinner("Creating your stories..."):
                # åˆä½µæ‰€æœ‰é¸å®šçš„æƒ…ç¯€
                combined_plots = "\n".join([f"- {p['plot']}" for p in st.session_state.selected_plots])
                
                stories = []  # å„²å­˜ç”Ÿæˆçš„æ•…äº‹
                progress = st.progress(0)  # é€²åº¦æ¢
                
                # ä½¿ç”¨æ¯å€‹é¸å®šçš„æ¨¡å‹ç”Ÿæˆæ•…äº‹
                for idx, model_key in enumerate(story_models):
                    progress.progress((idx + 1) / len(story_models))
                    
                    # ç²å–æ¨¡å‹åç¨±
                    model_name = next(m['display_name'] for m in available_models 
                                     if m['key'] == model_key)
                    
                    st.info(f"Generating story with {model_name}...")
                    
                    # èª¿ç”¨ LLM ç”Ÿæˆæ•…äº‹
                    story = st.session_state.llm_manager.generate_story(
                        model_key, combined_plots, story_style.lower()
                    )
                    
                    # å„²å­˜æ•…äº‹å’Œç›¸é—œå…ƒæ•¸æ“š
                    stories.append({
                        'model': model_name,
                        'story': story,
                        'metadata': {
                            'title': f"{story_style} Story",
                            'model': model_name,
                            'date': datetime.now().strftime("%Y-%m-%d %H:%M"),
                            'style': story_style,
                            'based_on_plots': len(st.session_state.selected_plots)
                        }
                    })
                
                progress.empty()  # æ¸…é™¤é€²åº¦æ¢
                st.success("âœ… Stories generated successfully!")
                
                # ========== é¡¯ç¤ºç”Ÿæˆçš„æ•…äº‹ ==========
                for story_data in stories:
                    st.divider()
                    
                    col1, col2 = st.columns([3, 1])
                    with col1:
                        # æ•…äº‹æ¨™é¡Œ
                        st.subheader(f"ğŸ“– {story_data['metadata']['title']} by {story_data['model']}")
                    
                    with col2:
                        # æº–å‚™ Markdown æ ¼å¼çš„å…§å®¹ä¾›ä¸‹è¼‰
                        markdown_content = f"""# {story_data['metadata']['title']}

**Generated by:** {story_data['metadata']['model']}  
**Date:** {story_data['metadata']['date']}  
**Style:** {story_data['metadata']['style']}  
**Based on:** {story_data['metadata']['based_on_plots']} selected plots  

---

{story_data['story']}

---

*This story was generated using AI through Story Generator Hub.*  
*Powered by YourAPI & Stima API*
"""
                        
                        # ä¸‹è¼‰æŒ‰éˆ•
                        st.download_button(
                            label=f"ğŸ“¥ Download",
                            data=markdown_content,
                            file_name=f"story_{story_data['metadata']['style'].lower()}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md",
                            mime="text/markdown",
                            key=f"download_{story_data['model']}_{idx}"
                        )
                    
                    # é¡¯ç¤ºæ•…äº‹å…§å®¹
                    with st.container():
                        st.markdown(story_data['story'])
                    
    else:
        # æ²’æœ‰é¸æ“‡æƒ…ç¯€æ™‚çš„æç¤º
        st.info("âœ‹ Please select plots in Tab 2 first")
        
        # æä¾›å¿«é€Ÿé–‹å§‹é¸é …
        if st.session_state.generated_plots:
            if st.button("ğŸš€ Quick Start - Select All Plots"):
                # é¸æ“‡æ‰€æœ‰å·²ç”Ÿæˆçš„æƒ…ç¯€
                st.session_state.selected_plots = st.session_state.generated_plots.copy()
                st.rerun()
```

é€™å€‹ç¨‹å¼ç¢¼æ˜¯ä¸€å€‹å®Œæ•´çš„æ•…äº‹ç”Ÿæˆæ‡‰ç”¨ç¨‹å¼ï¼Œä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š

1. **åˆå§‹åŒ–éšæ®µ**ï¼šè¨­å®šé é¢é…ç½®å’Œåˆå§‹åŒ–å¿…è¦çš„å…ƒä»¶
2. **å´é‚Šæ¬„**ï¼šé¡¯ç¤ºé…ç½®è³‡è¨Šã€å¯ç”¨æ¨¡å‹å’Œ API é€£æ¥ç‹€æ…‹
3. **Tab 1**ï¼šä½¿ç”¨å¤šå€‹ AI æ¨¡å‹ç”Ÿæˆæ•…äº‹æƒ…ç¯€ï¼Œä¸¦è‡ªå‹•åˆ†ç¾¤
4. **Tab 2**ï¼šå¾åˆ†ç¾¤å¾Œçš„æƒ…ç¯€ä¸­é¸æ“‡è¦ä½¿ç”¨çš„å…§å®¹
5. **Tab 3**ï¼šåŸºæ–¼é¸å®šçš„æƒ…ç¯€ï¼Œä½¿ç”¨ä¸åŒæ¨¡å‹ç”Ÿæˆå®Œæ•´æ•…äº‹ï¼Œä¸¦æä¾›ä¸‹è¼‰åŠŸèƒ½

# 02

æˆ‘ä¾†ç‚ºé€™å€‹ `LLMManager` é¡åˆ¥æ·»åŠ è©³ç´°çš„ä¸­æ–‡è¨»è§£ï¼š

```python
import os
from typing import List, Dict, Optional
from dataclasses import dataclass
from openai import OpenAI
import requests
from dotenv import load_dotenv

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸æª”æ¡ˆ (.env)
load_dotenv()

@dataclass
class ModelConfig:
    """
    æ¨¡å‹é…ç½®è³‡æ–™é¡åˆ¥
    ç”¨æ–¼å„²å­˜æ¯å€‹ AI æ¨¡å‹çš„é…ç½®è³‡è¨Š
    """
    name: str           # æ¨¡å‹çš„å¯¦éš›åç¨±ï¼ˆAPI ä½¿ç”¨çš„åç¨±ï¼‰
    provider: str       # æä¾›è€…åç¨±ï¼ˆå¦‚ 'stima'ï¼‰
    available: bool     # æ¨¡å‹æ˜¯å¦å¯ç”¨
    display_name: str   # é¡¯ç¤ºçµ¦ä½¿ç”¨è€…çœ‹çš„å‹å–„åç¨±
    
class LLMManager:
    """
    LLM (å¤§å‹èªè¨€æ¨¡å‹) ç®¡ç†å™¨
    è² è²¬ç®¡ç†å’Œèª¿ç”¨å„ç¨® AI æ¨¡å‹ï¼Œé€é Stima API æä¾›çµ±ä¸€çš„ä»‹é¢
    """
    
    def __init__(self):
        """åˆå§‹åŒ– LLM ç®¡ç†å™¨"""
        self.client = None                              # OpenAI å®¢æˆ¶ç«¯å¯¦ä¾‹
        self.models = self._initialize_models()        # åˆå§‹åŒ–å¯ç”¨æ¨¡å‹å­—å…¸
        self.is_connected = False                      # API é€£æ¥ç‹€æ…‹æ¨™è¨˜
        
    def _initialize_models(self) -> Dict[str, ModelConfig]:
        """
        åˆå§‹åŒ– Stima API å’Œæ¨¡å‹åˆ—è¡¨
        
        Returns:
            Dict[str, ModelConfig]: æ¨¡å‹é…ç½®å­—å…¸ï¼Œkey ç‚ºæ¨¡å‹åç¨±ï¼Œvalue ç‚º ModelConfig ç‰©ä»¶
        """
        models = {}  # å„²å­˜æ‰€æœ‰æ¨¡å‹é…ç½®çš„å­—å…¸
        
        # ========== Stima API è¨­å®š ==========
        # å¾ç’°å¢ƒè®Šæ•¸å–å¾— API é‡‘é‘°
        api_key = os.getenv('STIMA_API_KEY')
        
        # æª¢æŸ¥ API é‡‘é‘°æ˜¯å¦æœ‰æ•ˆï¼ˆéç©ºä¸”ä¸æ˜¯é è¨­å€¼ï¼‰
        if api_key and api_key != 'your_stima_api_key_here':
            try:
                # åˆå§‹åŒ– OpenAI å®¢æˆ¶ç«¯ï¼ŒæŒ‡å‘ Stima API endpoint
                self.client = OpenAI(
                    api_key=api_key,
                    base_url="https://api.stima.tech/v1"  # Stima API çš„åŸºç¤ URL
                )
                self.is_connected = True  # æ¨™è¨˜ç‚ºå·²æˆåŠŸé€£æ¥
                
                # ========== å‹•æ…‹ç²å–æ¨¡å‹åˆ—è¡¨ ==========
                try:
                    # å˜—è©¦å¾ Stima API å‹•æ…‹ç²å–å¯ç”¨æ¨¡å‹
                    stima_models = self._fetch_stima_models()
                    
                    if stima_models:  # å¦‚æœæˆåŠŸç²å–åˆ°æ¨¡å‹åˆ—è¡¨
                        # ç‚ºæ¯å€‹æ¨¡å‹å»ºç«‹ ModelConfig ç‰©ä»¶
                        for model_id in stima_models:
                            models[model_id] = ModelConfig(
                                name=model_id,
                                provider='stima',
                                available=True,
                                # å°‡æ¨¡å‹ ID è½‰æ›ç‚ºå‹å–„çš„é¡¯ç¤ºåç¨±
                                # ä¾‹å¦‚: 'gpt-4o-mini' -> 'GPT 4O Mini'
                                display_name=model_id.upper().replace('-', ' ').title()
                            )
                    else:
                        raise Exception("No models fetched")
                        
                except:
                    # ========== ä½¿ç”¨é è¨­æ¨¡å‹åˆ—è¡¨ï¼ˆå‚™ç”¨æ–¹æ¡ˆï¼‰==========
                    # å¦‚æœç„¡æ³•å¾ API ç²å–ï¼Œä½¿ç”¨ç¡¬ç·¨ç¢¼çš„é è¨­åˆ—è¡¨
                    default_stima_models = [
                        ('gpt-4o', 'GPT-4o'),                    # OpenAI æœ€æ–°æ¨¡å‹
                        ('gpt-4o-mini', 'GPT-4o Mini'),          # è¼•é‡ç‰ˆ GPT-4o
                        ('gpt-4-turbo', 'GPT-4 Turbo'),          # GPT-4 Turbo
                        ('gpt-3.5-turbo', 'GPT-3.5 Turbo'),      # GPT-3.5
                        ('claude-3-5-sonnet', 'Claude 3.5 Sonnet'),  # Anthropic æœ€æ–°æ¨¡å‹
                        ('claude-3-opus', 'Claude 3 Opus'),          # Claude 3 é«˜éšç‰ˆ
                        ('claude-3-sonnet', 'Claude 3 Sonnet'),      # Claude 3 ä¸­éšç‰ˆ
                        ('claude-3-haiku', 'Claude 3 Haiku'),        # Claude 3 è¼•é‡ç‰ˆ
                        ('gemini-1.5-pro', 'Gemini 1.5 Pro'),        # Google Gemini Pro
                        ('gemini-1.5-flash', 'Gemini 1.5 Flash'),    # Google Gemini Flash
                        ('llama-3.1-70b', 'Llama 3.1 70B'),          # Meta Llama å¤§æ¨¡å‹
                        ('llama-3.1-8b', 'Llama 3.1 8B'),            # Meta Llama å°æ¨¡å‹
                        ('mixtral-8x7b', 'Mixtral 8x7B'),            # Mistral æ··åˆå°ˆå®¶æ¨¡å‹
                        ('deepseek-chat', 'DeepSeek Chat'),          # DeepSeek å°è©±æ¨¡å‹
                    ]
                    
                    # å°‡é è¨­æ¨¡å‹åŠ å…¥åˆ° models å­—å…¸
                    for model_id, display_name in default_stima_models:
                        models[model_id] = ModelConfig(
                            name=model_id,
                            provider='stima',
                            available=True,
                            display_name=display_name
                        )
                        
            except Exception as e:
                # åˆå§‹åŒ–å¤±æ•—çš„éŒ¯èª¤è™•ç†
                print(f"Failed to initialize Stima client: {e}")
                self.is_connected = False  # æ¨™è¨˜ç‚ºæœªé€£æ¥
        
        return models
    
    def _fetch_stima_models(self) -> List[str]:
        """
        å¾ Stima API å‹•æ…‹ç²å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
        
        Returns:
            List[str]: å¯ç”¨æ¨¡å‹ ID çš„åˆ—è¡¨ï¼Œå¦‚æœå¤±æ•—å‰‡è¿”å›ç©ºåˆ—è¡¨
        """
        try:
            # ç²å– API é‡‘é‘°
            api_key = os.getenv('STIMA_API_KEY')
            if not api_key:
                return []
            
            # è¨­å®š HTTP è«‹æ±‚æ¨™é ­ï¼ŒåŒ…å«èªè­‰è³‡è¨Š
            headers = {
                'Authorization': f"Bearer {api_key}"
            }
            
            # ç™¼é€ GET è«‹æ±‚åˆ° Stima API çš„ models endpoint
            response = requests.get(
                "https://api.stima.tech/v1/models",
                headers=headers,
                timeout=5  # 5 ç§’è¶…æ™‚é™åˆ¶
            )
            
            # æª¢æŸ¥å›æ‡‰ç‹€æ…‹
            if response.status_code == 200:
                data = response.json()
                # å¾å›æ‡‰ä¸­æå–æ¨¡å‹ ID
                # API å›æ‡‰æ ¼å¼: {"data": [{"id": "model-name", ...}, ...]}
                models = [model['id'] for model in data.get('data', [])]
                return models if models else []
                
        except Exception as e:
            print(f"Failed to fetch models: {e}")
            
        return []
    
    def is_api_connected(self) -> bool:
        """
        æª¢æŸ¥ API æ˜¯å¦å·²æˆåŠŸé€£æ¥
        
        Returns:
            bool: True è¡¨ç¤ºå·²é€£æ¥ï¼ŒFalse è¡¨ç¤ºæœªé€£æ¥
        """
        return self.is_connected and self.client is not None
    
    def get_available_models(self) -> List[Dict[str, str]]:
        """
        ç²å–æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨ï¼ˆä¾› UI é¡¯ç¤ºä½¿ç”¨ï¼‰
        
        Returns:
            List[Dict[str, str]]: æ¨¡å‹è³‡è¨Šåˆ—è¡¨ï¼Œæ¯å€‹å…ƒç´ åŒ…å« key å’Œ display_name
        """
        return [
            {
                'key': key,                           # æ¨¡å‹çš„å…§éƒ¨ key
                'display_name': config.display_name   # é¡¯ç¤ºçµ¦ä½¿ç”¨è€…çš„åç¨±
            }
            for key, config in self.models.items() 
            if config.available  # åªè¿”å›å¯ç”¨çš„æ¨¡å‹
        ]
    
    def generate_plot(self, model_key: str, prompt: str) -> str:
        """
        ä½¿ç”¨æŒ‡å®šæ¨¡å‹ç”Ÿæˆæ•…äº‹æƒ…ç¯€
        
        Args:
            model_key (str): æ¨¡å‹çš„ keyï¼ˆå¦‚ 'gpt-4o'ï¼‰
            prompt (str): ç”Ÿæˆæƒ…ç¯€çš„æç¤ºè©
            
        Returns:
            str: ç”Ÿæˆçš„æƒ…ç¯€æ–‡å­—ï¼Œæˆ–éŒ¯èª¤è¨Šæ¯
        """
        # æª¢æŸ¥ API é€£æ¥ç‹€æ…‹
        if not self.is_api_connected():
            return "Stima API not configured. Please check your API key."
        
        # æª¢æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
        if model_key not in self.models:
            return f"Model {model_key} not available"
        
        # ç²å–æ¨¡å‹é…ç½®
        config = self.models[model_key]
        
        try:
            # èª¿ç”¨ OpenAI API ç”Ÿæˆæƒ…ç¯€
            response = self.client.chat.completions.create(
                model=config.name,  # ä½¿ç”¨å¯¦éš›çš„æ¨¡å‹åç¨±
                messages=[
                    {
                        "role": "system", 
                        "content": "You are a creative story writer. Create unique and engaging story plots in Traditional Chinese."
                    },
                    {
                        "role": "user", 
                        "content": prompt
                    }
                ],
                max_tokens=300,     # é™åˆ¶è¼¸å‡ºé•·åº¦ï¼ˆç´„ 150-200 å­—ï¼‰
                temperature=0.8     # å‰µé€ æ€§ç¨‹åº¦ï¼ˆ0.8 è¡¨ç¤ºè¼ƒé«˜å‰µé€ æ€§ï¼‰
            )
            
            # è¿”å›ç”Ÿæˆçš„å…§å®¹
            return response.choices[0].message.content
            
        except Exception as e:
            # éŒ¯èª¤è™•ç†ï¼šè¿”å›éŒ¯èª¤è¨Šæ¯
            return f"Error with {config.display_name}: {str(e)}"
    
    def generate_story(self, model_key: str, plot: str, style: str = "narrative") -> str:
        """
        æ ¹æ“šæƒ…ç¯€ç”Ÿæˆå®Œæ•´æ•…äº‹
        
        Args:
            model_key (str): æ¨¡å‹çš„ key
            plot (str): æ•…äº‹æƒ…ç¯€
            style (str): æ•…äº‹é¢¨æ ¼ï¼ˆé è¨­ç‚º narrativeï¼‰
            
        Returns:
            str: ç”Ÿæˆçš„å®Œæ•´æ•…äº‹ï¼Œæˆ–éŒ¯èª¤è¨Šæ¯
        """
        # æª¢æŸ¥ API é€£æ¥ç‹€æ…‹
        if not self.is_api_connected():
            return "Stima API not configured. Please check your API key."
        
        # æª¢æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
        if model_key not in self.models:
            return f"Model {model_key} not available"
        
        config = self.models[model_key]
        
        # æ§‹å»ºæ•…äº‹ç”Ÿæˆçš„æç¤ºè©
        prompt = f"""Based on this plot: {plot}
        
        Write a complete short story in {style} style.
        Make it engaging and approximately 500-800 words.
        Include vivid descriptions, character development, and a satisfying conclusion.
        Write in Traditional Chinese.
        """
        
        try:
            # èª¿ç”¨ OpenAI API ç”Ÿæˆæ•…äº‹
            response = self.client.chat.completions.create(
                model=config.name,
                messages=[
                    {
                        "role": "system", 
                        "content": f"You are a skilled {style} story writer. Create immersive and captivating stories in Traditional Chinese."
                    },
                    {
                        "role": "user", 
                        "content": prompt
                    }
                ],
                max_tokens=2000,    # è¼ƒé•·çš„è¼¸å‡ºé™åˆ¶ï¼ˆç´„ 1000-1500 å­—ï¼‰
                temperature=0.7     # é©ä¸­çš„å‰µé€ æ€§ï¼ˆå¹³è¡¡å‰µæ„å’Œé€£è²«æ€§ï¼‰
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            return f"Error with {config.display_name}: {str(e)}"
    
    def cluster_plots_with_ai(self, plots: List[Dict[str, str]]) -> List[Dict]:
        """
        ä½¿ç”¨ AI æ¨¡å‹å°‡ç›¸ä¼¼çš„æƒ…ç¯€è‡ªå‹•åˆ†ç¾¤
        
        Args:
            plots (List[Dict[str, str]]): æƒ…ç¯€åˆ—è¡¨ï¼Œæ¯å€‹åŒ…å« 'plot' å’Œ 'model' éµ
            
        Returns:
            List[Dict]: åˆ†ç¾¤çµæœï¼Œæ¯å€‹ç¾¤çµ„åŒ…å«ä¸»é¡Œã€æƒ…ç¯€ç´¢å¼•å’Œå…±åŒå…ƒç´ 
        """
        # æª¢æŸ¥ API é€£æ¥
        if not self.is_api_connected():
            return self._simple_clustering(plots)  # ä½¿ç”¨å‚™ç”¨æ–¹æ¡ˆ
        
        # ========== é¸æ“‡é©åˆçš„æ¨¡å‹é€²è¡Œåˆ†é¡ ==========
        clustering_model = None
        # å„ªå…ˆä½¿ç”¨è¼ƒä¾¿å®œã€é€Ÿåº¦å¿«çš„æ¨¡å‹
        preferred_models = [
            'gpt-3.5-turbo',     # æœ€ä¾¿å®œä¸”å¿«é€Ÿ
            'gpt-4o-mini',       # è¼•é‡ç‰ˆ GPT-4
            'claude-3-haiku',    # Claude è¼•é‡ç‰ˆ
            'gemini-1.5-flash'   # Gemini å¿«é€Ÿç‰ˆ
        ]
        
        # å°‹æ‰¾ç¬¬ä¸€å€‹å¯ç”¨çš„åå¥½æ¨¡å‹
        for model in preferred_models:
            if model in self.models:
                clustering_model = model
                break
        
        # å¦‚æœæ²’æœ‰åå¥½æ¨¡å‹ï¼Œä½¿ç”¨ä»»ä½•å¯ç”¨çš„æ¨¡å‹
        if not clustering_model and self.models:
            clustering_model = list(self.models.keys())[0]
        
        # å¦‚æœæ²’æœ‰ä»»ä½•æ¨¡å‹å¯ç”¨ï¼Œä½¿ç”¨å‚™ç”¨æ–¹æ¡ˆ
        if not clustering_model:
            return self._simple_clustering(plots)
        
        # ========== æº–å‚™åˆ†ç¾¤æç¤ºè© ==========
        # å°‡æ‰€æœ‰æƒ…ç¯€æ ¼å¼åŒ–ç‚ºç·¨è™Ÿåˆ—è¡¨
        plot_text = "\n".join([f"{i+1}. {p['plot']}" for i, p in enumerate(plots)])
        
        # æ§‹å»ºè¦æ±‚ AI åˆ†ç¾¤çš„æç¤ºè©
        prompt = f"""
        Analyze these story plots and group similar ones by theme, genre, or narrative elements.
        Return ONLY valid JSON format.
        
        Plots:
        {plot_text}
        
        Required JSON format:
        {{
            "groups": [
                {{
                    "theme": "Brief theme description in English",
                    "plot_indices": [1, 3, 5],
                    "common_elements": "What these plots share"
                }}
            ]
        }}
        """
        
        try:
            # èª¿ç”¨ AI é€²è¡Œåˆ†ç¾¤åˆ†æ
            response = self.client.chat.completions.create(
                model=clustering_model,
                messages=[
                    {
                        "role": "system", 
                        "content": "You are a literary analyst. Return only valid JSON."
                    },
                    {
                        "role": "user", 
                        "content": prompt
                    }
                ],
                max_tokens=500,     # è¶³å¤ è¿”å›åˆ†ç¾¤çµæœ
                temperature=0.3     # ä½æº«åº¦ä»¥ç²å¾—æ›´ä¸€è‡´çš„è¼¸å‡º
            )
            
            # ========== è§£æ JSON å›æ‡‰ ==========
            import json
            result_text = response.choices[0].message.content
            
            # æ¸…ç†å¯èƒ½çš„ markdown æ¨™è¨˜ï¼ˆæœ‰äº›æ¨¡å‹æœƒåŠ ä¸Š ```json``` åŒ…è£ï¼‰
            if '```json' in result_text:
                result_text = result_text.split('```json')[1].split('```')[0]
            elif '```' in result_text:
                result_text = result_text.split('```')[1].split('```')[0]
            
            # è§£æ JSON ä¸¦è¿”å›ç¾¤çµ„åˆ—è¡¨
            result = json.loads(result_text.strip())
            return result['groups']
            
        except Exception as e:
            # å¦‚æœ AI åˆ†ç¾¤å¤±æ•—ï¼Œä½¿ç”¨å‚™ç”¨æ–¹æ¡ˆ
            print(f"AI clustering failed: {e}")
            return self._simple_clustering(plots)
    
    def _simple_clustering(self, plots: List[Dict[str, str]]) -> List[Dict]:
        """
        ç°¡å–®çš„å‚™ç”¨åˆ†ç¾¤æ–¹æ³•
        ç•¶ AI åˆ†ç¾¤å¤±æ•—æ™‚ä½¿ç”¨ï¼Œå°‡æ‰€æœ‰æƒ…ç¯€æ”¾åœ¨åŒä¸€å€‹ç¾¤çµ„
        
        Args:
            plots (List[Dict[str, str]]): æƒ…ç¯€åˆ—è¡¨
            
        Returns:
            List[Dict]: åŒ…å«å–®ä¸€ç¾¤çµ„çš„åˆ—è¡¨
        """
        return [{
            "theme": "All generated plots",                           # ç¾¤çµ„ä¸»é¡Œ
            "plot_indices": list(range(1, len(plots) + 1)),          # åŒ…å«æ‰€æœ‰æƒ…ç¯€ç´¢å¼•
            "common_elements": "Various creative story ideas"         # å…±åŒå…ƒç´ æè¿°
        }]
```

é€™å€‹ `LLMManager` é¡åˆ¥çš„ä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š

1. **æ¨¡å‹ç®¡ç†**ï¼šå‹•æ…‹è¼‰å…¥å’Œç®¡ç†å¤šå€‹ AI æ¨¡å‹
2. **API é€£æ¥**ï¼šè™•ç† Stima API çš„é€£æ¥å’Œèªè­‰
3. **æƒ…ç¯€ç”Ÿæˆ**ï¼šä½¿ç”¨ä¸åŒæ¨¡å‹ç”Ÿæˆæ•…äº‹æƒ…ç¯€
4. **æ•…äº‹ç”Ÿæˆ**ï¼šåŸºæ–¼æƒ…ç¯€ç”Ÿæˆå®Œæ•´æ•…äº‹
5. **æ™ºèƒ½åˆ†ç¾¤**ï¼šä½¿ç”¨ AI è‡ªå‹•å°‡ç›¸ä¼¼æƒ…ç¯€åˆ†çµ„
6. **éŒ¯èª¤è™•ç†**ï¼šå®Œå–„çš„éŒ¯èª¤è™•ç†å’Œå‚™ç”¨æ–¹æ¡ˆ

# 03

æ‚¨çš„è¨­å®šæœ‰ä¸€äº›éœ€è¦èª¿æ•´çš„åœ°æ–¹ã€‚è®“æˆ‘åˆ†æä¸¦æä¾›å»ºè­°ï¼š

## ğŸ” å•é¡Œåˆ†æ

### 1. **max_tokens=50000 å¤ªé«˜äº†**
å¤§å¤šæ•¸æ¨¡å‹éƒ½æœ‰è¼¸å‡ºé™åˆ¶ï¼š
- GPT-3.5-turbo: æœ€å¤š 4,096 tokens
- GPT-4/GPT-4o: æœ€å¤š 4,096 tokens è¼¸å‡º
- Claude 3 ç³»åˆ—: æœ€å¤š 4,096 tokens è¼¸å‡º
- Gemini: æœ€å¤š 8,192 tokens

### 2. **å­—æ•¸è¦æ±‚éé«˜**
- 3000-8000 ä¸­æ–‡å­—ç´„éœ€è¦ 4,500-12,000 tokens
- é€™è¶…éäº†å¤§å¤šæ•¸æ¨¡å‹çš„å–®æ¬¡è¼¸å‡ºé™åˆ¶

## âœ… å»ºè­°çš„ä¿®æ”¹ç‰ˆæœ¬

```python
def generate_story(self, model_key: str, plot: str, style: str = "narrative", length: str = "medium") -> str:
    """
    æ ¹æ“šæƒ…ç¯€ç”Ÿæˆå®Œæ•´æ•…äº‹
    
    Args:
        model_key (str): æ¨¡å‹çš„ key
        plot (str): æ•…äº‹æƒ…ç¯€
        style (str): æ•…äº‹é¢¨æ ¼ï¼ˆé è¨­ç‚º narrativeï¼‰
        length (str): æ•…äº‹é•·åº¦ ("short", "medium", "long")
        
    Returns:
        str: ç”Ÿæˆçš„å®Œæ•´æ•…äº‹ï¼Œæˆ–éŒ¯èª¤è¨Šæ¯
    """
    # æª¢æŸ¥ API é€£æ¥ç‹€æ…‹
    if not self.is_api_connected():
        return "Stima API not configured. Please check your API key."
    
    # æª¢æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
    if model_key not in self.models:
        return f"Model {model_key} not available"
    
    config = self.models[model_key]
    
    # æ ¹æ“šé•·åº¦è¨­å®šå­—æ•¸å’Œ token é™åˆ¶
    length_settings = {
        "short": {
            "words": "500-800",
            "max_tokens": 1500,
            "description": "ç°¡çŸ­ç²¾ç…‰"
        },
        "medium": {
            "words": "1000-1500", 
            "max_tokens": 2500,
            "description": "ä¸­ç­‰é•·åº¦"
        },
        "long": {
            "words": "1500-2500",
            "max_tokens": 4000,
            "description": "è¼ƒé•·ç¯‡å¹…"
        }
    }
    
    setting = length_settings.get(length, length_settings["medium"])
    
    # é¢¨æ ¼å°æ‡‰çš„ä¸­æ–‡æè¿°
    style_map = {
        "narrative": "æ•˜äº‹",
        "mystery": "æ‡¸ç–‘",
        "sci-fi": "ç§‘å¹»",
        "fantasy": "å¥‡å¹»",
        "romance": "æµªæ¼«",
        "thriller": "é©šæ‚š",
        "comedy": "å–œåŠ‡",
        "drama": "æˆ²åŠ‡",
        "horror": "ææ€–",
        "adventure": "å†’éšª"
    }
    
    chinese_style = style_map.get(style.lower(), style)
    
    # æ§‹å»ºæ•…äº‹ç”Ÿæˆçš„æç¤ºè©
    prompt = f"""æ ¹æ“šä»¥ä¸‹æƒ…ç¯€ï¼š{plot}
    
    è«‹ä»¥{chinese_style}é¢¨æ ¼æ’°å¯«ä¸€ç¯‡{setting['description']}çš„çŸ­ç¯‡æ•…äº‹ã€‚
    
    è¦æ±‚ï¼š
    1. å­—æ•¸ç´„ {setting['words']} å­—
    2. åŒ…å«ç”Ÿå‹•çš„å ´æ™¯æå¯«å’Œç´°è†©çš„å¿ƒç†åˆ»ç•«
    3. è§’è‰²æ€§æ ¼é®®æ˜ï¼Œæœ‰é©ç•¶çš„ç™¼å±•è®ŠåŒ–
    4. æƒ…ç¯€èµ·æ‰¿è½‰åˆï¼Œçµæ§‹å®Œæ•´
    5. çµå±€ä»¤äººæ»¿æ„ï¼Œç•™æœ‰å›å‘³ç©ºé–“
    
    è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡æ›¸å¯«ï¼Œæ–‡ç­†æµæš¢å„ªç¾ã€‚
    """
    
    try:
        # èª¿ç”¨ OpenAI API ç”Ÿæˆæ•…äº‹
        response = self.client.chat.completions.create(
            model=config.name,
            messages=[
                {
                    "role": "system", 
                    "content": f"æ‚¨æ˜¯ä¸€ä½å°ˆæ¥­çš„{chinese_style}å°èªªä½œå®¶ï¼Œæ“…é•·ç”¨ç¹é«”ä¸­æ–‡å‰µä½œå¼•äººå…¥å‹çš„æ•…äº‹ã€‚æ‚¨çš„æ–‡ç­†ç´°è†©ï¼Œå–„æ–¼ç‡Ÿé€ æ°›åœï¼Œèƒ½å¤ è®“è®€è€…èº«è‡¨å…¶å¢ƒã€‚"
                },
                {
                    "role": "user", 
                    "content": prompt
                }
            ],
            max_tokens=setting['max_tokens'],  # æ ¹æ“šé•·åº¦å‹•æ…‹è¨­å®š
            temperature=0.7,                   # é©ä¸­çš„å‰µé€ æ€§
            top_p=0.9,                         # å¢åŠ å¤šæ¨£æ€§
            frequency_penalty=0.3,              # æ¸›å°‘é‡è¤‡
            presence_penalty=0.3                # é¼“å‹µæ–°è©±é¡Œ
        )
        
        story = response.choices[0].message.content
        
        # æª¢æŸ¥æ˜¯å¦å›  token é™åˆ¶è€Œè¢«æˆªæ–·
        if response.choices[0].finish_reason == "length":
            story += "\n\n[è¨»ï¼šç”±æ–¼é•·åº¦é™åˆ¶ï¼Œæ•…äº‹å¯èƒ½æœªå®Œæ•´å‘ˆç¾]"
        
        return story
        
    except Exception as e:
        return f"Error with {config.display_name}: {str(e)}"
```

## ğŸ”§ å¦‚æœæ‚¨çœŸçš„éœ€è¦é•·ç¯‡æ•…äº‹ï¼ˆ3000-8000å­—ï¼‰

å¯ä»¥è€ƒæ…®**åˆ†æ®µç”Ÿæˆ**çš„æ–¹å¼ï¼š

```python
def generate_long_story(self, model_key: str, plot: str, style: str = "narrative") -> str:
    """
    ç”Ÿæˆé•·ç¯‡æ•…äº‹ï¼ˆé€éåˆ†æ®µç”Ÿæˆï¼‰
    """
    if not self.is_api_connected():
        return "Stima API not configured. Please check your API key."
    
    if model_key not in self.models:
        return f"Model {model_key} not available"
    
    config = self.models[model_key]
    
    # å°‡æ•…äº‹åˆ†æˆå¤šå€‹éƒ¨åˆ†ç”Ÿæˆ
    story_parts = []
    
    # ç¬¬ä¸€éƒ¨åˆ†ï¼šé–‹é ­ï¼ˆèµ·ï¼‰
    part1_prompt = f"""æ ¹æ“šæƒ…ç¯€ï¼š{plot}
    è«‹æ’°å¯«æ•…äº‹çš„é–‹é ­éƒ¨åˆ†ï¼ˆèµ·ï¼‰ï¼Œç´„1500-2000å­—ã€‚
    åŒ…å«ï¼šèƒŒæ™¯ä»‹ç´¹ã€ä¸»è¦è§’è‰²ç™»å ´ã€åˆå§‹è¡çªã€‚
    ä½¿ç”¨{style}é¢¨æ ¼ï¼Œç¹é«”ä¸­æ–‡ã€‚"""
    
    # ç¬¬äºŒéƒ¨åˆ†ï¼šç™¼å±•ï¼ˆæ‰¿ï¼‰
    part2_prompt = f"""å»¶çºŒä¸Šæ–‡ï¼Œæ’°å¯«æ•…äº‹çš„ç™¼å±•éƒ¨åˆ†ï¼ˆæ‰¿ï¼‰ï¼Œç´„1500-2000å­—ã€‚
    æ·±åŒ–è¡çªï¼Œå±•é–‹æƒ…ç¯€ï¼Œè§’è‰²é—œä¿‚è®ŠåŒ–ã€‚"""
    
    # ç¬¬ä¸‰éƒ¨åˆ†ï¼šé«˜æ½®ï¼ˆè½‰ï¼‰
    part3_prompt = f"""å»¶çºŒä¸Šæ–‡ï¼Œæ’°å¯«æ•…äº‹çš„é«˜æ½®éƒ¨åˆ†ï¼ˆè½‰ï¼‰ï¼Œç´„1500-2000å­—ã€‚
    è¡çªé”åˆ°é ‚é»ï¼Œé—œéµè½‰æŠ˜å‡ºç¾ã€‚"""
    
    # ç¬¬å››éƒ¨åˆ†ï¼šçµå±€ï¼ˆåˆï¼‰
    part4_prompt = f"""å»¶çºŒä¸Šæ–‡ï¼Œæ’°å¯«æ•…äº‹çš„çµå±€éƒ¨åˆ†ï¼ˆåˆï¼‰ï¼Œç´„1500-2000å­—ã€‚
    è§£æ±ºè¡çªï¼Œæ”¶æŸæƒ…ç¯€ï¼Œç•™ä¸‹é¤˜éŸ»ã€‚"""
    
    prompts = [part1_prompt, part2_prompt, part3_prompt, part4_prompt]
    
    try:
        for i, prompt in enumerate(prompts):
            # ç‚ºå¾ŒçºŒéƒ¨åˆ†æä¾›å‰æ–‡æ‘˜è¦
            if i > 0:
                previous_summary = f"å‰æ–‡æ‘˜è¦ï¼š{story_parts[-1][:200]}..."
                prompt = f"{previous_summary}\n\n{prompt}"
            
            response = self.client.chat.completions.create(
                model=config.name,
                messages=[
                    {"role": "system", "content": f"æ‚¨æ˜¯{style}æ•…äº‹ä½œå®¶"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=3000,
                temperature=0.7
            )
            
            story_parts.append(response.choices[0].message.content)
        
        # åˆä½µæ‰€æœ‰éƒ¨åˆ†
        full_story = "\n\n".join(story_parts)
        return full_story
        
    except Exception as e:
        return f"Error: {str(e)}"
```

## ğŸ“ ä¸»ç¨‹å¼èª¿æ•´å»ºè­°

åœ¨ `app.py` ä¸­ï¼Œæ‚¨ä¹Ÿéœ€è¦ç›¸æ‡‰èª¿æ•´ï¼š

```python
# Tab 3 ä¸­çš„æ•…äº‹é•·åº¦é¸æ“‡
story_length = st.select_slider(
    "Story length:",
    options=["Short (500-800å­—)", "Medium (1000-1500å­—)", "Long (1500-2500å­—)"],
    value="Medium (1000-1500å­—)"
)

# æå–é•·åº¦é¡å‹
length_type = "short" if "Short" in story_length else "long" if "Long" in story_length else "medium"

# ç”Ÿæˆæ•…äº‹æ™‚å‚³å…¥é•·åº¦åƒæ•¸
story = st.session_state.llm_manager.generate_story(
    model_key, combined_plots, story_style.lower(), length_type
)
```

é€™æ¨£çš„è¨­å®šæœƒæ›´å¯¦éš›ä¸”ä¸æœƒè¶…é API é™åˆ¶ï¼


# 04

ç•¶ç„¶å¯ä»¥ï¼é€™æ˜¯å€‹å¾ˆå¥½çš„æƒ³æ³•ï¼Œè®“ä½¿ç”¨è€…å¯ä»¥æ ¹æ“šéœ€æ±‚é¸æ“‡ä¸åŒçš„ç”Ÿæˆæ–¹å¼ã€‚æˆ‘ä¾†å¹«æ‚¨æ•´åˆå…©ç¨®æ–¹æ³•ï¼š

## ğŸ“ æ›´æ–°çš„ `llm_manager.py`

```python
def generate_story(self, model_key: str, plot: str, style: str = "narrative", length: str = "medium") -> str:
    """
    æ ¹æ“šæƒ…ç¯€ç”Ÿæˆå®Œæ•´æ•…äº‹ï¼ˆæ¨™æº–ç‰ˆæœ¬ï¼‰
    
    Args:
        model_key (str): æ¨¡å‹çš„ key
        plot (str): æ•…äº‹æƒ…ç¯€
        style (str): æ•…äº‹é¢¨æ ¼ï¼ˆé è¨­ç‚º narrativeï¼‰
        length (str): æ•…äº‹é•·åº¦ ("short", "medium", "long")
        
    Returns:
        str: ç”Ÿæˆçš„å®Œæ•´æ•…äº‹ï¼Œæˆ–éŒ¯èª¤è¨Šæ¯
    """
    # æª¢æŸ¥ API é€£æ¥ç‹€æ…‹
    if not self.is_api_connected():
        return "Stima API not configured. Please check your API key."
    
    # æª¢æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
    if model_key not in self.models:
        return f"Model {model_key} not available"
    
    config = self.models[model_key]
    
    # æ ¹æ“šé•·åº¦è¨­å®šå­—æ•¸å’Œ token é™åˆ¶
    length_settings = {
        "short": {
            "words": "500-800",
            "max_tokens": 1500,
            "description": "ç°¡çŸ­ç²¾ç…‰"
        },
        "medium": {
            "words": "1000-1500", 
            "max_tokens": 2500,
            "description": "ä¸­ç­‰é•·åº¦"
        },
        "long": {
            "words": "1500-2500",
            "max_tokens": 4000,
            "description": "è¼ƒé•·ç¯‡å¹…"
        }
    }
    
    setting = length_settings.get(length, length_settings["medium"])
    
    # é¢¨æ ¼å°æ‡‰çš„ä¸­æ–‡æè¿°
    style_map = {
        "narrative": "æ•˜äº‹",
        "mystery": "æ‡¸ç–‘",
        "sci-fi": "ç§‘å¹»",
        "fantasy": "å¥‡å¹»",
        "romance": "æµªæ¼«",
        "thriller": "é©šæ‚š",
        "comedy": "å–œåŠ‡",
        "drama": "æˆ²åŠ‡",
        "horror": "ææ€–",
        "adventure": "å†’éšª"
    }
    
    chinese_style = style_map.get(style.lower(), style)
    
    # æ§‹å»ºæ•…äº‹ç”Ÿæˆçš„æç¤ºè©
    prompt = f"""æ ¹æ“šä»¥ä¸‹æƒ…ç¯€ï¼š{plot}
    
    è«‹ä»¥{chinese_style}é¢¨æ ¼æ’°å¯«ä¸€ç¯‡{setting['description']}çš„çŸ­ç¯‡æ•…äº‹ã€‚
    
    è¦æ±‚ï¼š
    1. å­—æ•¸ç´„ {setting['words']} å­—
    2. åŒ…å«ç”Ÿå‹•çš„å ´æ™¯æå¯«å’Œç´°è†©çš„å¿ƒç†åˆ»ç•«
    3. è§’è‰²æ€§æ ¼é®®æ˜ï¼Œæœ‰é©ç•¶çš„ç™¼å±•è®ŠåŒ–
    4. æƒ…ç¯€èµ·æ‰¿è½‰åˆï¼Œçµæ§‹å®Œæ•´
    5. çµå±€ä»¤äººæ»¿æ„ï¼Œç•™æœ‰å›å‘³ç©ºé–“
    
    è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡æ›¸å¯«ï¼Œæ–‡ç­†æµæš¢å„ªç¾ã€‚
    """
    
    try:
        # èª¿ç”¨ OpenAI API ç”Ÿæˆæ•…äº‹
        response = self.client.chat.completions.create(
            model=config.name,
            messages=[
                {
                    "role": "system", 
                    "content": f"æ‚¨æ˜¯ä¸€ä½å°ˆæ¥­çš„{chinese_style}å°èªªä½œå®¶ï¼Œæ“…é•·ç”¨ç¹é«”ä¸­æ–‡å‰µä½œå¼•äººå…¥å‹çš„æ•…äº‹ã€‚æ‚¨çš„æ–‡ç­†ç´°è†©ï¼Œå–„æ–¼ç‡Ÿé€ æ°›åœï¼Œèƒ½å¤ è®“è®€è€…èº«è‡¨å…¶å¢ƒã€‚"
                },
                {
                    "role": "user", 
                    "content": prompt
                }
            ],
            max_tokens=setting['max_tokens'],  # æ ¹æ“šé•·åº¦å‹•æ…‹è¨­å®š
            temperature=0.7,                   # é©ä¸­çš„å‰µé€ æ€§
            top_p=0.9,                         # å¢åŠ å¤šæ¨£æ€§
            frequency_penalty=0.3,              # æ¸›å°‘é‡è¤‡
            presence_penalty=0.3                # é¼“å‹µæ–°è©±é¡Œ
        )
        
        story = response.choices[0].message.content
        
        # æª¢æŸ¥æ˜¯å¦å›  token é™åˆ¶è€Œè¢«æˆªæ–·
        if response.choices[0].finish_reason == "length":
            story += "\n\n[è¨»ï¼šç”±æ–¼é•·åº¦é™åˆ¶ï¼Œæ•…äº‹å¯èƒ½æœªå®Œæ•´å‘ˆç¾]"
        
        return story
        
    except Exception as e:
        return f"Error with {config.display_name}: {str(e)}"

def generate_long_story(self, model_key: str, plot: str, style: str = "narrative", 
                       progress_callback=None) -> str:
    """
    ç”Ÿæˆè¶…é•·ç¯‡æ•…äº‹ï¼ˆé€éåˆ†æ®µç”Ÿæˆï¼Œ3000-8000å­—ï¼‰
    
    Args:
        model_key (str): æ¨¡å‹çš„ key
        plot (str): æ•…äº‹æƒ…ç¯€
        style (str): æ•…äº‹é¢¨æ ¼
        progress_callback: é€²åº¦å›èª¿å‡½æ•¸ï¼ˆç”¨æ–¼æ›´æ–°UIï¼‰
        
    Returns:
        str: ç”Ÿæˆçš„é•·ç¯‡æ•…äº‹
    """
    # æª¢æŸ¥ API é€£æ¥ç‹€æ…‹
    if not self.is_api_connected():
        return "Stima API not configured. Please check your API key."
    
    if model_key not in self.models:
        return f"Model {model_key} not available"
    
    config = self.models[model_key]
    
    # é¢¨æ ¼å°æ‡‰çš„ä¸­æ–‡æè¿°
    style_map = {
        "narrative": "æ•˜äº‹",
        "mystery": "æ‡¸ç–‘",
        "sci-fi": "ç§‘å¹»",
        "fantasy": "å¥‡å¹»",
        "romance": "æµªæ¼«",
        "thriller": "é©šæ‚š",
        "comedy": "å–œåŠ‡",
        "drama": "æˆ²åŠ‡",
        "horror": "ææ€–",
        "adventure": "å†’éšª"
    }
    
    chinese_style = style_map.get(style.lower(), style)
    
    # å°‡æ•…äº‹åˆ†æˆå››å€‹éƒ¨åˆ†ç”Ÿæˆ
    story_parts = []
    part_names = ["èµ·ï¼ˆé–‹ç«¯ï¼‰", "æ‰¿ï¼ˆç™¼å±•ï¼‰", "è½‰ï¼ˆé«˜æ½®ï¼‰", "åˆï¼ˆçµå±€ï¼‰"]
    
    # å®šç¾©æ¯å€‹éƒ¨åˆ†çš„æç¤ºè©
    prompts = [
        # ç¬¬ä¸€éƒ¨åˆ†ï¼šèµ·
        f"""æ ¹æ“šæƒ…ç¯€ï¼š{plot}
        
        è«‹æ’°å¯«ä¸€ç¯‡{chinese_style}é¢¨æ ¼é•·ç¯‡æ•…äº‹çš„ç¬¬ä¸€éƒ¨åˆ†ã€èµ·ã€‘ã€‚
        
        é€™éƒ¨åˆ†éœ€è¦åŒ…å«ï¼š
        1. æ•…äº‹èƒŒæ™¯çš„è©³ç´°ä»‹ç´¹
        2. ä¸»è¦è§’è‰²çš„ç™»å ´å’Œæ€§æ ¼åˆ»ç•«
        3. åˆå§‹æƒ…æ³çš„å»ºç«‹
        4. å¼•ç™¼æ•…äº‹çš„äº‹ä»¶æˆ–è¡çª
        
        å­—æ•¸è¦æ±‚ï¼š1500-2000å­—
        è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œæ–‡ç­†ç”Ÿå‹•ç´°è†©ã€‚""",
        
        # ç¬¬äºŒéƒ¨åˆ†ï¼šæ‰¿
        f"""è«‹å»¶çºŒå‰æ–‡ï¼Œæ’°å¯«æ•…äº‹çš„ç¬¬äºŒéƒ¨åˆ†ã€æ‰¿ã€‘ã€‚
        
        é€™éƒ¨åˆ†éœ€è¦åŒ…å«ï¼š
        1. æƒ…ç¯€çš„æ·±å…¥ç™¼å±•
        2. è§’è‰²é—œä¿‚çš„è®ŠåŒ–
        3. è¡çªçš„é€æ­¥å‡ç´š
        4. æ–°çš„æŒ‘æˆ°æˆ–ç™¼ç¾
        
        å­—æ•¸è¦æ±‚ï¼š1500-2000å­—
        ä¿æŒ{chinese_style}é¢¨æ ¼ï¼Œç¹é«”ä¸­æ–‡ã€‚""",
        
        # ç¬¬ä¸‰éƒ¨åˆ†ï¼šè½‰
        f"""è«‹å»¶çºŒå‰æ–‡ï¼Œæ’°å¯«æ•…äº‹çš„ç¬¬ä¸‰éƒ¨åˆ†ã€è½‰ã€‘ã€‚
        
        é€™éƒ¨åˆ†éœ€è¦åŒ…å«ï¼š
        1. æ•…äº‹é”åˆ°é«˜æ½®
        2. é—œéµçš„è½‰æŠ˜é»
        3. é‡å¤§çœŸç›¸çš„æ­éœ²
        4. è§’è‰²çš„é‡è¦æ±ºå®š
        
        å­—æ•¸è¦æ±‚ï¼š1500-2000å­—
        ä¿æŒ{chinese_style}é¢¨æ ¼ï¼Œç¹é«”ä¸­æ–‡ã€‚""",
        
        # ç¬¬å››éƒ¨åˆ†ï¼šåˆ
        f"""è«‹å»¶çºŒå‰æ–‡ï¼Œæ’°å¯«æ•…äº‹çš„æœ€å¾Œéƒ¨åˆ†ã€åˆã€‘ã€‚
        
        é€™éƒ¨åˆ†éœ€è¦åŒ…å«ï¼š
        1. è¡çªçš„è§£æ±º
        2. æ•…äº‹ç·šçš„æ”¶æŸ
        3. è§’è‰²çš„æˆé•·æˆ–æ”¹è®Š
        4. æ„å‘³æ·±é•·çš„çµå±€
        
        å­—æ•¸è¦æ±‚ï¼š1500-2000å­—
        ä¿æŒ{chinese_style}é¢¨æ ¼ï¼Œç¹é«”ä¸­æ–‡ã€‚"""
    ]
    
    try:
        context = ""  # ç”¨æ–¼ä¿å­˜å‰æ–‡æ‘˜è¦
        
        for i, (prompt, part_name) in enumerate(zip(prompts, part_names)):
            # æ›´æ–°é€²åº¦ï¼ˆå¦‚æœæœ‰å›èª¿å‡½æ•¸ï¼‰
            if progress_callback:
                progress_callback(f"æ­£åœ¨ç”Ÿæˆï¼š{part_name}", (i + 1) / 4)
            
            # ç‚ºå¾ŒçºŒéƒ¨åˆ†æ·»åŠ å‰æ–‡æ‘˜è¦
            if i > 0 and context:
                full_prompt = f"""å‰æ–‡æ‘˜è¦ï¼š
{context}

{prompt}"""
            else:
                full_prompt = prompt
            
            # ç”Ÿæˆç•¶å‰éƒ¨åˆ†
            response = self.client.chat.completions.create(
                model=config.name,
                messages=[
                    {
                        "role": "system", 
                        "content": f"""æ‚¨æ˜¯ä¸€ä½å°ˆæ¥­çš„{chinese_style}é•·ç¯‡å°èªªä½œå®¶ã€‚
æ‚¨æ“…é•·å‰µä½œçµæ§‹å®Œæ•´ã€æƒ…ç¯€ç·Šæ¹Šçš„æ•…äº‹ã€‚
è«‹ç¢ºä¿æ¯å€‹éƒ¨åˆ†éƒ½èˆ‡å‰æ–‡ç·Šå¯†éŠœæ¥ï¼Œä¿æŒé¢¨æ ¼ä¸€è‡´ã€‚"""
                    },
                    {
                        "role": "user", 
                        "content": full_prompt
                    }
                ],
                max_tokens=3000,        # æ¯éƒ¨åˆ†çš„æœ€å¤§ token æ•¸
                temperature=0.7,
                top_p=0.9,
                frequency_penalty=0.3,
                presence_penalty=0.3
            )
            
            part_content = response.choices[0].message.content
            story_parts.append(f"\n\nã€{part_name}ã€‘\n\n{part_content}")
            
            # ç”Ÿæˆç•¶å‰éƒ¨åˆ†çš„æ‘˜è¦ä½œç‚ºä¸‹ä¸€éƒ¨åˆ†çš„ä¸Šä¸‹æ–‡
            if i < 3:  # ä¸æ˜¯æœ€å¾Œä¸€éƒ¨åˆ†
                summary_response = self.client.chat.completions.create(
                    model=config.name,
                    messages=[
                        {
                            "role": "system", 
                            "content": "è«‹ç°¡è¦ç¸½çµæ•…äº‹å…§å®¹ï¼Œä¿ç•™é—œéµæƒ…ç¯€å’Œè§’è‰²ç‹€æ…‹ï¼Œç´„100-150å­—ã€‚"
                        },
                        {
                            "role": "user", 
                            "content": f"è«‹ç¸½çµä»¥ä¸‹å…§å®¹ï¼š\n{part_content}"
                        }
                    ],
                    max_tokens=300,
                    temperature=0.3
                )
                context = summary_response.choices[0].message.content
        
        # åˆä½µæ‰€æœ‰éƒ¨åˆ†
        full_story = "".join(story_parts)
        
        # æ·»åŠ å­—æ•¸çµ±è¨ˆ
        word_count = len(full_story.replace(" ", "").replace("\n", ""))
        full_story = f"ã€æ•…äº‹å…¨æ–‡ã€‘ï¼ˆå…± {word_count} å­—ï¼‰\n{full_story}"
        
        return full_story
        
    except Exception as e:
        return f"Error generating long story with {config.display_name}: {str(e)}"
```

## ğŸ“ æ›´æ–°çš„ `app.py` (Tab 3 éƒ¨åˆ†)

```python
# ========== Tab 3: ç”Ÿæˆå®Œæ•´æ•…äº‹ ==========
with tab3:
    st.header("Step 3: Generate Complete Story")
    
    if st.session_state.selected_plots:
        st.subheader("ğŸ“‹ Selected Plots")
        
        # é¡¯ç¤ºå·²é¸æ“‡çš„æƒ…ç¯€ï¼ˆå¯å±•é–‹ï¼‰
        with st.expander("View selected plots", expanded=False):
            for idx, plot in enumerate(st.session_state.selected_plots):
                st.write(f"{idx+1}. **{plot['model']}**: {plot['plot']}")
        
        st.divider()
        
        # ========== ç”Ÿæˆæ¨¡å¼é¸æ“‡ ==========
        generation_mode = st.radio(
            "é¸æ“‡ç”Ÿæˆæ¨¡å¼ Generation Mode:",
            options=["æ¨™æº–æ¨¡å¼ Standard", "é•·ç¯‡æ¨¡å¼ Extended"],
            horizontal=True,
            help="æ¨™æº–æ¨¡å¼ï¼š500-2500å­— | é•·ç¯‡æ¨¡å¼ï¼š3000-8000å­—ï¼ˆåˆ†æ®µç”Ÿæˆï¼‰"
        )
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            # é¸æ“‡ç”¨æ–¼ç”Ÿæˆæ•…äº‹çš„æ¨¡å‹
            if available_models:
                story_model_options = {model['display_name']: model['key'] 
                                     for model in available_models}
                
                # æ ¹æ“šæ¨¡å¼èª¿æ•´å¯é¸æ¨¡å‹æ•¸é‡
                max_models = 3 if generation_mode == "æ¨™æº–æ¨¡å¼ Standard" else 1
                
                selected_story_displays = st.multiselect(
                    f"Select models for story generation (max {max_models}):",
                    list(story_model_options.keys()),
                    default=list(story_model_options.keys())[:min(1, len(story_model_options))],
                    max_selections=max_models
                )
                
                story_models = [story_model_options[name] for name in selected_story_displays]
            
            # æ•…äº‹é¢¨æ ¼é¸æ“‡
            story_style = st.selectbox(
                "Story style æ•…äº‹é¢¨æ ¼:",
                ["Narrative æ•˜äº‹", "Mystery æ‡¸ç–‘", "Sci-Fi ç§‘å¹»", 
                 "Fantasy å¥‡å¹»", "Romance æµªæ¼«", "Thriller é©šæ‚š", 
                 "Comedy å–œåŠ‡", "Drama æˆ²åŠ‡", "Horror ææ€–", "Adventure å†’éšª"]
            )
            # æå–è‹±æ–‡é¢¨æ ¼åç¨±
            style_english = story_style.split()[0].lower()
            
            # æ¨™æº–æ¨¡å¼çš„é•·åº¦é¸æ“‡
            if generation_mode == "æ¨™æº–æ¨¡å¼ Standard":
                story_length = st.select_slider(
                    "Story length æ•…äº‹é•·åº¦:",
                    options=["Short çŸ­ç¯‡ (500-800å­—)", 
                            "Medium ä¸­ç¯‡ (1000-1500å­—)", 
                            "Long é•·ç¯‡ (1500-2500å­—)"],
                    value="Medium ä¸­ç¯‡ (1000-1500å­—)"
                )
                # æå–é•·åº¦é¡å‹
                if "Short" in story_length:
                    length_type = "short"
                elif "Long" in story_length:
                    length_type = "long"
                else:
                    length_type = "medium"
            else:
                st.info("é•·ç¯‡æ¨¡å¼å°‡ç”Ÿæˆ 3000-8000 å­—çš„å®Œæ•´æ•…äº‹ï¼Œåˆ†ç‚ºèµ·æ‰¿è½‰åˆå››å€‹éƒ¨åˆ†ã€‚")
        
        with col2:
            if generation_mode == "æ¨™æº–æ¨¡å¼ Standard":
                st.info("""
                **ğŸ“– æ¨™æº–æ¨¡å¼ï¼š**
                - å¤šå€‹æ¨¡å‹åŒæ™‚ç”Ÿæˆ
                - å¿«é€Ÿç”Ÿæˆ
                - é©åˆçŸ­ä¸­ç¯‡æ•…äº‹
                - å¯ä¸‹è¼‰ç‚º Markdown
                """)
            else:
                st.info("""
                **ğŸ“š é•·ç¯‡æ¨¡å¼ï¼š**
                - å–®ä¸€æ¨¡å‹åˆ†æ®µç”Ÿæˆ
                - èµ·æ‰¿è½‰åˆå››éƒ¨åˆ†
                - 3000-8000å­—é•·ç¯‡
                - ä¿æŒæƒ…ç¯€é€£è²«æ€§
                """)
        
        # ========== ç”Ÿæˆæ•…äº‹æŒ‰éˆ• ==========
        if st.button("âœ¨ Generate Stories", type="primary"):
            # åˆä½µæ‰€æœ‰é¸å®šçš„æƒ…ç¯€
            combined_plots = "\n".join([f"- {p['plot']}" for p in st.session_state.selected_plots])
            
            if generation_mode == "æ¨™æº–æ¨¡å¼ Standard":
                # ===== æ¨™æº–æ¨¡å¼ç”Ÿæˆ =====
                with st.spinner("Creating your stories..."):
                    stories = []
                    progress = st.progress(0)
                    
                    for idx, model_key in enumerate(story_models):
                        progress.progress((idx + 1) / len(story_models))
                        
                        model_name = next(m['display_name'] for m in available_models 
                                        if m['key'] == model_key)
                        
                        st.info(f"Generating story with {model_name}...")
                        
                        # èª¿ç”¨æ¨™æº–ç”Ÿæˆæ–¹æ³•
                        story = st.session_state.llm_manager.generate_story(
                            model_key, combined_plots, style_english, length_type
                        )
                        
                        stories.append({
                            'model': model_name,
                            'story': story,
                            'metadata': {
                                'title': f"{story_style.split()[0]} Story",
                                'model': model_name,
                                'date': datetime.now().strftime("%Y-%m-%d %H:%M"),
                                'style': story_style,
                                'length': story_length,
                                'based_on_plots': len(st.session_state.selected_plots)
                            }
                        })
                    
                    progress.empty()
                    st.success("âœ… Stories generated successfully!")
                    
                    # é¡¯ç¤ºç”Ÿæˆçš„æ•…äº‹
                    for story_data in stories:
                        st.divider()
                        
                        col1, col2 = st.columns([3, 1])
                        with col1:
                            st.subheader(f"ğŸ“– {story_data['metadata']['title']} by {story_data['model']}")
                        
                        with col2:
                            markdown_content = f"""# {story_data['metadata']['title']}

**Generated by:** {story_data['metadata']['model']}  
**Date:** {story_data['metadata']['date']}  
**Style:** {story_data['metadata']['style']}  
**Length:** {story_data['metadata']['length']}  
**Based on:** {story_data['metadata']['based_on_plots']} selected plots  

---

{story_data['story']}

---

*Generated by PlotWeaver*  
*Powered by Stima API*
"""
                            
                            st.download_button(
                                label=f"ğŸ“¥ Download",
                                data=markdown_content,
                                file_name=f"story_{style_english}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md",
                                mime="text/markdown",
                                key=f"download_{story_data['model']}_{idx}"
                            )
                        
                        with st.container():
                            st.markdown(story_data['story'])
            
            else:
                # ===== é•·ç¯‡æ¨¡å¼ç”Ÿæˆ =====
                if len(story_models) > 0:
                    model_key = story_models[0]
                    model_name = next(m['display_name'] for m in available_models 
                                    if m['key'] == model_key)
                    
                    # å‰µå»ºé€²åº¦å®¹å™¨
                    progress_container = st.container()
                    with progress_container:
                        progress_bar = st.progress(0)
                        status_text = st.empty()
                        
                        # å®šç¾©é€²åº¦å›èª¿å‡½æ•¸
                        def update_progress(message, progress):
                            status_text.text(message)
                            progress_bar.progress(progress)
                        
                        # èª¿ç”¨é•·ç¯‡ç”Ÿæˆæ–¹æ³•
                        long_story = st.session_state.llm_manager.generate_long_story(
                            model_key, 
                            combined_plots, 
                            style_english,
                            progress_callback=update_progress
                        )
                        
                        progress_bar.empty()
                        status_text.empty()
                    
                    st.success("âœ… Long story generated successfully!")
                    
                    # é¡¯ç¤ºé•·ç¯‡æ•…äº‹
                    st.divider()
                    
                    col1, col2 = st.columns([3, 1])
                    with col1:
                        st.subheader(f"ğŸ“š Extended {story_style.split()[0]} Story by {model_name}")
                    
                    with col2:
                        markdown_content = f"""# Extended {story_style.split()[0]} Story

**Generated by:** {model_name}  
**Date:** {datetime.now().strftime("%Y-%m-%d %H:%M")}  
**Style:** {story_style}  
**Type:** Extended (3000-8000 words)  
**Based on:** {len(st.session_state.selected_plots)} selected plots  

---

{long_story}

---

*Generated by PlotWeaver - Extended Mode*  
*Powered by Stima API*
"""
                        
                        st.download_button(
                            label=f"ğŸ“¥ Download Extended",
                            data=markdown_content,
                            file_name=f"extended_story_{style_english}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md",
                            mime="text/markdown"
                        )
                    
                    with st.container():
                        st.markdown(long_story)
                else:
                    st.error("è«‹è‡³å°‘é¸æ“‡ä¸€å€‹æ¨¡å‹")
    
    else:
        st.info("âœ‹ Please select plots in Tab 2 first")
        
        if st.session_state.generated_plots:
            if st.button("ğŸš€ Quick Start - Select All Plots"):
                st.session_state.selected_plots = st.session_state.generated_plots.copy()
                st.rerun()
```

## ğŸ¯ ä¸»è¦ç‰¹é»

1. **é›™æ¨¡å¼é¸æ“‡**ï¼šä½¿ç”¨è€…å¯ä»¥é¸æ“‡æ¨™æº–æ¨¡å¼æˆ–é•·ç¯‡æ¨¡å¼
2. **æ¨™æº–æ¨¡å¼**ï¼šæ”¯æ´å¤šæ¨¡å‹ã€å¿«é€Ÿç”Ÿæˆã€500-2500å­—
3. **é•·ç¯‡æ¨¡å¼**ï¼šå–®ä¸€æ¨¡å‹ã€åˆ†æ®µç”Ÿæˆã€3000-8000å­—ã€é¡¯ç¤ºé€²åº¦
4. **æ™ºèƒ½æç¤º**ï¼šä¸åŒæ¨¡å¼æœ‰ä¸åŒçš„UIæç¤ºå’Œé™åˆ¶
5. **é€²åº¦é¡¯ç¤º**ï¼šé•·ç¯‡æ¨¡å¼æœƒé¡¯ç¤ºç”Ÿæˆé€²åº¦ï¼ˆèµ·æ‰¿è½‰åˆï¼‰
6. **ä¸‹è¼‰åŠŸèƒ½**ï¼šå…©ç¨®æ¨¡å¼éƒ½æ”¯æ´ Markdown æ ¼å¼ä¸‹è¼‰

é€™æ¨£çš„è¨­è¨ˆçµ¦ä½¿ç”¨è€…æœ€å¤§çš„å½ˆæ€§ï¼Œå¯ä»¥æ ¹æ“šéœ€æ±‚é¸æ“‡æœ€é©åˆçš„ç”Ÿæˆæ–¹å¼ï¼
