# 01


æˆ‘å¹«æ‚¨å‰µå»ºå°ˆæ¥­çš„ GitHub README æ–‡ä»¶ï¼š

## è‹±æ–‡ç‰ˆ README.md

```markdown
# Story Generator Hub

ğŸ­ A multi-model collaborative story generation platform powered by various LLMs

## ğŸ“– Overview

Story Generator Hub is an innovative platform that leverages multiple Large Language Models (LLMs) to generate diverse story plots, intelligently categorize them, and create complete stories based on user preferences.

## âœ¨ Features

- **Multi-Model Plot Generation**: Generate diverse story plots using up to 5 different LLM models simultaneously
- **Intelligent Clustering**: Automatically group similar plots using GPT for better organization
- **Interactive Selection**: Users can select preferred plot clusters to guide story generation
- **Collaborative Story Creation**: Generate complete stories using up to 3 models for richer narratives
- **Markdown Export**: All stories are exported in clean Markdown format
- **Random Model Selection**: Option to randomly select models for varied results

## ğŸš€ How It Works

### Phase 1: Plot Generation
1. Select up to 5 LLM models (or choose random selection)
2. Generate multiple story plots from each model
3. GPT analyzes and clusters similar plots automatically

### Phase 2: Plot Selection
1. Review categorized plot clusters
2. Select preferred plot groups
3. Confirm selections for story generation

### Phase 3: Story Generation
1. Choose up to 3 models for story generation (or random)
2. Generate complete stories based on selected plots
3. Export stories in Markdown format

## ğŸ› ï¸ Technology Stack

- **Frontend**: [Your frontend framework]
- **Backend**: [Your backend framework]
- **LLM Integration**: Multiple model APIs (GPT, Claude, Gemini, etc.)
- **Classification Engine**: GPT for plot clustering and categorization

## ğŸ“¦ Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/story-generator-hub.git

# Navigate to project directory
cd story-generator-hub

# Install dependencies
npm install

# Set up environment variables
cp .env.example .env
# Add your API keys to .env

# Start the application
npm start
```

## ğŸ”§ Configuration

Create a `.env` file with the following variables:

```env
OPENAI_API_KEY=your_openai_key
ANTHROPIC_API_KEY=your_anthropic_key
GOOGLE_AI_KEY=your_google_key
# Add other model API keys as needed
```

## ğŸ“ Usage

1. **Initial Setup**: Configure your preferred LLM models in settings
2. **Generate Plots**: Click "Generate Plots" and select your models
3. **Review & Select**: Browse categorized plots and select interesting ones
4. **Generate Story**: Choose models and generate your complete story
5. **Export**: Download your story in Markdown format

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Thanks to all LLM providers for their APIs
- Community contributors and testers
- Open source libraries used in this project

## ğŸ“§ Contact

- Project Link: [https://github.com/yourusername/story-generator-hub](https://github.com/yourusername/story-generator-hub)
- Issues: [https://github.com/yourusername/story-generator-hub/issues](https://github.com/yourusername/story-generator-hub/issues)
```

## ä¸­æ–‡ç‰ˆ README_zh-TW.md

```markdown
# Story Generator Hub æ•…äº‹ç”Ÿæˆå™¨ä¸­å¿ƒ

ğŸ­ åŸºæ–¼å¤šå€‹å¤§å‹èªè¨€æ¨¡å‹å”ä½œçš„æ•…äº‹ç”Ÿæˆå¹³å°

## ğŸ“– å°ˆæ¡ˆç°¡ä»‹

Story Generator Hub æ˜¯ä¸€å€‹å‰µæ–°çš„æ•…äº‹ç”Ÿæˆå¹³å°ï¼Œé€éæ•´åˆå¤šå€‹å¤§å‹èªè¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¾†ç”Ÿæˆå¤šæ¨£åŒ–çš„æ•…äº‹æƒ…ç¯€ï¼Œä¸¦æ™ºæ…§åˆ†é¡æ­¸ç´ï¼Œæœ€çµ‚æ ¹æ“šä½¿ç”¨è€…åå¥½å‰µä½œå®Œæ•´çš„æ•…äº‹ã€‚

## âœ¨ æ ¸å¿ƒåŠŸèƒ½

- **å¤šæ¨¡å‹æƒ…ç¯€ç”Ÿæˆ**ï¼šåŒæ™‚ä½¿ç”¨æœ€å¤š 5 å€‹ä¸åŒçš„ LLM æ¨¡å‹ç”Ÿæˆå¤šæ¨£åŒ–çš„æ•…äº‹æƒ…ç¯€
- **æ™ºæ…§åˆ†é¡æ­¸ç´**ï¼šä½¿ç”¨ GPT è‡ªå‹•å°‡ç›¸ä¼¼æƒ…ç¯€åˆ†çµ„ï¼Œä¾¿æ–¼ç®¡ç†å’Œé¸æ“‡
- **äº’å‹•å¼é¸æ“‡**ï¼šä½¿ç”¨è€…å¯å‹¾é¸æ„Ÿèˆˆè¶£çš„æƒ…ç¯€ç¾¤çµ„ä¾†å¼•å°æ•…äº‹ç”Ÿæˆ
- **å”ä½œå¼æ•…äº‹å‰µä½œ**ï¼šä½¿ç”¨æœ€å¤š 3 å€‹æ¨¡å‹ç”Ÿæˆå®Œæ•´æ•…äº‹ï¼Œå‰µé€ æ›´è±å¯Œçš„æ•˜äº‹
- **Markdown è¼¸å‡º**ï¼šæ‰€æœ‰æ•…äº‹çš†ä»¥ç°¡æ½”çš„ Markdown æ ¼å¼åŒ¯å‡º
- **éš¨æ©Ÿæ¨¡å‹é¸æ“‡**ï¼šæä¾›éš¨æ©Ÿé¸æ“‡æ¨¡å‹é¸é …ï¼Œå¢åŠ çµæœå¤šæ¨£æ€§

## ğŸš€ é‹ä½œæµç¨‹

### ç¬¬ä¸€éšæ®µï¼šæƒ…ç¯€ç”Ÿæˆ
1. é¸æ“‡æœ€å¤š 5 å€‹ LLM æ¨¡å‹ï¼ˆæˆ–é¸æ“‡éš¨æ©Ÿï¼‰
2. æ¯å€‹æ¨¡å‹ç”Ÿæˆå¤šå€‹æ•…äº‹æƒ…ç¯€
3. GPT è‡ªå‹•åˆ†æä¸¦å°‡ç›¸ä¼¼æƒ…ç¯€åˆ†é¡

### ç¬¬äºŒéšæ®µï¼šæƒ…ç¯€ç¯©é¸
1. ç€è¦½åˆ†é¡å¾Œçš„æƒ…ç¯€ç¾¤çµ„
2. å‹¾é¸åå¥½çš„æƒ…ç¯€ç¾¤çµ„
3. ç¢ºèªé¸æ“‡ä»¥é€²è¡Œæ•…äº‹ç”Ÿæˆ

### ç¬¬ä¸‰éšæ®µï¼šæ•…äº‹ç”Ÿæˆ
1. é¸æ“‡æœ€å¤š 3 å€‹æ¨¡å‹é€²è¡Œæ•…äº‹ç”Ÿæˆï¼ˆæˆ–éš¨æ©Ÿï¼‰
2. åŸºæ–¼é¸å®šæƒ…ç¯€ç”Ÿæˆå®Œæ•´æ•…äº‹
3. ä»¥ Markdown æ ¼å¼åŒ¯å‡ºæ•…äº‹

## ğŸ› ï¸ æŠ€è¡“æ¶æ§‹

- **å‰ç«¯æ¡†æ¶**ï¼š[æ‚¨çš„å‰ç«¯æ¡†æ¶]
- **å¾Œç«¯æ¡†æ¶**ï¼š[æ‚¨çš„å¾Œç«¯æ¡†æ¶]
- **LLM æ•´åˆ**ï¼šå¤šå€‹æ¨¡å‹ APIï¼ˆGPTã€Claudeã€Gemini ç­‰ï¼‰
- **åˆ†é¡å¼•æ“**ï¼šGPT è² è²¬æƒ…ç¯€åˆ†é¡å’Œæ­¸ç´

## ğŸ“¦ å®‰è£æŒ‡å—

```bash
# è¤‡è£½å°ˆæ¡ˆ
git clone https://github.com/yourusername/story-generator-hub.git

# é€²å…¥å°ˆæ¡ˆç›®éŒ„
cd story-generator-hub

# å®‰è£ä¾è³´å¥—ä»¶
npm install

# è¨­å®šç’°å¢ƒè®Šæ•¸
cp .env.example .env
# åœ¨ .env ä¸­åŠ å…¥æ‚¨çš„ API é‡‘é‘°

# å•Ÿå‹•æ‡‰ç”¨ç¨‹å¼
npm start
```

## ğŸ”§ ç’°å¢ƒè¨­å®š

å»ºç«‹ `.env` æª”æ¡ˆä¸¦åŠ å…¥ä»¥ä¸‹è®Šæ•¸ï¼š

```env
OPENAI_API_KEY=your_openai_key
ANTHROPIC_API_KEY=your_anthropic_key
GOOGLE_AI_KEY=your_google_key
# æ ¹æ“šéœ€è¦åŠ å…¥å…¶ä»–æ¨¡å‹çš„ API é‡‘é‘°
```

## ğŸ“ ä½¿ç”¨èªªæ˜

1. **åˆå§‹è¨­å®š**ï¼šåœ¨è¨­å®šä¸­é…ç½®æ‚¨åå¥½çš„ LLM æ¨¡å‹
2. **ç”Ÿæˆæƒ…ç¯€**ï¼šé»æ“Šã€Œç”Ÿæˆæƒ…ç¯€ã€ä¸¦é¸æ“‡æ¨¡å‹
3. **æª¢è¦–èˆ‡é¸æ“‡**ï¼šç€è¦½åˆ†é¡å¾Œçš„æƒ…ç¯€ä¸¦å‹¾é¸æ„Ÿèˆˆè¶£çš„é …ç›®
4. **ç”Ÿæˆæ•…äº‹**ï¼šé¸æ“‡æ¨¡å‹ä¸¦ç”Ÿæˆå®Œæ•´æ•…äº‹
5. **åŒ¯å‡º**ï¼šä»¥ Markdown æ ¼å¼ä¸‹è¼‰æ‚¨çš„æ•…äº‹

## ğŸ¤ è²¢ç»æŒ‡å—

æ­¡è¿è²¢ç»ï¼è©³ç´°è³‡è¨Šè«‹åƒé–± [è²¢ç»æŒ‡å—](CONTRIBUTING.md)ã€‚

## ğŸ“„ æˆæ¬Šæ¢æ¬¾

æœ¬å°ˆæ¡ˆæ¡ç”¨ MIT æˆæ¬Šæ¢æ¬¾ - è©³è¦‹ [LICENSE](LICENSE) æª”æ¡ˆã€‚

## ğŸ™ è‡´è¬

- æ„Ÿè¬æ‰€æœ‰ LLM æä¾›å•†çš„ API æ”¯æ´
- ç¤¾ç¾¤è²¢ç»è€…å’Œæ¸¬è©¦äººå“¡
- æœ¬å°ˆæ¡ˆä½¿ç”¨çš„é–‹æºå‡½å¼åº«

## ğŸ“§ è¯çµ¡æ–¹å¼

- å°ˆæ¡ˆé€£çµï¼š[https://github.com/yourusername/story-generator-hub](https://github.com/yourusername/story-generator-hub)
- å•é¡Œå›å ±ï¼š[https://github.com/yourusername/story-generator-hub/issues](https://github.com/yourusername/story-generator-hub/issues)

é€™å…©å€‹ç‰ˆæœ¬çš„ README éƒ½åŒ…å«äº†æ‚¨å°ˆæ¡ˆçš„æ ¸å¿ƒæ¦‚å¿µï¼Œä¸¦ä»¥å°ˆæ¥­çš„æ ¼å¼å‘ˆç¾ã€‚æ‚¨å¯ä»¥æ ¹æ“šå¯¦éš›çš„æŠ€è¡“é¸æ“‡å’Œå°ˆæ¡ˆåç¨±é€²è¡Œèª¿æ•´ã€‚éœ€è¦æˆ‘å”åŠ©æ‚¨æ·»åŠ å…¶ä»–ç« ç¯€æˆ–èª¿æ•´å…§å®¹å—ï¼Ÿ

# 02

æˆ‘å¹«æ‚¨è¨­è¨ˆä¸€å€‹æ¼¸é€²å¼çš„å¯¦ç¾æ–¹æ¡ˆï¼Œå¾æœ€ç°¡å–®å¯é‹è¡Œçš„ç‰ˆæœ¬é–‹å§‹ï¼š

## ç‰ˆæœ¬ 1: æœ€åŸºç¤ç‰ˆæœ¬ï¼ˆä½¿ç”¨ Streamlit + OpenAIï¼‰

### 1. é¦–å…ˆåœ¨ Codespace å»ºç«‹å°ˆæ¡ˆçµæ§‹

```bash
story-generator-hub/
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ utils/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ llm_manager.py
    â””â”€â”€ story_processor.py
```

### 2. `requirements.txt`

```txt
streamlit
openai
python-dotenv
anthropic
google-generativeai
pandas
```

### 3. `.env.example`

```env
OPENAI_API_KEY=your_key_here
ANTHROPIC_API_KEY=your_key_here
GOOGLE_API_KEY=your_key_here
```

### 4. `utils/llm_manager.py` - LLM ç®¡ç†å™¨

```python
import os
from typing import List, Dict, Optional
from dataclasses import dataclass
import openai
import anthropic
import google.generativeai as genai
from dotenv import load_dotenv

load_dotenv()

@dataclass
class ModelConfig:
    name: str
    provider: str
    available: bool
    
class LLMManager:
    def __init__(self):
        self.models = self._initialize_models()
        
    def _initialize_models(self) -> Dict[str, ModelConfig]:
        """æª¢æŸ¥å“ªäº›æ¨¡å‹å¯ç”¨"""
        models = {}
        
        # OpenAI
        if os.getenv('OPENAI_API_KEY'):
            openai.api_key = os.getenv('OPENAI_API_KEY')
            models['gpt-3.5-turbo'] = ModelConfig('GPT-3.5', 'openai', True)
            models['gpt-4'] = ModelConfig('GPT-4', 'openai', True)
        
        # Anthropic
        if os.getenv('ANTHROPIC_API_KEY'):
            self.anthropic_client = anthropic.Anthropic(
                api_key=os.getenv('ANTHROPIC_API_KEY')
            )
            models['claude-3-haiku'] = ModelConfig('Claude 3 Haiku', 'anthropic', True)
        
        # Google
        if os.getenv('GOOGLE_API_KEY'):
            genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))
            models['gemini-pro'] = ModelConfig('Gemini Pro', 'google', True)
            
        return models
    
    def get_available_models(self) -> List[str]:
        """ç²å–æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
        return [name for name, config in self.models.items() if config.available]
    
    def generate_plot(self, model_name: str, prompt: str) -> str:
        """ä½¿ç”¨æŒ‡å®šæ¨¡å‹ç”Ÿæˆæ•…äº‹æƒ…ç¯€"""
        if model_name not in self.models:
            return f"Model {model_name} not available"
        
        config = self.models[model_name]
        
        try:
            if config.provider == 'openai':
                response = openai.ChatCompletion.create(
                    model=model_name,
                    messages=[
                        {"role": "system", "content": "You are a creative story writer."},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=200
                )
                return response.choices[0].message.content
                
            elif config.provider == 'anthropic':
                message = self.anthropic_client.messages.create(
                    model=model_name,
                    max_tokens=200,
                    messages=[{"role": "user", "content": prompt}]
                )
                return message.content[0].text
                
            elif config.provider == 'google':
                model = genai.GenerativeModel(model_name)
                response = model.generate_content(prompt)
                return response.text
                
        except Exception as e:
            return f"Error with {model_name}: {str(e)}"
    
    def generate_story(self, model_name: str, plot: str, style: str = "narrative") -> str:
        """æ ¹æ“šæƒ…ç¯€ç”Ÿæˆå®Œæ•´æ•…äº‹"""
        prompt = f"""Based on this plot: {plot}
        
        Write a complete short story in {style} style.
        Make it engaging and about 500 words.
        """
        
        # ä½¿ç”¨ç›¸åŒçš„ç”Ÿæˆé‚è¼¯ï¼Œä½† max_tokens æ›´å¤§
        if model_name not in self.models:
            return f"Model {model_name} not available"
        
        config = self.models[model_name]
        
        try:
            if config.provider == 'openai':
                response = openai.ChatCompletion.create(
                    model=model_name,
                    messages=[
                        {"role": "system", "content": "You are a creative story writer."},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=1000
                )
                return response.choices[0].message.content
                
            # ... å…¶ä»–æ¨¡å‹çš„å¯¦ç¾é¡ä¼¼
            
        except Exception as e:
            return f"Error: {str(e)}"
```

### 5. `utils/story_processor.py` - æ•…äº‹è™•ç†å™¨

```python
import openai
from typing import List, Dict
import json

class StoryProcessor:
    def __init__(self):
        self.plots = []
        
    def cluster_plots(self, plots: List[Dict[str, str]]) -> List[Dict]:
        """ä½¿ç”¨ GPT å°‡ç›¸ä¼¼çš„æƒ…ç¯€åˆ†ç¾¤"""
        
        # æº–å‚™æƒ…ç¯€åˆ—è¡¨
        plot_text = "\n".join([f"{i+1}. {p['plot']}" for i, p in enumerate(plots)])
        
        prompt = f"""
        Please analyze these story plots and group similar ones together.
        Return the result as JSON format with groups and their plot numbers.
        
        Plots:
        {plot_text}
        
        Format:
        {{
            "groups": [
                {{
                    "theme": "theme description",
                    "plot_indices": [1, 3, 5]
                }}
            ]
        }}
        """
        
        try:
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "You are a story analyst."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=500
            )
            
            result = json.loads(response.choices[0].message.content)
            return result['groups']
            
        except Exception as e:
            print(f"Clustering error: {e}")
            # å¦‚æœå¤±æ•—ï¼Œè¿”å›æ‰€æœ‰æƒ…ç¯€ä½œç‚ºä¸€å€‹ç¾¤çµ„
            return [{
                "theme": "All plots",
                "plot_indices": list(range(1, len(plots) + 1))
            }]
    
    def format_to_markdown(self, story: str, metadata: Dict) -> str:
        """å°‡æ•…äº‹æ ¼å¼åŒ–ç‚º Markdown"""
        markdown = f"""# {metadata.get('title', 'Generated Story')}

**Generated by:** {metadata.get('model', 'Unknown Model')}  
**Date:** {metadata.get('date', '')}  
**Based on plot:** {metadata.get('plot', '')}

---

{story}

---

*This story was generated using AI models.*
"""
        return markdown
```

### 6. `app.py` - ä¸»æ‡‰ç”¨ç¨‹å¼

```python
import streamlit as st
from datetime import datetime
import random
from utils.llm_manager import LLMManager
from utils.story_processor import StoryProcessor

# åˆå§‹åŒ–
st.set_page_config(page_title="Story Generator Hub", layout="wide")

if 'llm_manager' not in st.session_state:
    st.session_state.llm_manager = LLMManager()
    st.session_state.processor = StoryProcessor()
    st.session_state.generated_plots = []
    st.session_state.selected_plots = []
    st.session_state.clustered_plots = []

st.title("ğŸ­ Story Generator Hub")
st.markdown("Generate creative stories using multiple AI models")

# å´é‚Šæ¬„ - æ¨¡å‹é¸æ“‡
with st.sidebar:
    st.header("Settings")
    available_models = st.session_state.llm_manager.get_available_models()
    
    if not available_models:
        st.error("No models available. Please check your API keys in .env file")
    else:
        st.success(f"âœ… {len(available_models)} models available")

# ä¸»è¦å…§å®¹å€
tab1, tab2, tab3 = st.tabs(["ğŸ“ Generate Plots", "âœ… Select Plots", "ğŸ“– Generate Story"])

# Tab 1: ç”Ÿæˆæƒ…ç¯€
with tab1:
    st.header("Step 1: Generate Story Plots")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # åŸºæœ¬ prompt
        base_prompt = st.text_area(
            "Enter your story theme or idea:",
            "A mysterious adventure in a futuristic city",
            height=100
        )
        
        # é¸æ“‡æ¨¡å‹
        if available_models:
            selected_models = st.multiselect(
                "Select models for plot generation (max 5):",
                available_models,
                default=available_models[:min(3, len(available_models))]
            )
            
            if st.button("ğŸ² Random Selection"):
                num_models = min(5, len(available_models))
                selected_models = random.sample(available_models, 
                                              random.randint(1, num_models))
                st.rerun()
    
    with col2:
        st.info("""
        **Tips:**
        - Select multiple models for diverse plots
        - Use random selection for surprise
        - Each model will generate 2-3 plot variations
        """)
    
    if st.button("Generate Plots", type="primary", disabled=not available_models):
        with st.spinner("Generating plots..."):
            plots = []
            
            # ç‚ºæ¯å€‹é¸å®šçš„æ¨¡å‹ç”Ÿæˆæƒ…ç¯€
            for model in selected_models:
                for i in range(2):  # æ¯å€‹æ¨¡å‹ç”Ÿæˆ2å€‹æƒ…ç¯€
                    plot_prompt = f"{base_prompt} - Create a unique plot outline (variation {i+1})"
                    plot_text = st.session_state.llm_manager.generate_plot(
                        model, plot_prompt
                    )
                    plots.append({
                        'model': model,
                        'plot': plot_text,
                        'id': f"{model}_{i}"
                    })
            
            st.session_state.generated_plots = plots
            
            # è‡ªå‹•åˆ†ç¾¤
            if len(plots) > 0:
                st.session_state.clustered_plots = st.session_state.processor.cluster_plots(plots)
    
    # é¡¯ç¤ºç”Ÿæˆçš„æƒ…ç¯€
    if st.session_state.generated_plots:
        st.subheader("Generated Plots")
        for i, plot in enumerate(st.session_state.generated_plots):
            with st.expander(f"Plot {i+1} - {plot['model']}"):
                st.write(plot['plot'])

# Tab 2: é¸æ“‡æƒ…ç¯€
with tab2:
    st.header("Step 2: Select Plot Groups")
    
    if st.session_state.clustered_plots:
        st.subheader("Clustered Plot Groups")
        
        selected_groups = []
        for i, group in enumerate(st.session_state.clustered_plots):
            st.write(f"**Group {i+1}: {group['theme']}**")
            
            # é¡¯ç¤ºè©²ç¾¤çµ„çš„æƒ…ç¯€
            for idx in group['plot_indices']:
                if idx <= len(st.session_state.generated_plots):
                    plot = st.session_state.generated_plots[idx-1]
                    if st.checkbox(f"{plot['model']}: {plot['plot'][:100]}...", 
                                 key=f"plot_{idx}"):
                        if plot not in st.session_state.selected_plots:
                            st.session_state.selected_plots.append(plot)
            
            st.divider()
    else:
        st.info("Please generate plots first in Tab 1")

# Tab 3: ç”Ÿæˆæ•…äº‹
with tab3:
    st.header("Step 3: Generate Complete Story")
    
    if st.session_state.selected_plots:
        st.subheader("Selected Plots")
        for plot in st.session_state.selected_plots:
            st.info(f"{plot['model']}: {plot['plot'][:150]}...")
        
        # é¸æ“‡ç”¨æ–¼ç”Ÿæˆæ•…äº‹çš„æ¨¡å‹
        story_models = st.multiselect(
            "Select models for story generation (max 3):",
            available_models,
            default=available_models[:min(1, len(available_models))]
        )
        
        story_style = st.selectbox(
            "Story style:",
            ["Narrative", "Mystery", "Sci-Fi", "Fantasy", "Romance"]
        )
        
        if st.button("Generate Story", type="primary"):
            with st.spinner("Generating story..."):
                # åˆä½µé¸å®šçš„æƒ…ç¯€
                combined_plot = " ".join([p['plot'] for p in st.session_state.selected_plots])
                
                stories = []
                for model in story_models:
                    story = st.session_state.llm_manager.generate_story(
                        model, combined_plot, story_style.lower()
                    )
                    stories.append({
                        'model': model,
                        'story': story,
                        'metadata': {
                            'title': f"{story_style} Story",
                            'model': model,
                            'date': datetime.now().strftime("%Y-%m-%d"),
                            'plot': combined_plot[:200]
                        }
                    })
                
                # é¡¯ç¤ºç”Ÿæˆçš„æ•…äº‹
                for story_data in stories:
                    st.subheader(f"Story by {story_data['model']}")
                    
                    # Markdown è¼¸å‡º
                    markdown_content = st.session_state.processor.format_to_markdown(
                        story_data['story'],
                        story_data['metadata']
                    )
                    
                    st.markdown(markdown_content)
                    
                    # ä¸‹è¼‰æŒ‰éˆ•
                    st.download_button(
                        label=f"Download {story_data['model']} story",
                        data=markdown_content,
                        file_name=f"story_{story_data['model']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md",
                        mime="text/markdown"
                    )
    else:
        st.info("Please select plots in Tab 2 first")
```

### 7. åœ¨ GitHub Codespace é‹è¡Œ

```bash
# 1. å‰µå»º .env æ–‡ä»¶ä¸¦æ·»åŠ  API keys
cp .env.example .env
# ç·¨è¼¯ .env æ·»åŠ ä½ çš„ API keys

# 2. å®‰è£ä¾è³´
pip install -r requirements.txt

# 3. é‹è¡Œæ‡‰ç”¨
streamlit run app.py
```

## ç‰ˆæœ¬ 2: Hugging Face Spaces ç‰ˆæœ¬ï¼ˆä½¿ç”¨ Gradioï¼‰

å¦‚æœæ‚¨æƒ³åœ¨ Hugging Face Spaces éƒ¨ç½²ï¼Œé€™è£¡æ˜¯ Gradio ç‰ˆæœ¬ï¼š

### `app_gradio.py`

```python
import gradio as gr
import os
from typing import List
import json
from datetime import datetime

# ç°¡åŒ–ç‰ˆ - ä½¿ç”¨ Hugging Face çš„å…è²»æ¨¡å‹
from transformers import pipeline

class SimpleStoryGenerator:
    def __init__(self):
        # ä½¿ç”¨ Hugging Face çš„å…è²»æ¨¡å‹
        self.generator = pipeline('text-generation', model='gpt2')
        
    def generate_plots(self, theme: str, num_plots: int = 5) -> List[str]:
        """ç”Ÿæˆå¤šå€‹æ•…äº‹æƒ…ç¯€"""
        plots = []
        for i in range(num_plots):
            prompt = f"Story plot about {theme}, variation {i+1}:"
            result = self.generator(prompt, max_length=100, num_return_sequences=1)
            plots.append(result[0]['generated_text'])
        return plots
    
    def cluster_plots(self, plots: List[str]) -> str:
        """ç°¡å–®åˆ†ç¾¤ï¼ˆé€™è£¡ç”¨ç°¡å–®çš„è¦å‰‡ï¼‰"""
        # ç°¡åŒ–ç‰ˆï¼šæŒ‰é•·åº¦åˆ†ç¾¤
        short_plots = [p for p in plots if len(p) < 150]
        long_plots = [p for p in plots if len(p) >= 150]
        
        result = "## Short Plots\n"
        for p in short_plots:
            result += f"- {p}\n"
        
        result += "\n## Long Plots\n"
        for p in long_plots:
            result += f"- {p}\n"
            
        return result
    
    def generate_story(self, selected_plots: str, style: str) -> str:
        """ç”Ÿæˆå®Œæ•´æ•…äº‹"""
        prompt = f"Write a {style} story based on: {selected_plots[:200]}"
        result = self.generator(prompt, max_length=500, num_return_sequences=1)
        
        # æ ¼å¼åŒ–ç‚º Markdown
        story = f"""# Generated {style} Story

**Date:** {datetime.now().strftime('%Y-%m-%d')}

---

{result[0]['generated_text']}

---

*Generated using AI*
"""
        return story

# å‰µå»º Gradio ç•Œé¢
generator = SimpleStoryGenerator()

def process_plots(theme, num_plots):
    plots = generator.generate_plots(theme, int(num_plots))
    clustered = generator.cluster_plots(plots)
    return clustered, plots

def create_story(selected_plots, style):
    if not selected_plots:
        return "Please generate and select plots first!"
    story = generator.generate_story(str(selected_plots), style)
    return story

# Gradio ç•Œé¢
with gr.Blocks(title="Story Generator Hub") as demo:
    gr.Markdown("# ğŸ­ Story Generator Hub")
    
    with gr.Tab("Generate Plots"):
        theme_input = gr.Textbox(label="Story Theme", 
                                value="A mysterious adventure")
        num_plots = gr.Slider(minimum=3, maximum=10, value=5, 
                             label="Number of Plots")
        generate_btn = gr.Button("Generate Plots")
        
        plots_output = gr.Markdown(label="Generated Plots")
        plots_state = gr.State()
        
        generate_btn.click(
            process_plots,
            inputs=[theme_input, num_plots],
            outputs=[plots_output, plots_state]
        )
    
    with gr.Tab("Generate Story"):
        selected_plots = gr.Textbox(label="Selected Plots", 
                                   placeholder="Paste selected plots here")
        style = gr.Dropdown(choices=["Adventure", "Mystery", "Sci-Fi", 
                                    "Fantasy", "Romance"],
                          value="Adventure",
                          label="Story Style")
        
        story_btn = gr.Button("Generate Story")
        story_output = gr.Markdown(label="Generated Story")
        
        story_btn.click(
            create_story,
            inputs=[selected_plots, style],
            outputs=story_output
        )

if __name__ == "__main__":
    demo.launch()
```

### éƒ¨ç½²åˆ° Hugging Face Spaces

1. å‰µå»º `requirements.txt`:
```txt
gradio
transformers
torch
```

2. åœ¨ Hugging Face å‰µå»ºæ–°çš„ Space
3. ä¸Šå‚³æ–‡ä»¶
4. è‡ªå‹•éƒ¨ç½²é‹è¡Œ

é€™å€‹æ–¹æ¡ˆè®“æ‚¨å¯ä»¥ï¼š
1. å…ˆåœ¨ Codespace æœ¬åœ°æ¸¬è©¦å®Œæ•´ç‰ˆæœ¬
2. åœ¨ Hugging Face Spaces éƒ¨ç½²ç°¡åŒ–çš„å…è²»ç‰ˆæœ¬
3. é€æ­¥æ·»åŠ æ›´å¤šåŠŸèƒ½

éœ€è¦æˆ‘è©³ç´°è§£é‡‹æŸå€‹éƒ¨åˆ†æˆ–æ·»åŠ æ›´å¤šåŠŸèƒ½å—ï¼Ÿ

# 03

æˆ‘å¹«æ‚¨é‡æ–°æ”¹å¯«ç¨‹å¼ä¾†æ”¯æ´ YourAPI å’Œ Stima APIï¼Œå› ç‚ºé€™å…©å€‹éƒ½å…¼å®¹ OpenAI API æ ¼å¼ï¼Œæ‰€ä»¥å¯ä»¥ä½¿ç”¨ OpenAI SDK ä¸¦ä¿®æ”¹ base URLï¼š

## æ›´æ–°å¾Œçš„ `utils/llm_manager.py`

```python
import os
from typing import List, Dict, Optional
from dataclasses import dataclass
from openai import OpenAI
import requests
from dotenv import load_dotenv

load_dotenv()

@dataclass
class ModelConfig:
    name: str
    provider: str
    available: bool
    display_name: str
    
class LLMManager:
    def __init__(self):
        self.clients = {}
        self.models = self._initialize_models()
        
    def _initialize_models(self) -> Dict[str, ModelConfig]:
        """æª¢æŸ¥å“ªäº›æ¨¡å‹å¯ç”¨"""
        models = {}
        
        # YourAPI è¨­å®š
        if os.getenv('YOURAPI_KEY'):
            self.clients['yourapi'] = OpenAI(
                api_key=os.getenv('YOURAPI_KEY'),
                base_url="https://api.yourapi.cn/v1"
            )
            
            # YourAPI æ”¯æ´çš„æ¨¡å‹åˆ—è¡¨ï¼ˆæ ¹æ“š pricing é é¢ï¼‰
            yourapi_models = [
                ('gpt-4o', 'GPT-4o'),
                ('gpt-4o-mini', 'GPT-4o Mini'),
                ('gpt-4-turbo', 'GPT-4 Turbo'),
                ('gpt-4', 'GPT-4'),
                ('gpt-3.5-turbo', 'GPT-3.5 Turbo'),
                ('claude-3-5-sonnet-20241022', 'Claude 3.5 Sonnet'),
                ('claude-3-opus-20240229', 'Claude 3 Opus'),
                ('claude-3-sonnet-20240229', 'Claude 3 Sonnet'),
                ('claude-3-haiku-20240307', 'Claude 3 Haiku'),
                ('gemini-1.5-pro', 'Gemini 1.5 Pro'),
                ('gemini-1.5-flash', 'Gemini 1.5 Flash'),
                ('deepseek-chat', 'DeepSeek Chat'),
                ('deepseek-coder', 'DeepSeek Coder'),
                ('llama-3.1-405b-instruct', 'Llama 3.1 405B'),
                ('llama-3.1-70b-instruct', 'Llama 3.1 70B'),
                ('llama-3.1-8b-instruct', 'Llama 3.1 8B'),
            ]
            
            for model_id, display_name in yourapi_models:
                models[f'yourapi_{model_id}'] = ModelConfig(
                    name=model_id,
                    provider='yourapi',
                    available=True,
                    display_name=f"[YourAPI] {display_name}"
                )
        
        # Stima API è¨­å®š
        if os.getenv('STIMA_API_KEY'):
            self.clients['stima'] = OpenAI(
                api_key=os.getenv('STIMA_API_KEY'),
                base_url="https://api.stima.tech/v1"
            )
            
            # å˜—è©¦å¾ API ç²å–æ¨¡å‹åˆ—è¡¨
            try:
                stima_models = self._fetch_stima_models()
                for model_id in stima_models:
                    models[f'stima_{model_id}'] = ModelConfig(
                        name=model_id,
                        provider='stima',
                        available=True,
                        display_name=f"[Stima] {model_id}"
                    )
            except:
                # å¦‚æœç„¡æ³•ç²å–ï¼Œä½¿ç”¨é è¨­åˆ—è¡¨
                default_stima_models = [
                    'gpt-4o',
                    'gpt-4o-mini',
                    'gpt-3.5-turbo',
                    'claude-3-5-sonnet',
                    'gemini-1.5-pro',
                    'llama-3.1-70b',
                ]
                for model_id in default_stima_models:
                    models[f'stima_{model_id}'] = ModelConfig(
                        name=model_id,
                        provider='stima',
                        available=True,
                        display_name=f"[Stima] {model_id}"
                    )
        
        return models
    
    def _fetch_stima_models(self) -> List[str]:
        """å¾ Stima API ç²å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        try:
            headers = {
                'Authorization': f"Bearer {os.getenv('STIMA_API_KEY')}"
            }
            response = requests.get(
                "https://api.stima.tech/models",
                headers=headers,
                timeout=5
            )
            if response.status_code == 200:
                data = response.json()
                # å‡è¨­è¿”å›æ ¼å¼æ˜¯ {"models": [{"id": "model-name", ...}]}
                return [model['id'] for model in data.get('data', [])]
        except:
            pass
        return []
    
    def get_available_models(self) -> List[Dict[str, str]]:
        """ç²å–æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨ï¼Œè¿”å›åŒ…å« key å’Œé¡¯ç¤ºåç¨±çš„å­—å…¸"""
        return [
            {
                'key': key,
                'display_name': config.display_name
            }
            for key, config in self.models.items() 
            if config.available
        ]
    
    def generate_plot(self, model_key: str, prompt: str) -> str:
        """ä½¿ç”¨æŒ‡å®šæ¨¡å‹ç”Ÿæˆæ•…äº‹æƒ…ç¯€"""
        if model_key not in self.models:
            return f"Model {model_key} not available"
        
        config = self.models[model_key]
        provider = config.provider
        
        if provider not in self.clients:
            return f"Provider {provider} not configured"
        
        try:
            client = self.clients[provider]
            response = client.chat.completions.create(
                model=config.name,
                messages=[
                    {"role": "system", "content": "You are a creative story writer. Create unique and engaging story plots."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=200,
                temperature=0.8
            )
            return response.choices[0].message.content
            
        except Exception as e:
            return f"Error with {config.display_name}: {str(e)}"
    
    def generate_story(self, model_key: str, plot: str, style: str = "narrative") -> str:
        """æ ¹æ“šæƒ…ç¯€ç”Ÿæˆå®Œæ•´æ•…äº‹"""
        if model_key not in self.models:
            return f"Model {model_key} not available"
        
        config = self.models[model_key]
        provider = config.provider
        
        if provider not in self.clients:
            return f"Provider {provider} not configured"
        
        prompt = f"""Based on this plot: {plot}
        
        Write a complete short story in {style} style.
        Make it engaging and approximately 500 words.
        Include vivid descriptions, character development, and a satisfying conclusion.
        """
        
        try:
            client = self.clients[provider]
            response = client.chat.completions.create(
                model=config.name,
                messages=[
                    {"role": "system", "content": f"You are a skilled {style} story writer. Create immersive and captivating stories."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1500,
                temperature=0.7
            )
            return response.choices[0].message.content
            
        except Exception as e:
            return f"Error with {config.display_name}: {str(e)}"
    
    def cluster_plots_with_ai(self, plots: List[Dict[str, str]]) -> List[Dict]:
        """ä½¿ç”¨ AI æ¨¡å‹å°‡ç›¸ä¼¼çš„æƒ…ç¯€åˆ†ç¾¤"""
        # é¸æ“‡ä¸€å€‹å¯ç”¨çš„æ¨¡å‹ä¾†åšåˆ†é¡ï¼ˆå„ªå…ˆä½¿ç”¨ä¾¿å®œçš„ï¼‰
        preferred_models = [
            'yourapi_gpt-3.5-turbo',
            'stima_gpt-3.5-turbo',
            'yourapi_deepseek-chat',
            'yourapi_gpt-4o-mini',
        ]
        
        clustering_model = None
        for model in preferred_models:
            if model in self.models:
                clustering_model = model
                break
        
        if not clustering_model:
            # å¦‚æœæ²’æœ‰åå¥½çš„æ¨¡å‹ï¼Œä½¿ç”¨ç¬¬ä¸€å€‹å¯ç”¨çš„
            if self.models:
                clustering_model = list(self.models.keys())[0]
            else:
                return self._simple_clustering(plots)
        
        # æº–å‚™æƒ…ç¯€åˆ—è¡¨
        plot_text = "\n".join([f"{i+1}. {p['plot']}" for i, p in enumerate(plots)])
        
        prompt = f"""
        Analyze these story plots and group similar ones by theme, genre, or narrative elements.
        Return ONLY valid JSON format.
        
        Plots:
        {plot_text}
        
        Required JSON format:
        {{
            "groups": [
                {{
                    "theme": "Brief theme description",
                    "plot_indices": [1, 3, 5],
                    "common_elements": "What these plots share"
                }}
            ]
        }}
        """
        
        try:
            config = self.models[clustering_model]
            client = self.clients[config.provider]
            
            response = client.chat.completions.create(
                model=config.name,
                messages=[
                    {"role": "system", "content": "You are a literary analyst. Return only valid JSON."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=500,
                temperature=0.3
            )
            
            import json
            result = json.loads(response.choices[0].message.content)
            return result['groups']
            
        except Exception as e:
            print(f"AI clustering failed: {e}")
            return self._simple_clustering(plots)
    
    def _simple_clustering(self, plots: List[Dict[str, str]]) -> List[Dict]:
        """ç°¡å–®çš„å‚™ç”¨åˆ†ç¾¤æ–¹æ³•"""
        # å¦‚æœ AI åˆ†ç¾¤å¤±æ•—ï¼Œè¿”å›æ‰€æœ‰æƒ…ç¯€ä½œç‚ºä¸€å€‹ç¾¤çµ„
        return [{
            "theme": "All generated plots",
            "plot_indices": list(range(1, len(plots) + 1)),
            "common_elements": "Various creative story ideas"
        }]
```

## æ›´æ–° `.env.example`

```env
# YourAPI è¨­å®š
YOURAPI_KEY=your_yourapi_key_here

# Stima API è¨­å®š
STIMA_API_KEY=your_stima_api_key_here
```

## æ›´æ–°å¾Œçš„ `app.py` ä¸»ç¨‹å¼ï¼ˆç›¸æ‡‰çš„ä¿®æ”¹ï¼‰

```python
import streamlit as st
from datetime import datetime
import random
from utils.llm_manager import LLMManager
from utils.story_processor import StoryProcessor

# åˆå§‹åŒ–
st.set_page_config(page_title="Story Generator Hub", layout="wide", page_icon="ğŸ­")

if 'llm_manager' not in st.session_state:
    st.session_state.llm_manager = LLMManager()
    st.session_state.processor = StoryProcessor()
    st.session_state.generated_plots = []
    st.session_state.selected_plots = []
    st.session_state.clustered_plots = []

st.title("ğŸ­ Story Generator Hub")
st.markdown("Generate creative stories using multiple AI models from YourAPI and Stima")

# å´é‚Šæ¬„ - æ¨¡å‹è³‡è¨Š
with st.sidebar:
    st.header("âš™ï¸ Configuration")
    
    available_models = st.session_state.llm_manager.get_available_models()
    
    if not available_models:
        st.error("âŒ No models available")
        st.info("""
        Please add your API keys in `.env` file:
        - YOURAPI_KEY
        - STIMA_API_KEY
        """)
    else:
        st.success(f"âœ… {len(available_models)} models available")
        
        with st.expander("View Available Models"):
            for model in available_models:
                st.write(f"â€¢ {model['display_name']}")
    
    st.divider()
    
    # API è³‡è¨Š
    st.subheader("ğŸ“Š API Status")
    col1, col2 = st.columns(2)
    with col1:
        if 'yourapi' in st.session_state.llm_manager.clients:
            st.success("YourAPI âœ“")
        else:
            st.error("YourAPI âœ—")
    
    with col2:
        if 'stima' in st.session_state.llm_manager.clients:
            st.success("Stima âœ“")
        else:
            st.error("Stima âœ—")

# ä¸»è¦å…§å®¹å€
tab1, tab2, tab3 = st.tabs(["ğŸ“ Generate Plots", "âœ… Select Plots", "ğŸ“– Generate Story"])

# Tab 1: ç”Ÿæˆæƒ…ç¯€
with tab1:
    st.header("Step 1: Generate Story Plots")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # åŸºæœ¬ prompt
        base_prompt = st.text_area(
            "Enter your story theme or idea:",
            "A mysterious adventure in a futuristic city where AI and humans coexist",
            height=100
        )
        
        # é¸æ“‡æ¨¡å‹
        if available_models:
            # å‰µå»ºé¡¯ç¤ºåç¨±åˆ° key çš„æ˜ å°„
            model_options = {model['display_name']: model['key'] 
                           for model in available_models}
            
            selected_display_names = st.multiselect(
                "Select models for plot generation (max 5):",
                list(model_options.keys()),
                default=list(model_options.keys())[:min(3, len(model_options))]
            )
            
            # è½‰æ›å›å¯¦éš›çš„ model keys
            selected_models = [model_options[name] for name in selected_display_names]
            
            col_a, col_b = st.columns(2)
            with col_a:
                if st.button("ğŸ² Random Selection"):
                    num_models = min(5, len(available_models))
                    random_models = random.sample(list(model_options.keys()), 
                                                random.randint(2, num_models))
                    st.rerun()
            
            with col_b:
                num_variations = st.number_input("Plots per model:", 
                                               min_value=1, max_value=3, value=2)
    
    with col2:
        st.info("""
        **ğŸ’¡ Tips:**
        - YourAPI supports GPT, Claude, Gemini, Llama models
        - Stima provides various open-source models
        - Mix different models for creative diversity
        """)
    
    if st.button("ğŸš€ Generate Plots", type="primary", disabled=not available_models):
        with st.spinner("Generating plots from multiple models..."):
            plots = []
            progress_bar = st.progress(0)
            total_tasks = len(selected_models) * num_variations
            current_task = 0
            
            # ç‚ºæ¯å€‹é¸å®šçš„æ¨¡å‹ç”Ÿæˆæƒ…ç¯€
            for model_key in selected_models:
                model_name = next(m['display_name'] for m in available_models 
                                if m['key'] == model_key)
                
                for i in range(num_variations):
                    current_task += 1
                    progress_bar.progress(current_task / total_tasks)
                    
                    plot_prompt = f"""Create a unique story plot based on: {base_prompt}
                    
                    Variation {i+1}: Focus on a different aspect or perspective.
                    Be creative and original. About 50-100 words."""
                    
                    plot_text = st.session_state.llm_manager.generate_plot(
                        model_key, plot_prompt
                    )
                    
                    plots.append({
                        'model': model_name,
                        'plot': plot_text,
                        'id': f"{model_key}_{i}"
                    })
            
            progress_bar.empty()
            st.session_state.generated_plots = plots
            
            # ä½¿ç”¨ AI è‡ªå‹•åˆ†ç¾¤
            if len(plots) > 0:
                with st.spinner("Analyzing and clustering plots..."):
                    st.session_state.clustered_plots = st.session_state.llm_manager.cluster_plots_with_ai(plots)
                st.success(f"âœ… Generated {len(plots)} plots and organized them into {len(st.session_state.clustered_plots)} groups")
    
    # é¡¯ç¤ºç”Ÿæˆçš„æƒ…ç¯€
    if st.session_state.generated_plots:
        st.divider()
        st.subheader("ğŸ“š Generated Plots")
        
        for i, plot in enumerate(st.session_state.generated_plots):
            with st.expander(f"Plot {i+1} - {plot['model']}", expanded=(i==0)):
                st.write(plot['plot'])
                st.caption(f"Model: {plot['model']} | ID: {plot['id']}")

# Tab 2: é¸æ“‡æƒ…ç¯€
with tab2:
    st.header("Step 2: Select Plot Groups")
    
    if st.session_state.clustered_plots:
        st.info(f"Found {len(st.session_state.clustered_plots)} plot groups. Select the plots you want to use for story generation.")
        
        # æ¸…é™¤ä¹‹å‰çš„é¸æ“‡
        if st.button("ğŸ”„ Clear Selection"):
            st.session_state.selected_plots = []
            st.rerun()
        
        for group_idx, group in enumerate(st.session_state.clustered_plots):
            with st.expander(f"**Group {group_idx+1}: {group['theme']}**", expanded=True):
                if 'common_elements' in group:
                    st.caption(f"Common elements: {group['common_elements']}")
                
                st.write("**Plots in this group:**")
                
                # é¡¯ç¤ºè©²ç¾¤çµ„çš„æƒ…ç¯€
                for idx in group['plot_indices']:
                    if idx <= len(st.session_state.generated_plots):
                        plot = st.session_state.generated_plots[idx-1]
                        
                        col1, col2 = st.columns([1, 10])
                        with col1:
                            is_selected = st.checkbox("", key=f"plot_select_{idx}",
                                                    value=(plot in st.session_state.selected_plots))
                        with col2:
                            st.write(f"**{plot['model']}**")
                            st.write(plot['plot'])
                        
                        if is_selected and plot not in st.session_state.selected_plots:
                            st.session_state.selected_plots.append(plot)
                        elif not is_selected and plot in st.session_state.selected_plots:
                            st.session_state.selected_plots.remove(plot)
                
                st.divider()
        
        # é¡¯ç¤ºå·²é¸æ“‡çš„æ•¸é‡
        if st.session_state.selected_plots:
            st.success(f"âœ… Selected {len(st.session_state.selected_plots)} plots")
    else:
        st.info("ğŸ“ Please generate plots first in Tab 1")

# Tab 3: ç”Ÿæˆæ•…äº‹
with tab3:
    st.header("Step 3: Generate Complete Story")
    
    if st.session_state.selected_plots:
        st.subheader("ğŸ“‹ Selected Plots")
        
        # é¡¯ç¤ºé¸ä¸­çš„æƒ…ç¯€
        with st.expander("View selected plots", expanded=False):
            for idx, plot in enumerate(st.session_state.selected_plots):
                st.write(f"{idx+1}. **{plot['model']}**: {plot['plot']}")
        
        st.divider()
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            # é¸æ“‡ç”¨æ–¼ç”Ÿæˆæ•…äº‹çš„æ¨¡å‹
            if available_models:
                story_model_options = {model['display_name']: model['key'] 
                                     for model in available_models}
                
                selected_story_displays = st.multiselect(
                    "Select models for story generation (max 3):",
                    list(story_model_options.keys()),
                    default=list(story_model_options.keys())[:min(2, len(story_model_options))],
                    max_selections=3
                )
                
                story_models = [story_model_options[name] for name in selected_story_displays]
            
            story_style = st.selectbox(
                "Story style:",
                ["Narrative", "Mystery", "Sci-Fi", "Fantasy", "Romance", 
                 "Thriller", "Comedy", "Drama", "Horror", "Adventure"]
            )
            
            story_length = st.select_slider(
                "Story length:",
                options=["Short (500 words)", "Medium (1000 words)", "Long (1500 words)"],
                value="Medium (1000 words)"
            )
        
        with col2:
            st.info("""
            **ğŸ“– Story Generation:**
            - Multiple models create different versions
            - Each brings unique perspective
            - Export as Markdown files
            """)
        
        if st.button("âœ¨ Generate Stories", type="primary"):
            with st.spinner("Creating your stories..."):
                # åˆä½µé¸å®šçš„æƒ…ç¯€
                combined_plots = "\n".join([f"- {p['plot']}" for p in st.session_state.selected_plots])
                
                stories = []
                progress = st.progress(0)
                
                for idx, model_key in enumerate(story_models):
                    progress.progress((idx + 1) / len(story_models))
                    
                    model_name = next(m['display_name'] for m in available_models 
                                     if m['key'] == model_key)
                    
                    st.info(f"Generating story with {model_name}...")
                    
                    story = st.session_state.llm_manager.generate_story(
                        model_key, combined_plots, story_style.lower()
                    )
                    
                    stories.append({
                        'model': model_name,
                        'story': story,
                        'metadata': {
                            'title': f"{story_style} Story",
                            'model': model_name,
                            'date': datetime.now().strftime("%Y-%m-%d %H:%M"),
                            'style': story_style,
                            'based_on_plots': len(st.session_state.selected_plots)
                        }
                    })
                
                progress.empty()
                st.success("âœ… Stories generated successfully!")
                
                # é¡¯ç¤ºç”Ÿæˆçš„æ•…äº‹
                for story_data in stories:
                    st.divider()
                    
                    col1, col2 = st.columns([3, 1])
                    with col1:
                        st.subheader(f"ğŸ“– {story_data['metadata']['title']} by {story_data['model']}")
                    
                    with col2:
                        # Markdown æ ¼å¼åŒ–
                        markdown_content = f"""# {story_data['metadata']['title']}

**Generated by:** {story_data['metadata']['model']}  
**Date:** {story_data['metadata']['date']}  
**Style:** {story_data['metadata']['style']}  
**Based on:** {story_data['metadata']['based_on_plots']} selected plots  

---

{story_data['story']}

---

*This story was generated using AI through Story Generator Hub.*  
*Powered by YourAPI & Stima API*
"""
                        
                        # ä¸‹è¼‰æŒ‰éˆ•
                        st.download_button(
                            label=f"ğŸ“¥ Download",
                            data=markdown_content,
                            file_name=f"story_{story_data['metadata']['style'].lower()}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md",
                            mime="text/markdown",
                            key=f"download_{story_data['model']}_{idx}"
                        )
                    
                    # é¡¯ç¤ºæ•…äº‹å…§å®¹
                    with st.container():
                        st.markdown(story_data['story'])
                    
    else:
        st.info("âœ‹ Please select plots in Tab 2 first")
        
        # æä¾›å¿«é€Ÿé–‹å§‹é¸é …
        if st.session_state.generated_plots:
            if st.button("ğŸš€ Quick Start - Select All Plots"):
                st.session_state.selected_plots = st.session_state.generated_plots.copy()
                st.rerun()
```

## æ›´æ–° `requirements.txt`

```txt
streamlit
openai>=1.0.0
python-dotenv
requests
pandas
```

ç¾åœ¨é€™å€‹ç‰ˆæœ¬ï¼š

1. **æ”¯æ´ YourAPI å’Œ Stima API** - å…©å€‹éƒ½ä½¿ç”¨ OpenAI å…¼å®¹æ ¼å¼
2. **å‹•æ…‹æ¨¡å‹åˆ—è¡¨** - å¾ YourAPI pricing é é¢çš„æ¨¡å‹ï¼ŒStima å¯å¾ API ç²å–
3. **æ›´å¥½çš„éŒ¯èª¤è™•ç†** - å¦‚æœæŸå€‹ API å¤±æ•—ï¼Œå…¶ä»–çš„ä»å¯ä½¿ç”¨
4. **æ¸…æ™°çš„æ¨¡å‹æ¨™è­˜** - é¡¯ç¤º [YourAPI] æˆ– [Stima] å‰ç¶´
5. **éˆæ´»çš„é…ç½®** - å¯ä»¥åªä½¿ç”¨å…¶ä¸­ä¸€å€‹ APIï¼Œæˆ–åŒæ™‚ä½¿ç”¨å…©å€‹

æ‚¨åªéœ€è¦åœ¨ `.env` æ–‡ä»¶ä¸­è¨­ç½®ç›¸æ‡‰çš„ API keys å³å¯é–‹å§‹ä½¿ç”¨ï¼


# 04

æˆ‘ä¾†è©³ç´°èªªæ˜åœ¨ GitHub Codespace é‹è¡Œé€™å€‹å°ˆæ¡ˆçš„å®Œæ•´æ“ä½œæµç¨‹ï¼š

## ğŸ“‹ GitHub Codespace å®Œæ•´æ“ä½œæµç¨‹

### Step 1: å‰µå»º GitHub Repository

1. **ç™»å…¥ GitHub**
   - å‰å¾€ https://github.com
   - é»æ“Šå³ä¸Šè§’ `+` â†’ `New repository`

2. **è¨­å®š Repository**
```
Repository name: story-generator-hub
Description: Multi-model collaborative story generation platform
Public/Private: é¸æ“‡ Publicï¼ˆå…è²» Codespace æ™‚æ•¸è¼ƒå¤šï¼‰
âœ… Add a README file
âœ… Add .gitignore â†’ é¸æ“‡ Python
âœ… Choose a license â†’ MIT
```

3. **é»æ“Š `Create repository`**

### Step 2: é–‹å•Ÿ Codespace

1. **åœ¨ Repository é é¢**
   - é»æ“Šç¶ è‰² `<> Code` æŒ‰éˆ•
   - é¸æ“‡ `Codespaces` æ¨™ç±¤
   - é»æ“Š `Create codespace on main`

2. **ç­‰å¾… Codespace å•Ÿå‹•**ï¼ˆç´„ 30 ç§’åˆ° 1 åˆ†é˜ï¼‰

### Step 3: å‰µå»ºå°ˆæ¡ˆçµæ§‹

åœ¨ Codespace çš„çµ‚ç«¯æ©Ÿä¸­åŸ·è¡Œï¼š

```bash
# 1. å‰µå»ºå°ˆæ¡ˆç›®éŒ„çµæ§‹
mkdir -p utils
touch app.py
touch requirements.txt
touch .env
touch .env.example
touch utils/__init__.py
touch utils/llm_manager.py
touch utils/story_processor.py
```

### Step 4: è¤‡è£½ç¨‹å¼ç¢¼

#### 4.1 ç·¨è¼¯ `requirements.txt`
```bash
# é»æ“Šæª”æ¡ˆæˆ–ä½¿ç”¨å‘½ä»¤
code requirements.txt
```

è²¼å…¥ï¼š
```txt
streamlit
openai>=1.0.0
python-dotenv
requests
pandas
```

#### 4.2 ç·¨è¼¯ `.env.example`
```bash
code .env.example
```

è²¼å…¥ï¼š
```env
# YourAPI è¨­å®š
YOURAPI_KEY=your_yourapi_key_here

# Stima API è¨­å®š
STIMA_API_KEY=your_stima_api_key_here
```

#### 4.3 ç·¨è¼¯ `utils/llm_manager.py`
```bash
code utils/llm_manager.py
```
è²¼å…¥å®Œæ•´çš„ LLMManager ç¨‹å¼ç¢¼ï¼ˆä¸Šé¢æä¾›çš„ï¼‰

#### 4.4 ç·¨è¼¯ `utils/story_processor.py`
```bash
code utils/story_processor.py
```

è²¼å…¥ï¼š
```python
import json
from typing import List, Dict

class StoryProcessor:
    def __init__(self):
        self.plots = []
        
    def format_to_markdown(self, story: str, metadata: Dict) -> str:
        """å°‡æ•…äº‹æ ¼å¼åŒ–ç‚º Markdown"""
        markdown = f"""# {metadata.get('title', 'Generated Story')}

**Generated by:** {metadata.get('model', 'Unknown Model')}  
**Date:** {metadata.get('date', '')}  
**Style:** {metadata.get('style', '')}  
**Based on:** {metadata.get('based_on_plots', 0)} selected plots  

---

{story}

---

*This story was generated using AI through Story Generator Hub.*  
*Powered by YourAPI & Stima API*
"""
        return markdown
```

#### 4.5 ç·¨è¼¯ `app.py`
```bash
code app.py
```
è²¼å…¥å®Œæ•´çš„ Streamlit æ‡‰ç”¨ç¨‹å¼ç¢¼ï¼ˆä¸Šé¢æä¾›çš„ï¼‰

### Step 5: è¨­å®š API Keys

#### 5.1 ç²å– API Keys

**YourAPI:**
1. å‰å¾€ https://yourapi.cn
2. è¨»å†Š/ç™»å…¥
3. ç²å– API Key

**Stima API:**
1. å‰å¾€ https://stima.tech
2. è¨»å†Š/ç™»å…¥
3. ç²å– API Key

#### 5.2 è¨­å®šç’°å¢ƒè®Šæ•¸
```bash
# è¤‡è£½ç¯„ä¾‹æª”æ¡ˆ
cp .env.example .env

# ç·¨è¼¯ .env æª”æ¡ˆ
code .env
```

æ›¿æ›ç‚ºä½ çš„å¯¦éš› API Keysï¼š
```env
# YourAPI è¨­å®š
YOURAPI_KEY=sk-xxxxxxxxxxxxxxxxxx

# Stima API è¨­å®š
STIMA_API_KEY=stm_xxxxxxxxxxxxxxxxxx
```

### Step 6: å®‰è£å¥—ä»¶

åœ¨çµ‚ç«¯æ©ŸåŸ·è¡Œï¼š
```bash
# å‡ç´š pip
pip install --upgrade pip

# å®‰è£æ‰€æœ‰ä¾è³´
pip install -r requirements.txt
```

### Step 7: é‹è¡Œæ‡‰ç”¨ç¨‹å¼

```bash
# å•Ÿå‹• Streamlit æ‡‰ç”¨
streamlit run app.py
```

### Step 8: è¨ªå•æ‡‰ç”¨

1. **Codespace æœƒè‡ªå‹•é¡¯ç¤ºä¸€å€‹å½ˆå‡ºé€šçŸ¥**
   - é»æ“Š `Open in Browser`

2. **æˆ–æ‰‹å‹•é–‹å•Ÿ**
   - åœ¨ PORTS æ¨™ç±¤ä¸­æ‰¾åˆ° 8501 ç«¯å£
   - é»æ“Šåœ°çƒåœ–æ¨™é–‹å•Ÿ

### Step 9: æ¸¬è©¦æ‡‰ç”¨ç¨‹å¼

1. **æª¢æŸ¥æ¨¡å‹å¯ç”¨æ€§**
   - æŸ¥çœ‹å´é‚Šæ¬„çš„æ¨¡å‹ç‹€æ…‹
   - ç¢ºèª API é€£æ¥æˆåŠŸ

2. **ç”Ÿæˆæ•…äº‹æƒ…ç¯€**
   - Tab 1: è¼¸å…¥ä¸»é¡Œ
   - é¸æ“‡æ¨¡å‹ï¼ˆæˆ–éš¨æ©Ÿï¼‰
   - é»æ“Š Generate Plots

3. **é¸æ“‡æƒ…ç¯€**
   - Tab 2: æŸ¥çœ‹åˆ†çµ„çš„æƒ…ç¯€
   - å‹¾é¸æ„Ÿèˆˆè¶£çš„æƒ…ç¯€

4. **ç”Ÿæˆæ•…äº‹**
   - Tab 3: é¸æ“‡ç”Ÿæˆæ¨¡å‹
   - é¸æ“‡æ•…äº‹é¢¨æ ¼
   - ç”Ÿæˆä¸¦ä¸‹è¼‰æ•…äº‹

### ğŸ”§ æ•…éšœæ’é™¤

#### å•é¡Œ 1: æ¨¡å‹ä¸å¯ç”¨
```bash
# æª¢æŸ¥ç’°å¢ƒè®Šæ•¸
cat .env

# ç¢ºèª API Key æ ¼å¼æ­£ç¢º
python -c "from dotenv import load_dotenv; import os; load_dotenv(); print('YourAPI:', bool(os.getenv('YOURAPI_KEY'))); print('Stima:', bool(os.getenv('STIMA_API_KEY')))"
```

#### å•é¡Œ 2: Streamlit ç„¡æ³•å•Ÿå‹•
```bash
# æª¢æŸ¥ç«¯å£æ˜¯å¦è¢«å ç”¨
lsof -i :8501

# ä½¿ç”¨ä¸åŒç«¯å£
streamlit run app.py --server.port 8502
```

#### å•é¡Œ 3: å¥—ä»¶å®‰è£å¤±æ•—
```bash
# å‰µå»ºè™›æ“¬ç’°å¢ƒ
python -m venv venv
source venv/bin/activate  # Linux/Mac
pip install -r requirements.txt
```

### ğŸ“ ä¿å­˜å’Œæäº¤

```bash
# æ·»åŠ æ‰€æœ‰æª”æ¡ˆ
git add .

# æäº¤æ›´æ”¹
git commit -m "Initial story generator setup"

# æ¨é€åˆ° GitHub
git push origin main
```

### ğŸ¯ å¿«é€Ÿæ¸¬è©¦è…³æœ¬

å‰µå»º `test_api.py` ä¾†æ¸¬è©¦ API é€£æ¥ï¼š

```python
# test_api.py
from dotenv import load_dotenv
import os
from openai import OpenAI

load_dotenv()

def test_yourapi():
    if not os.getenv('YOURAPI_KEY'):
        print("âŒ YourAPI key not found")
        return
    
    try:
        client = OpenAI(
            api_key=os.getenv('YOURAPI_KEY'),
            base_url="https://api.yourapi.cn/v1"
        )
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": "Hello, test"}],
            max_tokens=10
        )
        print("âœ… YourAPI working!")
    except Exception as e:
        print(f"âŒ YourAPI error: {e}")

def test_stima():
    if not os.getenv('STIMA_API_KEY'):
        print("âŒ Stima API key not found")
        return
    
    try:
        client = OpenAI(
            api_key=os.getenv('STIMA_API_KEY'),
            base_url="https://api.stima.tech/v1"
        )
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": "Hello, test"}],
            max_tokens=10
        )
        print("âœ… Stima API working!")
    except Exception as e:
        print(f"âŒ Stima API error: {e}")

if __name__ == "__main__":
    print("Testing API connections...")
    test_yourapi()
    test_stima()
```

é‹è¡Œæ¸¬è©¦ï¼š
```bash
python test_api.py
```

### ğŸ’¡ Codespace ä½¿ç”¨æç¤º

1. **è‡ªå‹•ä¿å­˜**: Codespace æœƒè‡ªå‹•ä¿å­˜ä½ çš„å·¥ä½œ
2. **å…è²»æ™‚æ•¸**: GitHub Free æä¾›æ¯æœˆ 60 å°æ™‚å…è²»
3. **è‡ªå‹•é—œé–‰**: é–’ç½® 30 åˆ†é˜å¾Œè‡ªå‹•åœæ­¢
4. **é‡æ–°é–‹å•Ÿ**: å¾ Repository é é¢å¯ä»¥é‡æ–°é–‹å•Ÿä¹‹å‰çš„ Codespace

é€™æ¨£ä½ å°±å¯ä»¥åœ¨ Codespace ä¸­å®Œæ•´é‹è¡Œé€™å€‹æ•…äº‹ç”Ÿæˆå™¨äº†ï¼æœ‰ä»»ä½•å•é¡Œéš¨æ™‚å•æˆ‘ã€‚

# 05

æ˜ç™½äº†ï¼å¦‚æœæ‚¨å·²ç¶“æœ‰å®Œæ•´çš„å°ˆæ¡ˆæª”æ¡ˆï¼Œé€™æ˜¯ç°¡åŒ–çš„ Codespace æ“ä½œæµç¨‹ï¼š

## ğŸš€ å¿«é€Ÿå•Ÿå‹•æµç¨‹ï¼ˆå·²æœ‰å°ˆæ¡ˆæª”æ¡ˆï¼‰

### Step 1: ä¸Šå‚³å°ˆæ¡ˆåˆ° GitHub

```bash
# åœ¨æœ¬åœ°å°ˆæ¡ˆç›®éŒ„
git init
git add .
git commit -m "Initial commit"
git branch -M main
git remote add origin https://github.com/YOUR_USERNAME/story-generator-hub.git
git push -u origin main
```

### Step 2: é–‹å•Ÿ Codespace

1. é€²å…¥ä½ çš„ GitHub Repository
2. é»æ“Š `<> Code` â†’ `Codespaces` â†’ `Create codespace on main`
3. ç­‰å¾… Codespace å•Ÿå‹•

### Step 3: è¨­å®š API Keys

```bash
# åœ¨ Codespace çµ‚ç«¯æ©Ÿ
cp .env.example .env
```

ç·¨è¼¯ `.env`ï¼š
```bash
code .env
# æˆ–ä½¿ç”¨ nano
nano .env
```

å¡«å…¥ä½ çš„ API Keysï¼š
```env
YOURAPI_KEY=sk-your-actual-key-here
STIMA_API_KEY=stm-your-actual-key-here
```

### Step 4: å®‰è£ä¸¦é‹è¡Œ

```bash
# å®‰è£ä¾è³´
pip install -r requirements.txt

# é‹è¡Œæ‡‰ç”¨
streamlit run app.py
```

### Step 5: é–‹å•Ÿæ‡‰ç”¨

- Codespace æœƒé¡¯ç¤ºé€šçŸ¥ï¼Œé»æ“Š `Open in Browser`
- æˆ–åœ¨ PORTS æ¨™ç±¤æ‰¾åˆ° 8501 ç«¯å£ï¼Œé»æ“Šé–‹å•Ÿ

---

## ğŸ“Œ ä¸€éµå•Ÿå‹•è…³æœ¬

ç‚ºäº†æ›´æ–¹ä¾¿ï¼Œå¯ä»¥å‰µå»ºä¸€å€‹å•Ÿå‹•è…³æœ¬ï¼š

### å‰µå»º `start.sh`

```bash
#!/bin/bash

# æª¢æŸ¥ .env æª”æ¡ˆ
if [ ! -f .env ]; then
    echo "âš ï¸ Creating .env file from template..."
    cp .env.example .env
    echo "ğŸ“ Please edit .env file and add your API keys"
    echo "Run: code .env"
    exit 1
fi

# æª¢æŸ¥ API Keys
if grep -q "your_.*_key_here" .env; then
    echo "âš ï¸ Please update API keys in .env file"
    echo "Run: code .env"
    exit 1
fi

# å®‰è£ä¾è³´
echo "ğŸ“¦ Installing dependencies..."
pip install -r requirements.txt

# å•Ÿå‹•æ‡‰ç”¨
echo "ğŸš€ Starting Story Generator Hub..."
streamlit run app.py --server.port 8501 --server.address 0.0.0.0
```

ä½¿ç”¨è…³æœ¬ï¼š
```bash
# çµ¦äºˆåŸ·è¡Œæ¬Šé™
chmod +x start.sh

# é‹è¡Œ
./start.sh
```

---

## ğŸ”‘ Codespace Secretsï¼ˆæ›´å®‰å…¨çš„æ–¹å¼ï¼‰

ä¸æƒ³åœ¨ `.env` æª”æ¡ˆä¸­å„²å­˜ API Keysï¼Œå¯ä»¥ä½¿ç”¨ GitHub Secretsï¼š

### è¨­å®š Codespace Secrets

1. å‰å¾€ Repository Settings
2. å·¦å´é¸å–® `Secrets and variables` â†’ `Codespaces`
3. é»æ“Š `New repository secret`
4. æ·»åŠ ï¼š
   - Name: `YOURAPI_KEY`
   - Value: `ä½ çš„ API Key`
5. é‡è¤‡æ·»åŠ  `STIMA_API_KEY`

### ä¿®æ”¹ç¨‹å¼ç¢¼ä½¿ç”¨ Secrets

`utils/llm_manager.py` é–‹é ­ä¿®æ”¹ç‚ºï¼š
```python
import os
from typing import List, Dict, Optional
from dataclasses import dataclass
from openai import OpenAI
import requests

# å„ªå…ˆä½¿ç”¨ç’°å¢ƒè®Šæ•¸ï¼Œç„¶å¾Œæ˜¯ .env æª”æ¡ˆ
YOURAPI_KEY = os.environ.get('YOURAPI_KEY', None)
STIMA_API_KEY = os.environ.get('STIMA_API_KEY', None)

# å¦‚æœç’°å¢ƒè®Šæ•¸æ²’æœ‰ï¼Œæ‰è¼‰å…¥ .env
if not YOURAPI_KEY or not STIMA_API_KEY:
    from dotenv import load_dotenv
    load_dotenv()
    YOURAPI_KEY = YOURAPI_KEY or os.getenv('YOURAPI_KEY')
    STIMA_API_KEY = STIMA_API_KEY or os.getenv('STIMA_API_KEY')
```

---

## ğŸ§ª å¿«é€Ÿæ¸¬è©¦å‘½ä»¤

åœ¨ Codespace çµ‚ç«¯æ©Ÿæ¸¬è©¦ APIï¼š

```bash
# æ¸¬è©¦ YourAPI
curl https://api.yourapi.cn/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $YOURAPI_KEY" \
  -d '{
    "model": "gpt-3.5-turbo",
    "messages": [{"role": "user", "content": "Hello"}],
    "max_tokens": 10
  }'

# æ¸¬è©¦ Stima API
curl https://api.stima.tech/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $STIMA_API_KEY" \
  -d '{
    "model": "gpt-3.5-turbo",
    "messages": [{"role": "user", "content": "Hello"}],
    "max_tokens": 10
  }'
```

---

## ğŸ“ ç¸½çµï¼šæœ€ç°¡æ­¥é©Ÿ

å¦‚æœå°ˆæ¡ˆæª”æ¡ˆéƒ½å·²æº–å‚™å¥½ï¼Œå¯¦éš›ä¸Šåªéœ€è¦ï¼š

```bash
# 1. åœ¨ Codespace é–‹å•Ÿå¾Œ
cp .env.example .env
code .env  # å¡«å…¥ API Keys

# 2. å®‰è£ä¸¦é‹è¡Œ
pip install -r requirements.txt
streamlit run app.py

# 3. é»æ“Šå½ˆå‡ºçš„ "Open in Browser"
```

å°±é€™æ¨£ï¼ä¸‰å€‹æ­¥é©Ÿå°±å¯ä»¥é‹è¡Œäº†ã€‚

# 06
æˆ‘ç‚ºæ‚¨æ•´ç†å¹¾å€‹ç·šä¸Šé‹è¡Œçš„æ–¹æ¡ˆï¼Œå¾æœ€ç°¡å–®åˆ°æœ€å®Œæ•´ï¼š

## ğŸŒŸ æ–¹æ¡ˆ 1: Streamlit Cloudï¼ˆæœ€æ¨è–¦ï¼‰

### å„ªé»ï¼š
- å°ˆç‚º Streamlit è¨­è¨ˆ
- å…è²»
- è‡ªå‹•å¾ GitHub éƒ¨ç½²
- æ”¯æ´ Secrets ç®¡ç†

### éƒ¨ç½²æ­¥é©Ÿï¼š

1. **æº–å‚™ GitHub Repository**
```
story-generator-hub/
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ utils/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ llm_manager.py
    â””â”€â”€ story_processor.py
```

2. **å‰å¾€ Streamlit Cloud**
- è¨ªå• https://streamlit.io/cloud
- ç”¨ GitHub ç™»å…¥

3. **éƒ¨ç½²æ‡‰ç”¨**
- é»æ“Š `New app`
- é¸æ“‡ä½ çš„ repository
- Branch: `main`
- Main file path: `app.py`

4. **è¨­å®š Secrets**
- é»æ“Š `Advanced settings`
- åœ¨ `Secrets` å€åŸŸæ·»åŠ ï¼š
```toml
YOURAPI_KEY = "sk-xxxxx"
STIMA_API_KEY = "stm-xxxxx"
```

5. **é»æ“Š Deploy**

---

## ğŸ¤— æ–¹æ¡ˆ 2: Hugging Face Spaces

### å„ªé»ï¼š
- å…è²»
- AI ç¤¾ç¾¤æ´»èº
- æ”¯æ´ Gradio/Streamlit

### éƒ¨ç½²æ­¥é©Ÿï¼š

1. **å‰µå»º Space**
- å‰å¾€ https://huggingface.co/spaces
- é»æ“Š `Create new Space`
- é¸æ“‡ Streamlit SDK

2. **ä¸Šå‚³æª”æ¡ˆ**

å‰µå»º `app.py`ï¼ˆç¨å¾®ä¿®æ”¹ï¼‰ï¼š
```python
import os
import streamlit as st

# Hugging Face Spaces ä½¿ç”¨ secrets
YOURAPI_KEY = st.secrets.get("YOURAPI_KEY", "")
STIMA_API_KEY = st.secrets.get("STIMA_API_KEY", "")

# è¨­å®šç’°å¢ƒè®Šæ•¸
os.environ['YOURAPI_KEY'] = YOURAPI_KEY
os.environ['STIMA_API_KEY'] = STIMA_API_KEY

# å…¶é¤˜ç¨‹å¼ç¢¼ç›¸åŒ...
```

3. **è¨­å®š Secrets**
- Space Settings â†’ Repository secrets
- æ·»åŠ  API keys

---

## ğŸ”· æ–¹æ¡ˆ 3: Replit

### å„ªé»ï¼š
- ç·šä¸Š IDE
- å³æ™‚é è¦½
- å…è²»å±¤ç´šå¤ ç”¨

### éƒ¨ç½²æ­¥é©Ÿï¼š

1. **å‰µå»º Repl**
- å‰å¾€ https://replit.com
- é»æ“Š `+ Create Repl`
- é¸æ“‡ `Python` template

2. **ä¸Šå‚³æª”æ¡ˆæˆ–å¾ GitHub å°å…¥**
```bash
# åœ¨ Shell ä¸­
git clone https://github.com/YOUR_USERNAME/story-generator-hub.git .
```

3. **è¨­å®š Secrets**
- å·¦å´å·¥å…·åˆ— â†’ `Secrets`
- æ·»åŠ  API keys

4. **ä¿®æ”¹ `.replit` æª”æ¡ˆ**
```toml
run = "pip install -r requirements.txt && streamlit run app.py --server.port 5000"

[env]
PYTHONPATH = "${PYTHONPATH}:${HOME}/${REPL_SLUG}"
```

5. **é»æ“Š Run**

---

## ğŸ“˜ æ–¹æ¡ˆ 4: Google Colab

### å„ªé»ï¼š
- å…è²» GPU
- Google å¸³è™Ÿå³å¯

### éƒ¨ç½²æ­¥é©Ÿï¼š

å‰µå»º notebook `story_generator.ipynb`ï¼š

```python
# Cell 1: å®‰è£å¥—ä»¶
!pip install streamlit openai python-dotenv requests

# Cell 2: ä¸‹è¼‰ç¨‹å¼ç¢¼
!git clone https://github.com/YOUR_USERNAME/story-generator-hub.git
%cd story-generator-hub

# Cell 3: è¨­å®š API Keys
import os
os.environ['YOURAPI_KEY'] = 'sk-xxxxx'  # æ›¿æ›æˆä½ çš„ key
os.environ['STIMA_API_KEY'] = 'stm-xxxxx'  # æ›¿æ›æˆä½ çš„ key

# Cell 4: ä½¿ç”¨ localtunnel å…¬é–‹
!pip install pyngrok
from pyngrok import ngrok

# Cell 5: é‹è¡Œ Streamlit
!streamlit run app.py &
ngrok.set_auth_token("YOUR_NGROK_TOKEN")  # å¾ ngrok.com ç²å–
public_url = ngrok.connect(8501)
print(f"Public URL: {public_url}")
```

---

## ğŸš€ æ–¹æ¡ˆ 5: Railway

### å„ªé»ï¼š
- ç°¡å–®éƒ¨ç½²
- è‡ªå‹• CI/CD
- å…è²»é¡åº¦ $5/æœˆ

### éƒ¨ç½²æ­¥é©Ÿï¼š

1. **å‰µå»º `railway.json`**
```json
{
  "$schema": "https://railway.app/railway.schema.json",
  "build": {
    "builder": "NIXPACKS"
  },
  "deploy": {
    "startCommand": "streamlit run app.py --server.port $PORT --server.address 0.0.0.0",
    "restartPolicyType": "ON_FAILURE",
    "restartPolicyMaxRetries": 10
  }
}
```

2. **å‰µå»º `Procfile`**
```
web: streamlit run app.py --server.port $PORT --server.address 0.0.0.0
```

3. **éƒ¨ç½²åˆ° Railway**
- å‰å¾€ https://railway.app
- é€£æ¥ GitHub repo
- æ·»åŠ ç’°å¢ƒè®Šæ•¸
- è‡ªå‹•éƒ¨ç½²

---

## ğŸŒ æ–¹æ¡ˆ 6: Render

### å„ªé»ï¼š
- å…è²»å±¤ç´š
- è‡ªå‹•éƒ¨ç½²

### éƒ¨ç½²æ­¥é©Ÿï¼š

1. **å‰µå»º `render.yaml`**
```yaml
services:
  - type: web
    name: story-generator
    env: python
    buildCommand: pip install -r requirements.txt
    startCommand: streamlit run app.py --server.port $PORT --server.address 0.0.0.0
    envVars:
      - key: YOURAPI_KEY
        sync: false
      - key: STIMA_API_KEY
        sync: false
```

2. **éƒ¨ç½²**
- å‰å¾€ https://render.com
- New â†’ Web Service
- é€£æ¥ GitHub
- è¨­å®šç’°å¢ƒè®Šæ•¸

---

## ğŸ’¡ å¿«é€Ÿæ¯”è¼ƒ

| å¹³å° | å…è²»é¡åº¦ | éƒ¨ç½²é›£åº¦ | é©åˆå ´æ™¯ |
|------|---------|---------|---------|
| **Streamlit Cloud** | ç„¡é™åˆ¶ | â­ ç°¡å–® | Streamlit å°ˆç”¨ï¼Œæœ€æ¨è–¦ |
| **Hugging Face** | ç„¡é™åˆ¶ | â­â­ ä¸­ç­‰ | AI ç¤¾ç¾¤åˆ†äº« |
| **Replit** | æœ‰é™åˆ¶ | â­ ç°¡å–® | å¿«é€Ÿæ¸¬è©¦ |
| **Google Colab** | 12å°æ™‚/å¤© | â­â­â­ è¤‡é›œ | è‡¨æ™‚ä½¿ç”¨ |
| **Railway** | $5/æœˆ | â­â­ ä¸­ç­‰ | ç”Ÿç”¢ç’°å¢ƒ |
| **Render** | 750å°æ™‚/æœˆ | â­â­ ä¸­ç­‰ | ç”Ÿç”¢ç’°å¢ƒ |

---

## ğŸ¯ æœ€ç°¡å–®æ–¹æ¡ˆï¼šStreamlit Cloud ä¸‰æ­¥é©Ÿ

```bash
# 1. Push åˆ° GitHub
git push origin main

# 2. å‰å¾€ streamlit.io/cloud
# é€£æ¥ GitHub repo

# 3. è¨­å®š Secrets ä¸¦éƒ¨ç½²
# å®Œæˆï¼ç²å¾—å…¬é–‹ URL
```

æ¨è–¦æ‚¨ä½¿ç”¨ **Streamlit Cloud**ï¼Œå› ç‚ºï¼š
1. å®Œå…¨å…è²»
2. å°ˆç‚º Streamlit è¨­è¨ˆ
3. è‡ªå‹•è™•ç†ä¾è³´
4. å…§å»º Secrets ç®¡ç†
5. ç²å¾—æ°¸ä¹…å…¬é–‹ URL

éœ€è¦æˆ‘è©³ç´°èªªæ˜æŸå€‹å¹³å°çš„éƒ¨ç½²éç¨‹å—ï¼Ÿ

#07

æˆ‘å¹«æ‚¨è¨­è¨ˆå°ˆæ¥­çš„ GitHub README æŒ‰éˆ•å’Œå¾½ç« ä¾†å±•ç¤ºæ‚¨çš„ Streamlit æ‡‰ç”¨ï¼š

## ğŸ“Œ README.md æŒ‰éˆ•å’Œå¾½ç« è¨­è¨ˆ

### 1. **å¤§å‹å•Ÿå‹•æŒ‰éˆ•**

```markdown
[![Open in Streamlit](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://prompt-to-storygit.streamlit.app/)
```

æˆ–å½©è‰²ç‰ˆæœ¬ï¼š

```markdown
[![Open in Streamlit](https://static.streamlit.io/badges/streamlit_badge_red.svg)](https://prompt-to-storygit.streamlit.app/)
```

### 2. **è‡ªå®šç¾©æŒ‰éˆ•æ¨£å¼**

```markdown
<div align="center">
  
  # ğŸ­ Story Generator Hub
  
  <a href="https://prompt-to-storygit.streamlit.app/">
    <img src="https://img.shields.io/badge/Open%20in-Streamlit-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white" alt="Open in Streamlit">
  </a>
  
  <a href="https://github.com/YOUR_USERNAME/story-generator-hub">
    <img src="https://img.shields.io/badge/View%20on-GitHub-181717?style=for-the-badge&logo=github&logoColor=white" alt="View on GitHub">
  </a>
  
</div>
```

### 3. **ç‹€æ…‹å¾½ç« çµ„åˆ**

```markdown
<div align="center">

[![Streamlit App](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://prompt-to-storygit.streamlit.app/)
[![Python](https://img.shields.io/badge/Python-3.8+-blue?logo=python&logoColor=white)](https://python.org)
[![Status](https://img.shields.io/badge/Status-Live-success)](https://prompt-to-storygit.streamlit.app/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

</div>
```

### 4. **å®Œæ•´çš„å°ˆæ¥­ README é ‚éƒ¨**

```markdown
<div align="center">
  
  <img src="https://github.com/YOUR_USERNAME/story-generator-hub/assets/YOUR_ID/banner.png" alt="Story Generator Hub Banner" width="100%">
  
  # ğŸ­ Story Generator Hub
  
  ### Multi-Model AI Story Generation Platform
  
  <p align="center">
    <a href="https://prompt-to-storygit.streamlit.app/">
      <img src="https://img.shields.io/badge/ğŸš€%20Live%20Demo-Streamlit-FF4B4B?style=for-the-badge" alt="Live Demo">
    </a>
    <a href="#features">
      <img src="https://img.shields.io/badge/âœ¨%20Features-Explore-blue?style=for-the-badge" alt="Features">
    </a>
    <a href="#usage">
      <img src="https://img.shields.io/badge/ğŸ“–%20Usage-Guide-green?style=for-the-badge" alt="Usage">
    </a>
  </p>

  <p align="center">
    <img src="https://img.shields.io/badge/OpenAI-Compatible-412991?logo=openai&logoColor=white" alt="OpenAI">
    <img src="https://img.shields.io/badge/YourAPI-Supported-orange" alt="YourAPI">
    <img src="https://img.shields.io/badge/Stima-Integrated-purple" alt="Stima">
    <img src="https://img.shields.io/github/stars/YOUR_USERNAME/story-generator-hub?style=social" alt="Stars">
  </p>

</div>

---

## ğŸš€ Try it Now!

<div align="center">
  <a href="https://prompt-to-storygit.streamlit.app/">
    <img src="https://user-images.githubusercontent.com/7164864/217935870-c0bc60a3-6fc0-4047-b011-7b4c59488c91.png" alt="Streamlit App" width="300">
  </a>
  
  **ğŸ‘† Click to Launch the App**
</div>
```

### 5. **å´é‚Šæ¬„å¿«é€Ÿé€£çµ**

```markdown
## ğŸ”— Quick Links

| | |
|:---:|---|
| ğŸš€ | **[Launch App](https://prompt-to-storygit.streamlit.app/)** |
| ğŸ“– | [Documentation](docs/README.md) |
| ğŸ› | [Report Bug](../../issues) |
| ğŸ’¡ | [Request Feature](../../issues) |
```

### 6. **æ¼‚æµ®æŒ‰éˆ•æ•ˆæœ**

```markdown
<p align="center">
  <a href="https://prompt-to-storygit.streamlit.app/">
    <img src="https://img.shields.io/badge/ğŸ­%20Story%20Generator%20Hub-Launch%20App-FF4B4B?style=for-the-badge&labelColor=black&color=FF4B4B" alt="Launch App" height="60">
  </a>
</p>
```

### 7. **å¡ç‰‡å¼è¨­è¨ˆ**

```markdown
<div align="center">
  
```html
<table>
<tr>
<td width="50%">

### ğŸ¯ Live Application

<a href="https://prompt-to-storygit.streamlit.app/">
  <img src="https://static.streamlit.io/badges/streamlit_badge_black_white.svg" alt="Open in Streamlit" width="200">
</a>

**Status:** ğŸŸ¢ Online

</td>
<td width="50%">

### ğŸ› ï¸ Technologies

![Streamlit](https://img.shields.io/badge/Streamlit-FF4B4B?logo=streamlit&logoColor=white)
![Python](https://img.shields.io/badge/Python-3776AB?logo=python&logoColor=white)
![OpenAI](https://img.shields.io/badge/OpenAI-412991?logo=openai&logoColor=white)

</td>
</tr>
</table>
```

</div>
```

### 8. **å‹•æ…‹çµ±è¨ˆè³‡è¨Š**

```markdown
## ğŸ“Š App Statistics

![App Status](https://img.shields.io/website?url=https%3A%2F%2Fprompt-to-storygit.streamlit.app%2F&label=App%20Status)
![Uptime](https://img.shields.io/badge/Uptime-99.9%25-brightgreen)
![Response Time](https://img.shields.io/badge/Response%20Time-<1s-blue)
![Monthly Users](https://img.shields.io/badge/Monthly%20Users-1k+-orange)

[![Launch Story Generator](https://img.shields.io/badge/Launch%20Story%20Generator-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white)](https://prompt-to-storygit.streamlit.app/)
```

### 9. **README å®Œæ•´ç¯„ä¾‹**

```markdown
<div align="center">
  
  # ğŸ­ Story Generator Hub
  
  [![Open in Streamlit](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://prompt-to-storygit.streamlit.app/)
  [![GitHub](https://img.shields.io/github/stars/YOUR_USERNAME/story-generator-hub?style=social)](https://github.com/YOUR_USERNAME/story-generator-hub)
  
</div>

## ğŸŒŸ Features

Generate creative stories using multiple AI models with our [Live App](https://prompt-to-storygit.streamlit.app/)!

- ğŸ¤– **Multi-Model Support** - YourAPI & Stima integration
- ğŸ“š **Smart Plot Clustering** - AI-powered plot organization  
- âœ¨ **Interactive Selection** - Choose your favorite plots
- ğŸ“– **Story Generation** - Create complete stories
- ğŸ’¾ **Markdown Export** - Download your stories

## ğŸš€ Quick Start

### Use Online (Recommended)

Simply visit our deployed application:

<div align="center">
  <a href="https://prompt-to-storygit.streamlit.app/">
    <img src="https://img.shields.io/badge/ğŸš€%20Launch%20App-Story%20Generator%20Hub-FF4B4B?style=for-the-badge" alt="Launch App">
  </a>
</div>

### Run Locally

```bash
git clone https://github.com/YOUR_USERNAME/story-generator-hub.git
cd story-generator-hub
pip install -r requirements.txt
streamlit run app.py
```

## ğŸ“¸ Screenshots

<div align="center">
  <a href="https://prompt-to-storygit.streamlit.app/">
    <img src="screenshots/demo.gif" alt="App Demo" width="80%">
  </a>
  <p><i>Click image to try the live app</i></p>
</div>

## ğŸ”— Links

- ğŸŒ **[Live Application](https://prompt-to-storygit.streamlit.app/)**
- ğŸ“– **[Documentation](docs/README.md)**
- ğŸ› **[Issues](../../issues)**
- ğŸ’¬ **[Discussions](../../discussions)**

---

<div align="center">
  Made with â¤ï¸ using Streamlit
  
  <a href="https://prompt-to-storygit.streamlit.app/">
    <img src="https://img.shields.io/badge/Try%20it%20Now-FF4B4B?style=flat-square" alt="Try it Now">
  </a>
</div>
```

### 10. **GitHub å°ˆç”¨ HTML æŒ‰éˆ•**

å¦‚æœè¦æ›´èŠ±ä¿çš„è¨­è¨ˆï¼Œå¯ä»¥ç”¨ HTMLï¼š

```html
<div align="center">
  <br>
  <a href="https://prompt-to-storygit.streamlit.app/">
    <img src="https://raw.githubusercontent.com/streamlit/streamlit/develop/docs/static/img/favicon.png" width="40">
    <br><br>
    <img src="https://img.shields.io/badge/Launch%20Story%20Generator-Click%20Here-FF4B4B?style=for-the-badge&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAYAAAAfSC3RAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAA7AAAAOwBeShxvQAAABl0RVh0U29mdHdhcmUAd3d3Lmlua3NjYXBlLm9yZ5vuPBoAAADUSURBVCiRrdIxSgNBFMbx/868SEpBLLQIWFjYeQILD2DhCbyAB7C0svAAFh7AyhN4AsFGsBERREEsVNgQWGZ2Zt5nMeyyu8kqfvB47/++x" alt="Launch App">
  </a>
  <br><br>
</div>
```

## ğŸ’¡ å»ºè­°ä½¿ç”¨

æœ€ç°¡å–®ä¸”å°ˆæ¥­çš„çµ„åˆï¼š

```markdown
# ğŸ­ Story Generator Hub

[![Open in Streamlit](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://prompt-to-storygit.streamlit.app/)
[![Python](https://img.shields.io/badge/Python-3.8+-blue?logo=python)](https://python.org)
[![Status](https://img.shields.io/badge/Status-Live-success)](https://prompt-to-storygit.streamlit.app/)

Generate creative stories using multiple AI models.

**ğŸš€ [Try it Live](https://prompt-to-storygit.streamlit.app/)**
```

é€™æ¨£å°±èƒ½åœ¨ README ä¸­å„ªé›…åœ°å±•ç¤ºæ‚¨çš„ Streamlit æ‡‰ç”¨äº†ï¼

# 08

æˆ‘å¹«æ‚¨ç°¡åŒ–ç¨‹å¼ï¼Œåªä¿ç•™ Stima APIï¼š

## ç°¡åŒ–ç‰ˆ `utils/llm_manager.py`ï¼ˆåªç”¨ Stimaï¼‰

```python
import os
from typing import List, Dict, Optional
from dataclasses import dataclass
from openai import OpenAI
import requests
from dotenv import load_dotenv

load_dotenv()

@dataclass
class ModelConfig:
    name: str
    provider: str
    available: bool
    display_name: str
    
class LLMManager:
    def __init__(self):
        self.client = None
        self.models = self._initialize_models()
        
    def _initialize_models(self) -> Dict[str, ModelConfig]:
        """åˆå§‹åŒ– Stima API å’Œæ¨¡å‹åˆ—è¡¨"""
        models = {}
        
        # Stima API è¨­å®š
        if os.getenv('STIMA_API_KEY'):
            self.client = OpenAI(
                api_key=os.getenv('STIMA_API_KEY'),
                base_url="https://api.stima.tech/v1"
            )
            
            # å˜—è©¦å¾ API ç²å–æ¨¡å‹åˆ—è¡¨
            try:
                stima_models = self._fetch_stima_models()
                for model_id in stima_models:
                    models[model_id] = ModelConfig(
                        name=model_id,
                        provider='stima',
                        available=True,
                        display_name=model_id.upper().replace('-', ' ').title()
                    )
            except:
                # å¦‚æœç„¡æ³•ç²å–ï¼Œä½¿ç”¨é è¨­åˆ—è¡¨
                default_stima_models = [
                    ('gpt-4o', 'GPT-4o'),
                    ('gpt-4o-mini', 'GPT-4o Mini'),
                    ('gpt-4-turbo', 'GPT-4 Turbo'),
                    ('gpt-3.5-turbo', 'GPT-3.5 Turbo'),
                    ('claude-3-5-sonnet', 'Claude 3.5 Sonnet'),
                    ('claude-3-opus', 'Claude 3 Opus'),
                    ('claude-3-sonnet', 'Claude 3 Sonnet'),
                    ('claude-3-haiku', 'Claude 3 Haiku'),
                    ('gemini-1.5-pro', 'Gemini 1.5 Pro'),
                    ('gemini-1.5-flash', 'Gemini 1.5 Flash'),
                    ('llama-3.1-70b', 'Llama 3.1 70B'),
                    ('llama-3.1-8b', 'Llama 3.1 8B'),
                    ('mixtral-8x7b', 'Mixtral 8x7B'),
                    ('deepseek-chat', 'DeepSeek Chat'),
                ]
                
                for model_id, display_name in default_stima_models:
                    models[model_id] = ModelConfig(
                        name=model_id,
                        provider='stima',
                        available=True,
                        display_name=display_name
                    )
        
        return models
    
    def _fetch_stima_models(self) -> List[str]:
        """å¾ Stima API ç²å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        try:
            headers = {
                'Authorization': f"Bearer {os.getenv('STIMA_API_KEY')}"
            }
            response = requests.get(
                "https://api.stima.tech/v1/models",
                headers=headers,
                timeout=5
            )
            if response.status_code == 200:
                data = response.json()
                # æ ¹æ“š Stima API çš„å¯¦éš›è¿”å›æ ¼å¼
                return [model['id'] for model in data.get('data', [])]
        except Exception as e:
            print(f"Failed to fetch models: {e}")
        return []
    
    def get_available_models(self) -> List[Dict[str, str]]:
        """ç²å–æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
        return [
            {
                'key': key,
                'display_name': config.display_name
            }
            for key, config in self.models.items() 
            if config.available
        ]
    
    def generate_plot(self, model_key: str, prompt: str) -> str:
        """ä½¿ç”¨æŒ‡å®šæ¨¡å‹ç”Ÿæˆæ•…äº‹æƒ…ç¯€"""
        if not self.client:
            return "Stima API not configured. Please check your API key."
        
        if model_key not in self.models:
            return f"Model {model_key} not available"
        
        config = self.models[model_key]
        
        try:
            response = self.client.chat.completions.create(
                model=config.name,
                messages=[
                    {"role": "system", "content": "You are a creative story writer. Create unique and engaging story plots in Traditional Chinese."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=300,
                temperature=0.8
            )
            return response.choices[0].message.content
            
        except Exception as e:
            return f"Error with {config.display_name}: {str(e)}"
    
    def generate_story(self, model_key: str, plot: str, style: str = "narrative") -> str:
        """æ ¹æ“šæƒ…ç¯€ç”Ÿæˆå®Œæ•´æ•…äº‹"""
        if not self.client:
            return "Stima API not configured. Please check your API key."
            
        if model_key not in self.models:
            return f"Model {model_key} not available"
        
        config = self.models[model_key]
        
        prompt = f"""Based on this plot: {plot}
        
        Write a complete short story in {style} style.
        Make it engaging and approximately 500-800 words.
        Include vivid descriptions, character development, and a satisfying conclusion.
        Write in Traditional Chinese.
        """
        
        try:
            response = self.client.chat.completions.create(
                model=config.name,
                messages=[
                    {"role": "system", "content": f"You are a skilled {style} story writer. Create immersive and captivating stories in Traditional Chinese."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=2000,
                temperature=0.7
            )
            return response.choices[0].message.content
            
        except Exception as e:
            return f"Error with {config.display_name}: {str(e)}"
    
    def cluster_plots_with_ai(self, plots: List[Dict[str, str]]) -> List[Dict]:
        """ä½¿ç”¨ AI æ¨¡å‹å°‡ç›¸ä¼¼çš„æƒ…ç¯€åˆ†ç¾¤"""
        if not self.client:
            return self._simple_clustering(plots)
        
        # é¸æ“‡ä¸€å€‹ä¾¿å®œçš„æ¨¡å‹ä¾†åšåˆ†é¡
        clustering_model = None
        preferred_models = ['gpt-3.5-turbo', 'gpt-4o-mini', 'claude-3-haiku', 'gemini-1.5-flash']
        
        for model in preferred_models:
            if model in self.models:
                clustering_model = model
                break
        
        if not clustering_model and self.models:
            clustering_model = list(self.models.keys())[0]
        
        if not clustering_model:
            return self._simple_clustering(plots)
        
        # æº–å‚™æƒ…ç¯€åˆ—è¡¨
        plot_text = "\n".join([f"{i+1}. {p['plot']}" for i, p in enumerate(plots)])
        
        prompt = f"""
        Analyze these story plots and group similar ones by theme, genre, or narrative elements.
        Return ONLY valid JSON format.
        
        Plots:
        {plot_text}
        
        Required JSON format:
        {{
            "groups": [
                {{
                    "theme": "Brief theme description in English",
                    "plot_indices": [1, 3, 5],
                    "common_elements": "What these plots share"
                }}
            ]
        }}
        """
        
        try:
            response = self.client.chat.completions.create(
                model=clustering_model,
                messages=[
                    {"role": "system", "content": "You are a literary analyst. Return only valid JSON."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=500,
                temperature=0.3
            )
            
            import json
            result_text = response.choices[0].message.content
            # æ¸…ç†å¯èƒ½çš„ markdown æ¨™è¨˜
            if '```json' in result_text:
                result_text = result_text.split('```json')[1].split('```')[0]
            elif '```' in result_text:
                result_text = result_text.split('```')[1].split('```')[0]
                
            result = json.loads(result_text.strip())
            return result['groups']
            
        except Exception as e:
            print(f"AI clustering failed: {e}")
            return self._simple_clustering(plots)
    
    def _simple_clustering(self, plots: List[Dict[str, str]]) -> List[Dict]:
        """ç°¡å–®çš„å‚™ç”¨åˆ†ç¾¤æ–¹æ³•"""
        # å¦‚æœ AI åˆ†ç¾¤å¤±æ•—ï¼Œè¿”å›æ‰€æœ‰æƒ…ç¯€ä½œç‚ºä¸€å€‹ç¾¤çµ„
        return [{
            "theme": "All generated plots",
            "plot_indices": list(range(1, len(plots) + 1)),
            "common_elements": "Various creative story ideas"
        }]
```

## ç°¡åŒ–ç‰ˆ `.env.example`

```env
# Stima API è¨­å®š
STIMA_API_KEY=your_stima_api_key_here
```

## ç°¡åŒ–ç‰ˆ `app.py` çš„å´é‚Šæ¬„éƒ¨åˆ†

```python
# å´é‚Šæ¬„ - æ¨¡å‹è³‡è¨Š
with st.sidebar:
    st.header("âš™ï¸ Configuration")
    
    available_models = st.session_state.llm_manager.get_available_models()
    
    if not available_models:
        st.error("âŒ No models available")
        st.info("""
        Please add your Stima API key in `.env` file:
        
        ```
        STIMA_API_KEY=your_key_here
        ```
        
        Get your API key from:
        https://stima.tech
        """)
    else:
        st.success(f"âœ… {len(available_models)} models available")
        
        with st.expander("ğŸ“‹ Available Models"):
            for model in available_models:
                st.write(f"â€¢ {model['display_name']}")
    
    st.divider()
    
    # API ç‹€æ…‹
    st.subheader("ğŸ“Š API Status")
    if st.session_state.llm_manager.client:
        st.success("âœ… Stima API Connected")
    else:
        st.error("âŒ Stima API Not Connected")
        st.info("Please check your API key")
```

## æ¸¬è©¦è…³æœ¬ `test_stima.py`

```python
from dotenv import load_dotenv
import os
from openai import OpenAI

load_dotenv()

def test_stima():
    api_key = os.getenv('STIMA_API_KEY')
    
    if not api_key:
        print("âŒ STIMA_API_KEY not found in .env file")
        return
    
    print(f"âœ… API Key found: {api_key[:10]}...")
    
    try:
        # å‰µå»ºå®¢æˆ¶ç«¯
        client = OpenAI(
            api_key=api_key,
            base_url="https://api.stima.tech/v1"
        )
        
        # æ¸¬è©¦ç°¡å–®è«‹æ±‚
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": "Say hello in one word"}],
            max_tokens=10
        )
        
        print("âœ… Stima API working!")
        print(f"Response: {response.choices[0].message.content}")
        
        # åˆ—å‡ºå¯ç”¨æ¨¡å‹
        try:
            import requests
            headers = {'Authorization': f'Bearer {api_key}'}
            models_response = requests.get(
                "https://api.stima.tech/v1/models",
                headers=headers
            )
            if models_response.status_code == 200:
                models = models_response.json()
                print("\nğŸ“‹ Available models:")
                for model in models.get('data', [])[:10]:
                    print(f"  â€¢ {model['id']}")
        except:
            pass
            
    except Exception as e:
        print(f"âŒ Stima API error: {e}")

if __name__ == "__main__":
    print("Testing Stima API connection...\n")
    test_stima()
```

## ä½¿ç”¨æ­¥é©Ÿ

1. **è¨­å®šç’°å¢ƒè®Šæ•¸**
```bash
# å‰µå»º .env æª”æ¡ˆ
cp .env.example .env

# ç·¨è¼¯ä¸¦åŠ å…¥ä½ çš„ Stima API key
nano .env
# æˆ–
code .env
```

2. **æ¸¬è©¦é€£ç·š**
```bash
python test_stima.py
```

3. **é‹è¡Œæ‡‰ç”¨**
```bash
streamlit run app.py
```

## ä¸»è¦æ”¹è®Š

1. **ç§»é™¤äº† YourAPI** - æ‰€æœ‰ç›¸é—œç¨‹å¼ç¢¼éƒ½åˆªé™¤äº†
2. **ç°¡åŒ–äº†æ¨¡å‹ç®¡ç†** - åªéœ€è¦ç®¡ç†ä¸€å€‹ API client
3. **ç°¡åŒ–äº†é…ç½®** - åªéœ€è¦ä¸€å€‹ API key
4. **ä¿ç•™æ‰€æœ‰åŠŸèƒ½** - ç”Ÿæˆæƒ…ç¯€ã€åˆ†ç¾¤ã€ç”Ÿæˆæ•…äº‹éƒ½æ­£å¸¸é‹ä½œ

é€™æ¨£ç°¡åŒ–å¾Œï¼Œç¨‹å¼æ›´å®¹æ˜“ç¶­è­·ï¼Œè€Œä¸”åªéœ€è¦ä¸€å€‹ Stima API key å°±èƒ½é‹è¡Œæ‰€æœ‰åŠŸèƒ½ï¼


# 09

é€™å€‹éŒ¯èª¤æ˜¯å› ç‚º `LLMManager` é¡åˆ¥ä¸­çš„ `client` å±¬æ€§å­˜å–å•é¡Œã€‚æˆ‘ä¾†ä¿®æ­£é€™å€‹å•é¡Œï¼š

## ä¿®æ­£ç‰ˆ `utils/llm_manager.py`

åœ¨ `LLMManager` é¡åˆ¥ä¸­æ·»åŠ ä¸€å€‹æ–¹æ³•ä¾†æª¢æŸ¥ API ç‹€æ…‹ï¼š

```python
import os
from typing import List, Dict, Optional
from dataclasses import dataclass
from openai import OpenAI
import requests
from dotenv import load_dotenv

load_dotenv()

@dataclass
class ModelConfig:
    name: str
    provider: str
    available: bool
    display_name: str
    
class LLMManager:
    def __init__(self):
        self.client = None
        self.models = self._initialize_models()
        self.is_connected = False  # æ·»åŠ é€£æ¥ç‹€æ…‹æ¨™è¨˜
        
    def _initialize_models(self) -> Dict[str, ModelConfig]:
        """åˆå§‹åŒ– Stima API å’Œæ¨¡å‹åˆ—è¡¨"""
        models = {}
        
        # Stima API è¨­å®š
        api_key = os.getenv('STIMA_API_KEY')
        if api_key and api_key != 'your_stima_api_key_here':  # æª¢æŸ¥æ˜¯å¦ç‚ºçœŸå¯¦çš„ API key
            try:
                self.client = OpenAI(
                    api_key=api_key,
                    base_url="https://api.stima.tech/v1"
                )
                self.is_connected = True  # æ¨™è¨˜ç‚ºå·²é€£æ¥
                
                # å˜—è©¦å¾ API ç²å–æ¨¡å‹åˆ—è¡¨
                try:
                    stima_models = self._fetch_stima_models()
                    if stima_models:  # å¦‚æœæˆåŠŸç²å–æ¨¡å‹åˆ—è¡¨
                        for model_id in stima_models:
                            models[model_id] = ModelConfig(
                                name=model_id,
                                provider='stima',
                                available=True,
                                display_name=model_id.upper().replace('-', ' ').title()
                            )
                    else:
                        raise Exception("No models fetched")
                except:
                    # å¦‚æœç„¡æ³•ç²å–ï¼Œä½¿ç”¨é è¨­åˆ—è¡¨
                    default_stima_models = [
                        ('gpt-4o', 'GPT-4o'),
                        ('gpt-4o-mini', 'GPT-4o Mini'),
                        ('gpt-4-turbo', 'GPT-4 Turbo'),
                        ('gpt-3.5-turbo', 'GPT-3.5 Turbo'),
                        ('claude-3-5-sonnet', 'Claude 3.5 Sonnet'),
                        ('claude-3-opus', 'Claude 3 Opus'),
                        ('claude-3-sonnet', 'Claude 3 Sonnet'),
                        ('claude-3-haiku', 'Claude 3 Haiku'),
                        ('gemini-1.5-pro', 'Gemini 1.5 Pro'),
                        ('gemini-1.5-flash', 'Gemini 1.5 Flash'),
                        ('llama-3.1-70b', 'Llama 3.1 70B'),
                        ('llama-3.1-8b', 'Llama 3.1 8B'),
                        ('mixtral-8x7b', 'Mixtral 8x7B'),
                        ('deepseek-chat', 'DeepSeek Chat'),
                    ]
                    
                    for model_id, display_name in default_stima_models:
                        models[model_id] = ModelConfig(
                            name=model_id,
                            provider='stima',
                            available=True,
                            display_name=display_name
                        )
            except Exception as e:
                print(f"Failed to initialize Stima client: {e}")
                self.is_connected = False
        
        return models
    
    def _fetch_stima_models(self) -> List[str]:
        """å¾ Stima API ç²å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        try:
            api_key = os.getenv('STIMA_API_KEY')
            if not api_key:
                return []
                
            headers = {
                'Authorization': f"Bearer {api_key}"
            }
            response = requests.get(
                "https://api.stima.tech/v1/models",
                headers=headers,
                timeout=5
            )
            if response.status_code == 200:
                data = response.json()
                # æ ¹æ“š Stima API çš„å¯¦éš›è¿”å›æ ¼å¼
                models = [model['id'] for model in data.get('data', [])]
                return models if models else []
        except Exception as e:
            print(f"Failed to fetch models: {e}")
        return []
    
    def is_api_connected(self) -> bool:
        """æª¢æŸ¥ API æ˜¯å¦å·²é€£æ¥"""
        return self.is_connected and self.client is not None
    
    def get_available_models(self) -> List[Dict[str, str]]:
        """ç²å–æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
        return [
            {
                'key': key,
                'display_name': config.display_name
            }
            for key, config in self.models.items() 
            if config.available
        ]
    
    def generate_plot(self, model_key: str, prompt: str) -> str:
        """ä½¿ç”¨æŒ‡å®šæ¨¡å‹ç”Ÿæˆæ•…äº‹æƒ…ç¯€"""
        if not self.is_api_connected():
            return "Stima API not configured. Please check your API key."
        
        if model_key not in self.models:
            return f"Model {model_key} not available"
        
        config = self.models[model_key]
        
        try:
            response = self.client.chat.completions.create(
                model=config.name,
                messages=[
                    {"role": "system", "content": "You are a creative story writer. Create unique and engaging story plots in Traditional Chinese."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=300,
                temperature=0.8
            )
            return response.choices[0].message.content
            
        except Exception as e:
            return f"Error with {config.display_name}: {str(e)}"
    
    def generate_story(self, model_key: str, plot: str, style: str = "narrative") -> str:
        """æ ¹æ“šæƒ…ç¯€ç”Ÿæˆå®Œæ•´æ•…äº‹"""
        if not self.is_api_connected():
            return "Stima API not configured. Please check your API key."
            
        if model_key not in self.models:
            return f"Model {model_key} not available"
        
        config = self.models[model_key]
        
        prompt = f"""Based on this plot: {plot}
        
        Write a complete short story in {style} style.
        Make it engaging and approximately 500-800 words.
        Include vivid descriptions, character development, and a satisfying conclusion.
        Write in Traditional Chinese.
        """
        
        try:
            response = self.client.chat.completions.create(
                model=config.name,
                messages=[
                    {"role": "system", "content": f"You are a skilled {style} story writer. Create immersive and captivating stories in Traditional Chinese."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=2000,
                temperature=0.7
            )
            return response.choices[0].message.content
            
        except Exception as e:
            return f"Error with {config.display_name}: {str(e)}"
    
    def cluster_plots_with_ai(self, plots: List[Dict[str, str]]) -> List[Dict]:
        """ä½¿ç”¨ AI æ¨¡å‹å°‡ç›¸ä¼¼çš„æƒ…ç¯€åˆ†ç¾¤"""
        if not self.is_api_connected():
            return self._simple_clustering(plots)
        
        # é¸æ“‡ä¸€å€‹ä¾¿å®œçš„æ¨¡å‹ä¾†åšåˆ†é¡
        clustering_model = None
        preferred_models = ['gpt-3.5-turbo', 'gpt-4o-mini', 'claude-3-haiku', 'gemini-1.5-flash']
        
        for model in preferred_models:
            if model in self.models:
                clustering_model = model
                break
        
        if not clustering_model and self.models:
            clustering_model = list(self.models.keys())[0]
        
        if not clustering_model:
            return self._simple_clustering(plots)
        
        # æº–å‚™æƒ…ç¯€åˆ—è¡¨
        plot_text = "\n".join([f"{i+1}. {p['plot']}" for i, p in enumerate(plots)])
        
        prompt = f"""
        Analyze these story plots and group similar ones by theme, genre, or narrative elements.
        Return ONLY valid JSON format.
        
        Plots:
        {plot_text}
        
        Required JSON format:
        {{
            "groups": [
                {{
                    "theme": "Brief theme description in English",
                    "plot_indices": [1, 3, 5],
                    "common_elements": "What these plots share"
                }}
            ]
        }}
        """
        
        try:
            response = self.client.chat.completions.create(
                model=clustering_model,
                messages=[
                    {"role": "system", "content": "You are a literary analyst. Return only valid JSON."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=500,
                temperature=0.3
            )
            
            import json
            result_text = response.choices[0].message.content
            # æ¸…ç†å¯èƒ½çš„ markdown æ¨™è¨˜
            if '```json' in result_text:
                result_text = result_text.split('```json')[1].split('```')[0]
            elif '```' in result_text:
                result_text = result_text.split('```')[1].split('```')[0]
                
            result = json.loads(result_text.strip())
            return result['groups']
            
        except Exception as e:
            print(f"AI clustering failed: {e}")
            return self._simple_clustering(plots)
    
    def _simple_clustering(self, plots: List[Dict[str, str]]) -> List[Dict]:
        """ç°¡å–®çš„å‚™ç”¨åˆ†ç¾¤æ–¹æ³•"""
        # å¦‚æœ AI åˆ†ç¾¤å¤±æ•—ï¼Œè¿”å›æ‰€æœ‰æƒ…ç¯€ä½œç‚ºä¸€å€‹ç¾¤çµ„
        return [{
            "theme": "All generated plots",
            "plot_indices": list(range(1, len(plots) + 1)),
            "common_elements": "Various creative story ideas"
        }]
```

## ä¿®æ­£ç‰ˆ `app.py` å´é‚Šæ¬„éƒ¨åˆ†

```python
# å´é‚Šæ¬„ - æ¨¡å‹è³‡è¨Š
with st.sidebar:
    st.header("âš™ï¸ Configuration")
    
    available_models = st.session_state.llm_manager.get_available_models()
    
    if not available_models:
        st.error("âŒ No models available")
        st.info("""
        Please add your Stima API key in `.env` file:
        
        ```
        STIMA_API_KEY=your_key_here
        ```
        
        Get your API key from:
        https://stima.tech
        """)
    else:
        st.success(f"âœ… {len(available_models)} models available")
        
        with st.expander("ğŸ“‹ Available Models"):
            for model in available_models:
                st.write(f"â€¢ {model['display_name']}")
    
    st.divider()
    
    # API ç‹€æ…‹ - ä½¿ç”¨æ–°çš„æ–¹æ³•
    st.subheader("ğŸ“Š API Status")
    if st.session_state.llm_manager.is_api_connected():
        st.success("âœ… Stima API Connected")
    else:
        st.error("âŒ Stima API Not Connected")
        st.info("Please check your API key")
```

## å¦‚æœæ‚¨ä½¿ç”¨ Streamlit Cloud

åœ¨ Streamlit Cloud ä¸Šï¼Œæ‚¨æ‡‰è©²ä½¿ç”¨ Secrets ç®¡ç†è€Œä¸æ˜¯ `.env` æ–‡ä»¶ã€‚ä¿®æ”¹ `llm_manager.py` çš„é–‹é ­ï¼š

```python
import os
import streamlit as st
from typing import List, Dict, Optional
from dataclasses import dataclass
from openai import OpenAI
import requests

# é‡å° Streamlit Cloud çš„è¨­å®š
def get_api_key():
    """ç²å– API Key - æ”¯æ´ Streamlit Cloud Secrets å’Œæœ¬åœ° .env"""
    # é¦–å…ˆå˜—è©¦å¾ Streamlit secrets ç²å–
    try:
        if 'STIMA_API_KEY' in st.secrets:
            return st.secrets['STIMA_API_KEY']
    except:
        pass
    
    # ç„¶å¾Œå˜—è©¦å¾ç’°å¢ƒè®Šæ•¸ç²å–
    api_key = os.getenv('STIMA_API_KEY')
    
    # å¦‚æœéƒ½æ²’æœ‰ï¼Œå˜—è©¦å¾ .env è¼‰å…¥
    if not api_key:
        try:
            from dotenv import load_dotenv
            load_dotenv()
            api_key = os.getenv('STIMA_API_KEY')
        except:
            pass
    
    return api_key

# åœ¨ _initialize_models æ–¹æ³•ä¸­ä½¿ç”¨
def _initialize_models(self) -> Dict[str, ModelConfig]:
    """åˆå§‹åŒ– Stima API å’Œæ¨¡å‹åˆ—è¡¨"""
    models = {}
    
    # ç²å– API Key
    api_key = get_api_key()
    
    if api_key and api_key != 'your_stima_api_key_here':
        try:
            self.client = OpenAI(
                api_key=api_key,
                base_url="https://api.stima.tech/v1"
            )
            self.is_connected = True
            # ... å…¶é¤˜ç¨‹å¼ç¢¼ç›¸åŒ
```

## åœ¨ Streamlit Cloud è¨­å®š Secrets

1. åœ¨ Streamlit Cloud çš„æ‡‰ç”¨ç®¡ç†é é¢
2. é»æ“Š "Settings" â†’ "Secrets"
3. æ·»åŠ ï¼š
```toml
STIMA_API_KEY = "your-actual-api-key-here"
```

é€™æ¨£ä¿®æ­£æ‡‰è©²èƒ½è§£æ±º AttributeError å•é¡Œï¼


# 10

é€™å€‹éŒ¯èª¤è¡¨ç¤º `LLMManager` é¡åˆ¥æ²’æœ‰ `is_api_connected` æ–¹æ³•ã€‚è®“æˆ‘æä¾›ä¸€å€‹å¿«é€Ÿä¿®å¾©æ–¹æ¡ˆï¼š

## æ–¹æ¡ˆ 1: ä¿®æ”¹ `app.py`ï¼ˆå¿«é€Ÿä¿®å¾©ï¼‰

åœ¨ `app.py` ä¸­ï¼Œå°‡å´é‚Šæ¬„çš„ API ç‹€æ…‹æª¢æŸ¥éƒ¨åˆ†æ”¹ç‚ºï¼š

```python
# å´é‚Šæ¬„ - æ¨¡å‹è³‡è¨Š
with st.sidebar:
    st.header("âš™ï¸ Configuration")
    
    available_models = st.session_state.llm_manager.get_available_models()
    
    if not available_models:
        st.error("âŒ No models available")
        st.info("""
        Please add your Stima API key in Secrets:
        
        1. Click 'Manage app' (bottom right)
        2. Go to Settings â†’ Secrets
        3. Add: STIMA_API_KEY = "your_key"
        
        Get your API key from:
        https://stima.tech
        """)
    else:
        st.success(f"âœ… {len(available_models)} models available")
        
        with st.expander("ğŸ“‹ Available Models"):
            for model in available_models:
                st.write(f"â€¢ {model['display_name']}")
    
    st.divider()
    
    # API ç‹€æ…‹ - ç°¡åŒ–æª¢æŸ¥æ–¹å¼
    st.subheader("ğŸ“Š API Status")
    # æª¢æŸ¥æ˜¯å¦æœ‰ client å±¬æ€§ä¸”ä¸ç‚º None
    try:
        if hasattr(st.session_state.llm_manager, 'client') and st.session_state.llm_manager.client:
            st.success("âœ… Stima API Connected")
        else:
            st.error("âŒ Stima API Not Connected")
            st.info("Please check your API key")
    except:
        # å¦‚æœæœ‰ä»»ä½•éŒ¯èª¤ï¼Œæ ¹æ“šæ˜¯å¦æœ‰å¯ç”¨æ¨¡å‹ä¾†åˆ¤æ–·
        if available_models:
            st.success("âœ… API Connected")
        else:
            st.error("âŒ API Not Connected")
```

## æ–¹æ¡ˆ 2: å®Œæ•´æ›´æ–° `utils/llm_manager.py`

ç¢ºä¿æ‚¨çš„ `llm_manager.py` åŒ…å« `is_api_connected` æ–¹æ³•ã€‚é€™æ˜¯é—œéµéƒ¨åˆ†ï¼š

```python
class LLMManager:
    def __init__(self):
        self.client = None
        self.models = self._initialize_models()
        self.is_connected = False  # æ·»åŠ é€™è¡Œ
        
    # ... å…¶ä»–ä»£ç¢¼ ...
    
    def is_api_connected(self) -> bool:
        """æª¢æŸ¥ API æ˜¯å¦å·²é€£æ¥"""
        return self.is_connected and self.client is not None
```

## æ–¹æ¡ˆ 3: æœ€ç°¡å–®çš„ä¿®å¾©

å¦‚æœæ‚¨ä¸æƒ³ä¿®æ”¹å¤ªå¤šï¼Œåªè¦åœ¨ `app.py` ä¸­æŠŠé‚£æ®µæª¢æŸ¥æ”¹ç‚ºï¼š

```python
    # API ç‹€æ…‹
    st.subheader("ğŸ“Š API Status")
    # ç”¨å¯ç”¨æ¨¡å‹æ•¸é‡ä¾†åˆ¤æ–·é€£æ¥ç‹€æ…‹
    if len(available_models) > 0:
        st.success("âœ… Stima API Connected")
        st.info(f"Found {len(available_models)} models")
    else:
        st.error("âŒ Stima API Not Connected")
        st.info("Please add your API key in Secrets")
```

## é‡å° Streamlit Cloud çš„å®Œæ•´ä¿®å¾©

å¦‚æœæ‚¨åœ¨ Streamlit Cloud ä¸Šé‹è¡Œï¼Œé€™è£¡æ˜¯å®Œæ•´çš„ `utils/llm_manager.py` é©é…ç‰ˆæœ¬ï¼š

```python
import os
import streamlit as st
from typing import List, Dict, Optional
from dataclasses import dataclass
from openai import OpenAI
import requests

@dataclass
class ModelConfig:
    name: str
    provider: str
    available: bool
    display_name: str

class LLMManager:
    def __init__(self):
        self.client = None
        self.models = self._initialize_models()
    
    def _get_api_key(self):
        """ç²å– API Key - æ”¯æ´ Streamlit Cloud Secrets"""
        # å„ªå…ˆå¾ Streamlit secrets ç²å–
        try:
            return st.secrets.get("STIMA_API_KEY", None)
        except:
            # å¦‚æœä¸åœ¨ Streamlit Cloudï¼Œå¾ç’°å¢ƒè®Šæ•¸ç²å–
            api_key = os.getenv('STIMA_API_KEY')
            if not api_key:
                try:
                    from dotenv import load_dotenv
                    load_dotenv()
                    api_key = os.getenv('STIMA_API_KEY')
                except:
                    pass
            return api_key
    
    def _initialize_models(self) -> Dict[str, ModelConfig]:
        """åˆå§‹åŒ– Stima API å’Œæ¨¡å‹åˆ—è¡¨"""
        models = {}
        
        api_key = self._get_api_key()
        
        if api_key and api_key != 'your_stima_api_key_here':
            try:
                self.client = OpenAI(
                    api_key=api_key,
                    base_url="https://api.stima.tech/v1"
                )
                
                # ä½¿ç”¨é è¨­æ¨¡å‹åˆ—è¡¨
                default_stima_models = [
                    ('gpt-4o', 'GPT-4o'),
                    ('gpt-4o-mini', 'GPT-4o Mini'),
                    ('gpt-4-turbo', 'GPT-4 Turbo'),
                    ('gpt-3.5-turbo', 'GPT-3.5 Turbo'),
                    ('claude-3-5-sonnet', 'Claude 3.5 Sonnet'),
                    ('claude-3-opus', 'Claude 3 Opus'),
                    ('claude-3-sonnet', 'Claude 3 Sonnet'),
                    ('claude-3-haiku', 'Claude 3 Haiku'),
                    ('gemini-1.5-pro', 'Gemini 1.5 Pro'),
                    ('gemini-1.5-flash', 'Gemini 1.5 Flash'),
                    ('llama-3.1-70b', 'Llama 3.1 70B'),
                    ('llama-3.1-8b', 'Llama 3.1 8B'),
                    ('mixtral-8x7b', 'Mixtral 8x7B'),
                    ('deepseek-chat', 'DeepSeek Chat'),
                ]
                
                for model_id, display_name in default_stima_models:
                    models[model_id] = ModelConfig(
                        name=model_id,
                        provider='stima',
                        available=True,
                        display_name=display_name
                    )
                    
            except Exception as e:
                st.error(f"Failed to initialize API: {e}")
        
        return models
    
    def is_api_connected(self) -> bool:
        """æª¢æŸ¥ API æ˜¯å¦å·²é€£æ¥"""
        return self.client is not None
    
    # ... å…¶é¤˜æ–¹æ³•ä¿æŒä¸è®Š ...
```

é¸æ“‡æœ€é©åˆæ‚¨æƒ…æ³çš„æ–¹æ¡ˆã€‚å¦‚æœæ‚¨å·²ç¶“éƒ¨ç½²åœ¨ Streamlit Cloudï¼Œ**æ–¹æ¡ˆ 1 æˆ–æ–¹æ¡ˆ 3** æ˜¯æœ€å¿«çš„ä¿®å¾©æ–¹æ³•ï¼Œåªéœ€è¦ä¿®æ”¹ `app.py` å³å¯ã€‚
