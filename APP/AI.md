# 01


æˆ‘å¹«æ‚¨å‰µå»ºå°ˆæ¥­çš„ GitHub README æ–‡ä»¶ï¼š

## è‹±æ–‡ç‰ˆ README.md

```markdown
# Story Generator Hub

ğŸ­ A multi-model collaborative story generation platform powered by various LLMs

## ğŸ“– Overview

Story Generator Hub is an innovative platform that leverages multiple Large Language Models (LLMs) to generate diverse story plots, intelligently categorize them, and create complete stories based on user preferences.

## âœ¨ Features

- **Multi-Model Plot Generation**: Generate diverse story plots using up to 5 different LLM models simultaneously
- **Intelligent Clustering**: Automatically group similar plots using GPT for better organization
- **Interactive Selection**: Users can select preferred plot clusters to guide story generation
- **Collaborative Story Creation**: Generate complete stories using up to 3 models for richer narratives
- **Markdown Export**: All stories are exported in clean Markdown format
- **Random Model Selection**: Option to randomly select models for varied results

## ğŸš€ How It Works

### Phase 1: Plot Generation
1. Select up to 5 LLM models (or choose random selection)
2. Generate multiple story plots from each model
3. GPT analyzes and clusters similar plots automatically

### Phase 2: Plot Selection
1. Review categorized plot clusters
2. Select preferred plot groups
3. Confirm selections for story generation

### Phase 3: Story Generation
1. Choose up to 3 models for story generation (or random)
2. Generate complete stories based on selected plots
3. Export stories in Markdown format

## ğŸ› ï¸ Technology Stack

- **Frontend**: [Your frontend framework]
- **Backend**: [Your backend framework]
- **LLM Integration**: Multiple model APIs (GPT, Claude, Gemini, etc.)
- **Classification Engine**: GPT for plot clustering and categorization

## ğŸ“¦ Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/story-generator-hub.git

# Navigate to project directory
cd story-generator-hub

# Install dependencies
npm install

# Set up environment variables
cp .env.example .env
# Add your API keys to .env

# Start the application
npm start
```

## ğŸ”§ Configuration

Create a `.env` file with the following variables:

```env
OPENAI_API_KEY=your_openai_key
ANTHROPIC_API_KEY=your_anthropic_key
GOOGLE_AI_KEY=your_google_key
# Add other model API keys as needed
```

## ğŸ“ Usage

1. **Initial Setup**: Configure your preferred LLM models in settings
2. **Generate Plots**: Click "Generate Plots" and select your models
3. **Review & Select**: Browse categorized plots and select interesting ones
4. **Generate Story**: Choose models and generate your complete story
5. **Export**: Download your story in Markdown format

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Thanks to all LLM providers for their APIs
- Community contributors and testers
- Open source libraries used in this project

## ğŸ“§ Contact

- Project Link: [https://github.com/yourusername/story-generator-hub](https://github.com/yourusername/story-generator-hub)
- Issues: [https://github.com/yourusername/story-generator-hub/issues](https://github.com/yourusername/story-generator-hub/issues)
```

## ä¸­æ–‡ç‰ˆ README_zh-TW.md

```markdown
# Story Generator Hub æ•…äº‹ç”Ÿæˆå™¨ä¸­å¿ƒ

ğŸ­ åŸºæ–¼å¤šå€‹å¤§å‹èªè¨€æ¨¡å‹å”ä½œçš„æ•…äº‹ç”Ÿæˆå¹³å°

## ğŸ“– å°ˆæ¡ˆç°¡ä»‹

Story Generator Hub æ˜¯ä¸€å€‹å‰µæ–°çš„æ•…äº‹ç”Ÿæˆå¹³å°ï¼Œé€éæ•´åˆå¤šå€‹å¤§å‹èªè¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¾†ç”Ÿæˆå¤šæ¨£åŒ–çš„æ•…äº‹æƒ…ç¯€ï¼Œä¸¦æ™ºæ…§åˆ†é¡æ­¸ç´ï¼Œæœ€çµ‚æ ¹æ“šä½¿ç”¨è€…åå¥½å‰µä½œå®Œæ•´çš„æ•…äº‹ã€‚

## âœ¨ æ ¸å¿ƒåŠŸèƒ½

- **å¤šæ¨¡å‹æƒ…ç¯€ç”Ÿæˆ**ï¼šåŒæ™‚ä½¿ç”¨æœ€å¤š 5 å€‹ä¸åŒçš„ LLM æ¨¡å‹ç”Ÿæˆå¤šæ¨£åŒ–çš„æ•…äº‹æƒ…ç¯€
- **æ™ºæ…§åˆ†é¡æ­¸ç´**ï¼šä½¿ç”¨ GPT è‡ªå‹•å°‡ç›¸ä¼¼æƒ…ç¯€åˆ†çµ„ï¼Œä¾¿æ–¼ç®¡ç†å’Œé¸æ“‡
- **äº’å‹•å¼é¸æ“‡**ï¼šä½¿ç”¨è€…å¯å‹¾é¸æ„Ÿèˆˆè¶£çš„æƒ…ç¯€ç¾¤çµ„ä¾†å¼•å°æ•…äº‹ç”Ÿæˆ
- **å”ä½œå¼æ•…äº‹å‰µä½œ**ï¼šä½¿ç”¨æœ€å¤š 3 å€‹æ¨¡å‹ç”Ÿæˆå®Œæ•´æ•…äº‹ï¼Œå‰µé€ æ›´è±å¯Œçš„æ•˜äº‹
- **Markdown è¼¸å‡º**ï¼šæ‰€æœ‰æ•…äº‹çš†ä»¥ç°¡æ½”çš„ Markdown æ ¼å¼åŒ¯å‡º
- **éš¨æ©Ÿæ¨¡å‹é¸æ“‡**ï¼šæä¾›éš¨æ©Ÿé¸æ“‡æ¨¡å‹é¸é …ï¼Œå¢åŠ çµæœå¤šæ¨£æ€§

## ğŸš€ é‹ä½œæµç¨‹

### ç¬¬ä¸€éšæ®µï¼šæƒ…ç¯€ç”Ÿæˆ
1. é¸æ“‡æœ€å¤š 5 å€‹ LLM æ¨¡å‹ï¼ˆæˆ–é¸æ“‡éš¨æ©Ÿï¼‰
2. æ¯å€‹æ¨¡å‹ç”Ÿæˆå¤šå€‹æ•…äº‹æƒ…ç¯€
3. GPT è‡ªå‹•åˆ†æä¸¦å°‡ç›¸ä¼¼æƒ…ç¯€åˆ†é¡

### ç¬¬äºŒéšæ®µï¼šæƒ…ç¯€ç¯©é¸
1. ç€è¦½åˆ†é¡å¾Œçš„æƒ…ç¯€ç¾¤çµ„
2. å‹¾é¸åå¥½çš„æƒ…ç¯€ç¾¤çµ„
3. ç¢ºèªé¸æ“‡ä»¥é€²è¡Œæ•…äº‹ç”Ÿæˆ

### ç¬¬ä¸‰éšæ®µï¼šæ•…äº‹ç”Ÿæˆ
1. é¸æ“‡æœ€å¤š 3 å€‹æ¨¡å‹é€²è¡Œæ•…äº‹ç”Ÿæˆï¼ˆæˆ–éš¨æ©Ÿï¼‰
2. åŸºæ–¼é¸å®šæƒ…ç¯€ç”Ÿæˆå®Œæ•´æ•…äº‹
3. ä»¥ Markdown æ ¼å¼åŒ¯å‡ºæ•…äº‹

## ğŸ› ï¸ æŠ€è¡“æ¶æ§‹

- **å‰ç«¯æ¡†æ¶**ï¼š[æ‚¨çš„å‰ç«¯æ¡†æ¶]
- **å¾Œç«¯æ¡†æ¶**ï¼š[æ‚¨çš„å¾Œç«¯æ¡†æ¶]
- **LLM æ•´åˆ**ï¼šå¤šå€‹æ¨¡å‹ APIï¼ˆGPTã€Claudeã€Gemini ç­‰ï¼‰
- **åˆ†é¡å¼•æ“**ï¼šGPT è² è²¬æƒ…ç¯€åˆ†é¡å’Œæ­¸ç´

## ğŸ“¦ å®‰è£æŒ‡å—

```bash
# è¤‡è£½å°ˆæ¡ˆ
git clone https://github.com/yourusername/story-generator-hub.git

# é€²å…¥å°ˆæ¡ˆç›®éŒ„
cd story-generator-hub

# å®‰è£ä¾è³´å¥—ä»¶
npm install

# è¨­å®šç’°å¢ƒè®Šæ•¸
cp .env.example .env
# åœ¨ .env ä¸­åŠ å…¥æ‚¨çš„ API é‡‘é‘°

# å•Ÿå‹•æ‡‰ç”¨ç¨‹å¼
npm start
```

## ğŸ”§ ç’°å¢ƒè¨­å®š

å»ºç«‹ `.env` æª”æ¡ˆä¸¦åŠ å…¥ä»¥ä¸‹è®Šæ•¸ï¼š

```env
OPENAI_API_KEY=your_openai_key
ANTHROPIC_API_KEY=your_anthropic_key
GOOGLE_AI_KEY=your_google_key
# æ ¹æ“šéœ€è¦åŠ å…¥å…¶ä»–æ¨¡å‹çš„ API é‡‘é‘°
```

## ğŸ“ ä½¿ç”¨èªªæ˜

1. **åˆå§‹è¨­å®š**ï¼šåœ¨è¨­å®šä¸­é…ç½®æ‚¨åå¥½çš„ LLM æ¨¡å‹
2. **ç”Ÿæˆæƒ…ç¯€**ï¼šé»æ“Šã€Œç”Ÿæˆæƒ…ç¯€ã€ä¸¦é¸æ“‡æ¨¡å‹
3. **æª¢è¦–èˆ‡é¸æ“‡**ï¼šç€è¦½åˆ†é¡å¾Œçš„æƒ…ç¯€ä¸¦å‹¾é¸æ„Ÿèˆˆè¶£çš„é …ç›®
4. **ç”Ÿæˆæ•…äº‹**ï¼šé¸æ“‡æ¨¡å‹ä¸¦ç”Ÿæˆå®Œæ•´æ•…äº‹
5. **åŒ¯å‡º**ï¼šä»¥ Markdown æ ¼å¼ä¸‹è¼‰æ‚¨çš„æ•…äº‹

## ğŸ¤ è²¢ç»æŒ‡å—

æ­¡è¿è²¢ç»ï¼è©³ç´°è³‡è¨Šè«‹åƒé–± [è²¢ç»æŒ‡å—](CONTRIBUTING.md)ã€‚

## ğŸ“„ æˆæ¬Šæ¢æ¬¾

æœ¬å°ˆæ¡ˆæ¡ç”¨ MIT æˆæ¬Šæ¢æ¬¾ - è©³è¦‹ [LICENSE](LICENSE) æª”æ¡ˆã€‚

## ğŸ™ è‡´è¬

- æ„Ÿè¬æ‰€æœ‰ LLM æä¾›å•†çš„ API æ”¯æ´
- ç¤¾ç¾¤è²¢ç»è€…å’Œæ¸¬è©¦äººå“¡
- æœ¬å°ˆæ¡ˆä½¿ç”¨çš„é–‹æºå‡½å¼åº«

## ğŸ“§ è¯çµ¡æ–¹å¼

- å°ˆæ¡ˆé€£çµï¼š[https://github.com/yourusername/story-generator-hub](https://github.com/yourusername/story-generator-hub)
- å•é¡Œå›å ±ï¼š[https://github.com/yourusername/story-generator-hub/issues](https://github.com/yourusername/story-generator-hub/issues)
```

é€™å…©å€‹ç‰ˆæœ¬çš„ README éƒ½åŒ…å«äº†æ‚¨å°ˆæ¡ˆçš„æ ¸å¿ƒæ¦‚å¿µï¼Œä¸¦ä»¥å°ˆæ¥­çš„æ ¼å¼å‘ˆç¾ã€‚æ‚¨å¯ä»¥æ ¹æ“šå¯¦éš›çš„æŠ€è¡“é¸æ“‡å’Œå°ˆæ¡ˆåç¨±é€²è¡Œèª¿æ•´ã€‚éœ€è¦æˆ‘å”åŠ©æ‚¨æ·»åŠ å…¶ä»–ç« ç¯€æˆ–èª¿æ•´å…§å®¹å—ï¼Ÿ

# 02

æˆ‘å¹«æ‚¨è¨­è¨ˆä¸€å€‹æ¼¸é€²å¼çš„å¯¦ç¾æ–¹æ¡ˆï¼Œå¾æœ€ç°¡å–®å¯é‹è¡Œçš„ç‰ˆæœ¬é–‹å§‹ï¼š

## ç‰ˆæœ¬ 1: æœ€åŸºç¤ç‰ˆæœ¬ï¼ˆä½¿ç”¨ Streamlit + OpenAIï¼‰

### 1. é¦–å…ˆåœ¨ Codespace å»ºç«‹å°ˆæ¡ˆçµæ§‹

```bash
story-generator-hub/
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ utils/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ llm_manager.py
    â””â”€â”€ story_processor.py
```

### 2. `requirements.txt`

```txt
streamlit
openai
python-dotenv
anthropic
google-generativeai
pandas
```

### 3. `.env.example`

```env
OPENAI_API_KEY=your_key_here
ANTHROPIC_API_KEY=your_key_here
GOOGLE_API_KEY=your_key_here
```

### 4. `utils/llm_manager.py` - LLM ç®¡ç†å™¨

```python
import os
from typing import List, Dict, Optional
from dataclasses import dataclass
import openai
import anthropic
import google.generativeai as genai
from dotenv import load_dotenv

load_dotenv()

@dataclass
class ModelConfig:
    name: str
    provider: str
    available: bool
    
class LLMManager:
    def __init__(self):
        self.models = self._initialize_models()
        
    def _initialize_models(self) -> Dict[str, ModelConfig]:
        """æª¢æŸ¥å“ªäº›æ¨¡å‹å¯ç”¨"""
        models = {}
        
        # OpenAI
        if os.getenv('OPENAI_API_KEY'):
            openai.api_key = os.getenv('OPENAI_API_KEY')
            models['gpt-3.5-turbo'] = ModelConfig('GPT-3.5', 'openai', True)
            models['gpt-4'] = ModelConfig('GPT-4', 'openai', True)
        
        # Anthropic
        if os.getenv('ANTHROPIC_API_KEY'):
            self.anthropic_client = anthropic.Anthropic(
                api_key=os.getenv('ANTHROPIC_API_KEY')
            )
            models['claude-3-haiku'] = ModelConfig('Claude 3 Haiku', 'anthropic', True)
        
        # Google
        if os.getenv('GOOGLE_API_KEY'):
            genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))
            models['gemini-pro'] = ModelConfig('Gemini Pro', 'google', True)
            
        return models
    
    def get_available_models(self) -> List[str]:
        """ç²å–æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
        return [name for name, config in self.models.items() if config.available]
    
    def generate_plot(self, model_name: str, prompt: str) -> str:
        """ä½¿ç”¨æŒ‡å®šæ¨¡å‹ç”Ÿæˆæ•…äº‹æƒ…ç¯€"""
        if model_name not in self.models:
            return f"Model {model_name} not available"
        
        config = self.models[model_name]
        
        try:
            if config.provider == 'openai':
                response = openai.ChatCompletion.create(
                    model=model_name,
                    messages=[
                        {"role": "system", "content": "You are a creative story writer."},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=200
                )
                return response.choices[0].message.content
                
            elif config.provider == 'anthropic':
                message = self.anthropic_client.messages.create(
                    model=model_name,
                    max_tokens=200,
                    messages=[{"role": "user", "content": prompt}]
                )
                return message.content[0].text
                
            elif config.provider == 'google':
                model = genai.GenerativeModel(model_name)
                response = model.generate_content(prompt)
                return response.text
                
        except Exception as e:
            return f"Error with {model_name}: {str(e)}"
    
    def generate_story(self, model_name: str, plot: str, style: str = "narrative") -> str:
        """æ ¹æ“šæƒ…ç¯€ç”Ÿæˆå®Œæ•´æ•…äº‹"""
        prompt = f"""Based on this plot: {plot}
        
        Write a complete short story in {style} style.
        Make it engaging and about 500 words.
        """
        
        # ä½¿ç”¨ç›¸åŒçš„ç”Ÿæˆé‚è¼¯ï¼Œä½† max_tokens æ›´å¤§
        if model_name not in self.models:
            return f"Model {model_name} not available"
        
        config = self.models[model_name]
        
        try:
            if config.provider == 'openai':
                response = openai.ChatCompletion.create(
                    model=model_name,
                    messages=[
                        {"role": "system", "content": "You are a creative story writer."},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=1000
                )
                return response.choices[0].message.content
                
            # ... å…¶ä»–æ¨¡å‹çš„å¯¦ç¾é¡ä¼¼
            
        except Exception as e:
            return f"Error: {str(e)}"
```

### 5. `utils/story_processor.py` - æ•…äº‹è™•ç†å™¨

```python
import openai
from typing import List, Dict
import json

class StoryProcessor:
    def __init__(self):
        self.plots = []
        
    def cluster_plots(self, plots: List[Dict[str, str]]) -> List[Dict]:
        """ä½¿ç”¨ GPT å°‡ç›¸ä¼¼çš„æƒ…ç¯€åˆ†ç¾¤"""
        
        # æº–å‚™æƒ…ç¯€åˆ—è¡¨
        plot_text = "\n".join([f"{i+1}. {p['plot']}" for i, p in enumerate(plots)])
        
        prompt = f"""
        Please analyze these story plots and group similar ones together.
        Return the result as JSON format with groups and their plot numbers.
        
        Plots:
        {plot_text}
        
        Format:
        {{
            "groups": [
                {{
                    "theme": "theme description",
                    "plot_indices": [1, 3, 5]
                }}
            ]
        }}
        """
        
        try:
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "You are a story analyst."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=500
            )
            
            result = json.loads(response.choices[0].message.content)
            return result['groups']
            
        except Exception as e:
            print(f"Clustering error: {e}")
            # å¦‚æœå¤±æ•—ï¼Œè¿”å›æ‰€æœ‰æƒ…ç¯€ä½œç‚ºä¸€å€‹ç¾¤çµ„
            return [{
                "theme": "All plots",
                "plot_indices": list(range(1, len(plots) + 1))
            }]
    
    def format_to_markdown(self, story: str, metadata: Dict) -> str:
        """å°‡æ•…äº‹æ ¼å¼åŒ–ç‚º Markdown"""
        markdown = f"""# {metadata.get('title', 'Generated Story')}

**Generated by:** {metadata.get('model', 'Unknown Model')}  
**Date:** {metadata.get('date', '')}  
**Based on plot:** {metadata.get('plot', '')}

---

{story}

---

*This story was generated using AI models.*
"""
        return markdown
```

### 6. `app.py` - ä¸»æ‡‰ç”¨ç¨‹å¼

```python
import streamlit as st
from datetime import datetime
import random
from utils.llm_manager import LLMManager
from utils.story_processor import StoryProcessor

# åˆå§‹åŒ–
st.set_page_config(page_title="Story Generator Hub", layout="wide")

if 'llm_manager' not in st.session_state:
    st.session_state.llm_manager = LLMManager()
    st.session_state.processor = StoryProcessor()
    st.session_state.generated_plots = []
    st.session_state.selected_plots = []
    st.session_state.clustered_plots = []

st.title("ğŸ­ Story Generator Hub")
st.markdown("Generate creative stories using multiple AI models")

# å´é‚Šæ¬„ - æ¨¡å‹é¸æ“‡
with st.sidebar:
    st.header("Settings")
    available_models = st.session_state.llm_manager.get_available_models()
    
    if not available_models:
        st.error("No models available. Please check your API keys in .env file")
    else:
        st.success(f"âœ… {len(available_models)} models available")

# ä¸»è¦å…§å®¹å€
tab1, tab2, tab3 = st.tabs(["ğŸ“ Generate Plots", "âœ… Select Plots", "ğŸ“– Generate Story"])

# Tab 1: ç”Ÿæˆæƒ…ç¯€
with tab1:
    st.header("Step 1: Generate Story Plots")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # åŸºæœ¬ prompt
        base_prompt = st.text_area(
            "Enter your story theme or idea:",
            "A mysterious adventure in a futuristic city",
            height=100
        )
        
        # é¸æ“‡æ¨¡å‹
        if available_models:
            selected_models = st.multiselect(
                "Select models for plot generation (max 5):",
                available_models,
                default=available_models[:min(3, len(available_models))]
            )
            
            if st.button("ğŸ² Random Selection"):
                num_models = min(5, len(available_models))
                selected_models = random.sample(available_models, 
                                              random.randint(1, num_models))
                st.rerun()
    
    with col2:
        st.info("""
        **Tips:**
        - Select multiple models for diverse plots
        - Use random selection for surprise
        - Each model will generate 2-3 plot variations
        """)
    
    if st.button("Generate Plots", type="primary", disabled=not available_models):
        with st.spinner("Generating plots..."):
            plots = []
            
            # ç‚ºæ¯å€‹é¸å®šçš„æ¨¡å‹ç”Ÿæˆæƒ…ç¯€
            for model in selected_models:
                for i in range(2):  # æ¯å€‹æ¨¡å‹ç”Ÿæˆ2å€‹æƒ…ç¯€
                    plot_prompt = f"{base_prompt} - Create a unique plot outline (variation {i+1})"
                    plot_text = st.session_state.llm_manager.generate_plot(
                        model, plot_prompt
                    )
                    plots.append({
                        'model': model,
                        'plot': plot_text,
                        'id': f"{model}_{i}"
                    })
            
            st.session_state.generated_plots = plots
            
            # è‡ªå‹•åˆ†ç¾¤
            if len(plots) > 0:
                st.session_state.clustered_plots = st.session_state.processor.cluster_plots(plots)
    
    # é¡¯ç¤ºç”Ÿæˆçš„æƒ…ç¯€
    if st.session_state.generated_plots:
        st.subheader("Generated Plots")
        for i, plot in enumerate(st.session_state.generated_plots):
            with st.expander(f"Plot {i+1} - {plot['model']}"):
                st.write(plot['plot'])

# Tab 2: é¸æ“‡æƒ…ç¯€
with tab2:
    st.header("Step 2: Select Plot Groups")
    
    if st.session_state.clustered_plots:
        st.subheader("Clustered Plot Groups")
        
        selected_groups = []
        for i, group in enumerate(st.session_state.clustered_plots):
            st.write(f"**Group {i+1}: {group['theme']}**")
            
            # é¡¯ç¤ºè©²ç¾¤çµ„çš„æƒ…ç¯€
            for idx in group['plot_indices']:
                if idx <= len(st.session_state.generated_plots):
                    plot = st.session_state.generated_plots[idx-1]
                    if st.checkbox(f"{plot['model']}: {plot['plot'][:100]}...", 
                                 key=f"plot_{idx}"):
                        if plot not in st.session_state.selected_plots:
                            st.session_state.selected_plots.append(plot)
            
            st.divider()
    else:
        st.info("Please generate plots first in Tab 1")

# Tab 3: ç”Ÿæˆæ•…äº‹
with tab3:
    st.header("Step 3: Generate Complete Story")
    
    if st.session_state.selected_plots:
        st.subheader("Selected Plots")
        for plot in st.session_state.selected_plots:
            st.info(f"{plot['model']}: {plot['plot'][:150]}...")
        
        # é¸æ“‡ç”¨æ–¼ç”Ÿæˆæ•…äº‹çš„æ¨¡å‹
        story_models = st.multiselect(
            "Select models for story generation (max 3):",
            available_models,
            default=available_models[:min(1, len(available_models))]
        )
        
        story_style = st.selectbox(
            "Story style:",
            ["Narrative", "Mystery", "Sci-Fi", "Fantasy", "Romance"]
        )
        
        if st.button("Generate Story", type="primary"):
            with st.spinner("Generating story..."):
                # åˆä½µé¸å®šçš„æƒ…ç¯€
                combined_plot = " ".join([p['plot'] for p in st.session_state.selected_plots])
                
                stories = []
                for model in story_models:
                    story = st.session_state.llm_manager.generate_story(
                        model, combined_plot, story_style.lower()
                    )
                    stories.append({
                        'model': model,
                        'story': story,
                        'metadata': {
                            'title': f"{story_style} Story",
                            'model': model,
                            'date': datetime.now().strftime("%Y-%m-%d"),
                            'plot': combined_plot[:200]
                        }
                    })
                
                # é¡¯ç¤ºç”Ÿæˆçš„æ•…äº‹
                for story_data in stories:
                    st.subheader(f"Story by {story_data['model']}")
                    
                    # Markdown è¼¸å‡º
                    markdown_content = st.session_state.processor.format_to_markdown(
                        story_data['story'],
                        story_data['metadata']
                    )
                    
                    st.markdown(markdown_content)
                    
                    # ä¸‹è¼‰æŒ‰éˆ•
                    st.download_button(
                        label=f"Download {story_data['model']} story",
                        data=markdown_content,
                        file_name=f"story_{story_data['model']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md",
                        mime="text/markdown"
                    )
    else:
        st.info("Please select plots in Tab 2 first")
```

### 7. åœ¨ GitHub Codespace é‹è¡Œ

```bash
# 1. å‰µå»º .env æ–‡ä»¶ä¸¦æ·»åŠ  API keys
cp .env.example .env
# ç·¨è¼¯ .env æ·»åŠ ä½ çš„ API keys

# 2. å®‰è£ä¾è³´
pip install -r requirements.txt

# 3. é‹è¡Œæ‡‰ç”¨
streamlit run app.py
```

## ç‰ˆæœ¬ 2: Hugging Face Spaces ç‰ˆæœ¬ï¼ˆä½¿ç”¨ Gradioï¼‰

å¦‚æœæ‚¨æƒ³åœ¨ Hugging Face Spaces éƒ¨ç½²ï¼Œé€™è£¡æ˜¯ Gradio ç‰ˆæœ¬ï¼š

### `app_gradio.py`

```python
import gradio as gr
import os
from typing import List
import json
from datetime import datetime

# ç°¡åŒ–ç‰ˆ - ä½¿ç”¨ Hugging Face çš„å…è²»æ¨¡å‹
from transformers import pipeline

class SimpleStoryGenerator:
    def __init__(self):
        # ä½¿ç”¨ Hugging Face çš„å…è²»æ¨¡å‹
        self.generator = pipeline('text-generation', model='gpt2')
        
    def generate_plots(self, theme: str, num_plots: int = 5) -> List[str]:
        """ç”Ÿæˆå¤šå€‹æ•…äº‹æƒ…ç¯€"""
        plots = []
        for i in range(num_plots):
            prompt = f"Story plot about {theme}, variation {i+1}:"
            result = self.generator(prompt, max_length=100, num_return_sequences=1)
            plots.append(result[0]['generated_text'])
        return plots
    
    def cluster_plots(self, plots: List[str]) -> str:
        """ç°¡å–®åˆ†ç¾¤ï¼ˆé€™è£¡ç”¨ç°¡å–®çš„è¦å‰‡ï¼‰"""
        # ç°¡åŒ–ç‰ˆï¼šæŒ‰é•·åº¦åˆ†ç¾¤
        short_plots = [p for p in plots if len(p) < 150]
        long_plots = [p for p in plots if len(p) >= 150]
        
        result = "## Short Plots\n"
        for p in short_plots:
            result += f"- {p}\n"
        
        result += "\n## Long Plots\n"
        for p in long_plots:
            result += f"- {p}\n"
            
        return result
    
    def generate_story(self, selected_plots: str, style: str) -> str:
        """ç”Ÿæˆå®Œæ•´æ•…äº‹"""
        prompt = f"Write a {style} story based on: {selected_plots[:200]}"
        result = self.generator(prompt, max_length=500, num_return_sequences=1)
        
        # æ ¼å¼åŒ–ç‚º Markdown
        story = f"""# Generated {style} Story

**Date:** {datetime.now().strftime('%Y-%m-%d')}

---

{result[0]['generated_text']}

---

*Generated using AI*
"""
        return story

# å‰µå»º Gradio ç•Œé¢
generator = SimpleStoryGenerator()

def process_plots(theme, num_plots):
    plots = generator.generate_plots(theme, int(num_plots))
    clustered = generator.cluster_plots(plots)
    return clustered, plots

def create_story(selected_plots, style):
    if not selected_plots:
        return "Please generate and select plots first!"
    story = generator.generate_story(str(selected_plots), style)
    return story

# Gradio ç•Œé¢
with gr.Blocks(title="Story Generator Hub") as demo:
    gr.Markdown("# ğŸ­ Story Generator Hub")
    
    with gr.Tab("Generate Plots"):
        theme_input = gr.Textbox(label="Story Theme", 
                                value="A mysterious adventure")
        num_plots = gr.Slider(minimum=3, maximum=10, value=5, 
                             label="Number of Plots")
        generate_btn = gr.Button("Generate Plots")
        
        plots_output = gr.Markdown(label="Generated Plots")
        plots_state = gr.State()
        
        generate_btn.click(
            process_plots,
            inputs=[theme_input, num_plots],
            outputs=[plots_output, plots_state]
        )
    
    with gr.Tab("Generate Story"):
        selected_plots = gr.Textbox(label="Selected Plots", 
                                   placeholder="Paste selected plots here")
        style = gr.Dropdown(choices=["Adventure", "Mystery", "Sci-Fi", 
                                    "Fantasy", "Romance"],
                          value="Adventure",
                          label="Story Style")
        
        story_btn = gr.Button("Generate Story")
        story_output = gr.Markdown(label="Generated Story")
        
        story_btn.click(
            create_story,
            inputs=[selected_plots, style],
            outputs=story_output
        )

if __name__ == "__main__":
    demo.launch()
```

### éƒ¨ç½²åˆ° Hugging Face Spaces

1. å‰µå»º `requirements.txt`:
```txt
gradio
transformers
torch
```

2. åœ¨ Hugging Face å‰µå»ºæ–°çš„ Space
3. ä¸Šå‚³æ–‡ä»¶
4. è‡ªå‹•éƒ¨ç½²é‹è¡Œ

é€™å€‹æ–¹æ¡ˆè®“æ‚¨å¯ä»¥ï¼š
1. å…ˆåœ¨ Codespace æœ¬åœ°æ¸¬è©¦å®Œæ•´ç‰ˆæœ¬
2. åœ¨ Hugging Face Spaces éƒ¨ç½²ç°¡åŒ–çš„å…è²»ç‰ˆæœ¬
3. é€æ­¥æ·»åŠ æ›´å¤šåŠŸèƒ½

éœ€è¦æˆ‘è©³ç´°è§£é‡‹æŸå€‹éƒ¨åˆ†æˆ–æ·»åŠ æ›´å¤šåŠŸèƒ½å—ï¼Ÿ

# 03

